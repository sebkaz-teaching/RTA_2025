---
title:  Wykład 2
format:
  html:
    code-fold: true
jupyter: python3
---

 
⏳ Czas trwania: 1,5h
*🎯 Cel wykładu* 

zrozumienie, jak dane ewoluowały w różnych branżach i jakie narzędzia są dziś wykorzystywane do ich analizy.

---

Na tym wykładzie przedstawimy ewolucję analizy danych, pokazując, jak zmieniały się technologie i podejścia do przetwarzania danych na przestrzeni lat. 
Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aż po nowoczesne podejście do strumieniowego przetwarzania danych. 


# **Ewolucja danych**  

## **1. Dane tabelaryczne (tabele SQL)**  
Początkowo dane były przechowywane w postaci tabel, gdzie każda tabela zawierała zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).  
Modele takie doskonale nadawały się do danych **ustrukturyzowanych**.  

### 📌 **Cechy:**  
✅ Dane podzielone na kolumny o stałej strukturze.  
✅ Możliwość stosowania operacji CRUD (Create, Read, Update, Delete).  
✅ Ścisłe reguły spójności i normalizacji.  

### 📌 **Przykłady:**  
➡️ Systemy bankowe, e-commerce, ERP, systemy CRM.  

### 🖥️ **Przykładowy kod w Pythonie (SQLite):**  
```python
import sqlite3
conn = sqlite3.connect(':memory:')
cursor = conn.cursor()
cursor.execute("CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)")
cursor.execute("INSERT INTO users (name, age) VALUES ('Alice', 30)")
cursor.execute("SELECT * FROM users")
print(cursor.fetchall())
conn.close()
```

---

## **2. Dane grafowe**  
Wraz z rozwojem potrzeb biznesowych pojawiły się **dane grafowe**, w których relacje między obiektami są reprezentowane jako **wierzchołki i krawędzie**.  

### 📌 **Cechy:**  
✅ Dane opisujące relacje i powiązania.  
✅ Elastyczna struktura (grafy zamiast tabel).  
✅ Możliwość analizy połączeń (np. algorytmy PageRank, centralność).  

### 📌 **Przykłady:**  
➡️ Sieci społecznościowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).  

### 🖥️ **Przykładowy kod w Pythonie (Graf Karate - NetworkX):**  
```{python}
import networkx as nx
G = nx.karate_club_graph()
nx.draw(G, with_labels=True)
```

---

## **3. Dane półstrukturyzowane (JSON, XML, YAML)**  
Dane te nie są w pełni ustrukturyzowane jak w bazach SQL, ale mają pewien schemat.  

### 📌 **Cechy:**  
✅ Hierarchiczna struktura (np. klucz-wartość, obiekty zagnieżdżone).  
✅ Brak ścisłego schematu (możliwość dodawania nowych pól).  
✅ Popularność w systemach NoSQL i API.  

### 📌 **Przykłady:**  
➡️ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logów.  

### 🖥️ **Przykładowy kod w Pythonie (JSON):**  
```{python}
import json
data = {'name': 'Alice', 'age': 30, 'city': 'New York'}
json_str = json.dumps(data)
print(json.loads(json_str))
```

---

## **4. Dane tekstowe (NLP)**  
Tekst stał się kluczowym źródłem informacji, szczególnie w analizie opinii, chatbotach czy wyszukiwarkach.  

### 📌 **Cechy:**  
✅ Nieustrukturyzowane dane wymagające przekształcenia.  
✅ Stosowanie **embeddingów** (np. Word2Vec, BERT, GPT).  
✅ Duże zastosowanie w analizie sentymentu i chatbotach.  

### 📌 **Przykłady:**  
➡️ Media społecznościowe, e-maile, chatboty, tłumaczenie maszynowe.  

### 🖥️ **Przykładowy kod w Pythonie:**  
```{python}
import ollama

# Przykładowe zdanie
sentence = "Sztuczna inteligencja zmienia świat."
response = ollama.embeddings(model='llama3.2', prompt=sentence)
embedding = response['embedding']
print(embedding[:4])
```

---

## **5. Dane multimedialne (obrazy, dźwięk, wideo)**  
Nowoczesne systemy analizy danych wykorzystują również obrazy i dźwięk.  

### 📌 **Cechy:**  
✅ Wymagają dużej mocy obliczeniowej (sztuczna inteligencja, deep learning).  
✅ Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dźwięk).  

### 📌 **Przykłady:**  
➡️ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treści wideo.  

### 🖥️ **Przykładowy kod w Pythonie (Obraz - OpenCV):**  
```python
import cv2
image = cv2.imread('cloud.jpeg')
cv2.waitKey(0)
cv2.destroyAllWindows()
```
---

## **6. Dane strumieniowe**  
Obecnie najbardziej dynamicznie rozwija się analiza **danych strumieniowych**, gdzie dane są analizowane na bieżąco, w miarę ich napływania.  

### 📌 **Cechy:**  
✅ Przetwarzanie w czasie rzeczywistym.  
✅ Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.  

### 📌 **Przykłady:**  
➡️ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.  

### 🖥️ **Przykładowy kod w Pythonie (Strumieniowe transakcje bankowe):**  
```{python}
import time
transactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]
for transaction in transactions:
    print(f"Processing transaction: {transaction}")
    time.sleep(1)
```

---

## **7. Dane sensoryczne i IoT**  
Dane z czujników i urządzeń IoT są kolejnym krokiem w ewolucji.  

### 📌 **Cechy:**  
✅ Często pochodzą z miliardów urządzeń (big data).  
✅ Wymagają analizy brzegowej (_edge computing_).  

### 📌 **Przykłady:**  
➡️ Smart home, wearables, samochody autonomiczne, systemy przemysłowe.  

### 🖥️ **Przykładowy kod w Pythonie (Sensor - temperatura):**  
```{python}
import random
def get_temperature():
    return round(random.uniform(20.0, 25.0), 2)
print(f"Current temperature: {get_temperature()}°C")
```

# Rzeczywisty proces generowania danych

Dane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów.
W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych.
Czy na następnych zajęciach lub też jutro nie będziesz ich generował?

> Dane zawsze generowane są jako forma strumienia danych.

📌 Systemy obsługujące strumienie danych:

- Hurtownie danych
- Systemy monitorujące działania urządzeń (IoT)
- Systemy transakcyjne
- Systemy analityczne stron www
- Reklamy on-line
- Media społecznościowe
- Systemy logowania

> Firma to organizacja, która generuje i odpowiada na ciągły strumień danych.

W przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest **plik**.

Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań).

Nazwa pliku to element identyfikujący zbiór rekordów.

W przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą).
Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców).
Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics).


# Duże dane 

Kiedy podjąć decyzję biznesową ?

<img alt="Batch Processing" src="img/batch0.png" class="center" />

## **Hadoop Map-Reduce – Skalowanie obliczeń na Big Data**

Kiedy mówimy o **skalowalnym przetwarzaniu danych**, pierwszym skojarzeniem może być **Google**.  
Ale co tak naprawdę sprawia, że możemy wyszukiwać informacje w ułamku sekundy, przetwarzając petabajty danych?  

👉 **Czy wiesz, że nazwa "Google" pochodzi od słowa "Googol", czyli liczby równej 10¹⁰⁰?**  
To więcej niż liczba atomów w znanym Wszechświecie! 🌌  

### **🔥 Wyzwanie: Czy uda Ci się zapisać liczbę Googol do końca zajęć?**  

---  

## **🔍 Dlaczego SQL i klasyczne algorytmy nie wystarczają?**  

Tradycyjne **bazy danych SQL** czy **jednowątkowe algorytmy** zawodzą, gdy skala danych przekracza pojedynczy komputer.  
W tym miejscu pojawia się **MapReduce** – rewolucyjny model obliczeniowy stworzony przez Google.  

### **🛠️ Rozwiązania Google dla Big Data:**  
✅ **Google File System (GFS)** – rozproszony system plików.  
✅ **Bigtable** – system do przechowywania ogromnych ilości ustrukturyzowanych danych.  
✅ **MapReduce** – algorytm podziału pracy na wiele maszyn.  

## **🖼️ Graficzne przedstawienie MapReduce**  

### **1. Mapowanie rozdziela zadania (Map)**  
Każde wejście dzielone jest na mniejsze części i przetwarzane równolegle.  

🌍 Wyobraź sobie, że masz **książkę telefoniczną** i chcesz znaleźć wszystkie osoby o nazwisku "Nowak".  
➡️ **Podziel książkę na fragmenty i daj każdemu do przeanalizowania jeden fragment.**  

### **2. Redukcja zbiera wyniki (Reduce)**  
Wszystkie częściowe wyniki są łączone w jedną, końcową odpowiedź.  

🔄 **Wszyscy uczniowie zgłaszają swoje wyniki, a jeden student zbiera i podsumowuje odpowiedź.**  

<img alt="MapReduce w działaniu – rozdzielanie i agregowanie danych" src="img/mapreduce_flow.png" class="center" />
---  

## **💡 Klasyczny przykład: Liczenie słów w tekście**  
Załóżmy, że mamy miliony książek i chcemy policzyć, ile razy występuje każde słowo.  

### **🖥️ Kod MapReduce w Pythonie (z użyciem multiprocessing)**  

```python
from multiprocessing import Pool
from collections import Counter

# Funkcja Map (podział tekstu na słowa)
def map_function(text):
    words = text.split()
    return Counter(words)

# Funkcja Reduce (sumowanie wyników)
def reduce_function(counters):
    total_count = Counter()
    for counter in counters:
        total_count.update(counter)
    return total_count

texts = [
        "big data is amazing",
        "data science and big data",
        "big data is everywhere"
    ]
if __name__ == '__main__':    
    with Pool() as pool:
        mapped_results = pool.map(map_function, texts)
    
    final_result = reduce_function(mapped_results)
    print(final_result)

# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})
```

### **🔹 Co tu się dzieje?**  
✅ Każdy fragment tekstu jest przetwarzany niezależnie (**map**).  
✅ Wyniki są zbierane i sumowane (**reduce**).  
✅ **Efekt:** Możemy przetwarzać **terabajty tekstu równolegle**!  

<img alt="Batch Processing" src="img/batch3.png" class="center" />

---  

## **🎨 Wizualizacja – Porównanie klasycznego podejścia i MapReduce**  

📊 **Stare podejście** – Jeden komputer wykonuje wszystko sekwencyjnie.  
📊 **Nowe podejście (MapReduce)** – Każda maszyna liczy fragment i wyniki są agregowane.  


<img alt="MapReduce Processing" src="img/mapreduce_vs_classic.png" class="center" />

---  

## **🚀 Wyzwanie dla Ciebie!**  

🔹 **Znajdź i uruchom swój własny algorytm MapReduce w dowolnym języku!**  
🔹 **Czy potrafisz zaimplementować własny MapReduce do innego zadania?** (np. analiza logów, zliczanie kliknięć na stronie)  


## Big Data

Systemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)

Ale Hurtownie danych nie są systemami Big Data!

1. Hurtownie danych
- przetrzymywanie danych wysoko strukturyzowanych
- skupione na analizach i procesie raportowania
- 100\% accuracy

2. Big Data
- dane o dowolnej strukturze
- służy do różnorodnych celów opartych na danych (analityka, data science ...)
- poniżej 100\% accuracy

> _,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.''_ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University

### one, two, ... four V

1. **Volume**  (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.
2. **Velocity** (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.
3. **Variety** (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT
4. **Veracity** (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?
5. **Value** - The value that the data actually holds. In the end, it's all about cost and benefits.

> _Celem obliczeń nie są liczby, lecz ich zrozumienie_ R.W. Hamming 1962. 


## Modele przetwarzania danych

Dane w biznesie przetwarzane są praktycznie od zawsze. 
W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.

### Trochę historii

- Lata 60-te : Kolekcje danych, bazy danych
- Lata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP
- 1975 : Pierwsze komputery osobiste 
- Lata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.
- 1983 : Początek internetu
- Lata 90-te : Data mining, hurtownie danych, systemy OLAP
- Później : NoSQL, Hadoop, SPARK, data lake
- 2002 : AWS , 2005: Hadoop, Cloud computing 


Większość danych przechowywana jest w bazach lub hurtowniach danych.
Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.

Sposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy **modelem przetwarzania**.
Najczęściej używane są dwie implementacje:

### Model Tradycyjny

**Model tradycyjny** - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing).
Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp.
Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.

![](img/baza1.png){.center}

Model ten dostarcza efektywnych rozwiązań m.in do:

- efektywnego i bezpiecznego przechowywania danych,
- transakcyjnego odtwarzanie danych po awarii,
- optymalizacji dostępu do danych,
- zarządzania współbieżnością,
- przetwarzania zdarzeń -> odczyt -> zapis

Co w przypadku gdy mamy do czynienia z:

- agregacjami danych z wielu systemów (np. dla wielu sklepów),
- raportowanie i podsumowania danych,
- optymalizacja złożonych zapytań,
- wspomaganie decyzji biznesowych.

Badania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - **Hurtownie Danych** _(Data warehouse)_.

### Model OLAP

**Przetwarzanie analityczne on-line OLAP (on-line analytic processing).**

 Wspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (`czas`, `miejsce`, `produkt`).

 Proces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).

 Analiza danych z hurtowni to przede wszystkim obliczanie **agregatów** (podsumowań) dotyczących wymiarów hurtowni.
 Proces ten jest całkowicie sterowany przez użytkownika.

**Przykład**

Załóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie.
Jak przeanalizować zapytania:

1. Jaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?
2. Jaka jest sprzedaż produktów z podziałem na rodzaje produktów ?
3. Jaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?

Odpowiedzi na te pytania pozwalają określić `wąskie gardła` sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.

W ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym):
1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki
2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe.

![](img/baza2.png){.center}


# Strumienie danych

Strumieniowanie możesz kojarzyć z serwisów przesyłających wideo online. Gdy oglądasz swój ulubiony serial (tak jak teraz na zajęciach), platforma streamingowa w sposób nieprzerwany dostarcza do Ciebie kolejne „porcje” wideo.

Podobna koncepcja dotyczy danych strumieniowych. Przesyłane porcje nie muszą mieć formatu wideo – wszystko zależy od celu biznesowego. Przykładem może być ciągły pomiar z czujników w fabrykach, elektrowniach czy innych systemach monitorujących.

Kluczowe jest to, że masz do czynienia z nieprzerwanym strumieniem danych, które musisz przetwarzać w czasie rzeczywistym. Nie możesz czekać na zatrzymanie linii produkcyjnej, aby przeprowadzić analizę – wszelkie pojawiające się problemy powinny być rejestrowane natychmiast, aby szybko na nie reagować.

	Analiza strumieniowa to ciągłe przetwarzanie i analiza dużych zbiorów danych w ruchu.

Można to porównać do innych aspektów Big Data. Przetwarzanie wsadowe (batchowe) to przeciwieństwo przetwarzania strumieniowego – najpierw gromadzisz duże ilości danych, a dopiero potem je analizujesz. Oczywiście, możesz pobrać całe wideo przed obejrzeniem, ale czy miałoby to sens?

Istnieją sytuacje, w których podejście wsadowe jest wystarczające, ale już teraz widzisz, że przetwarzanie strumieniowe może dostarczyć biznesowi dodatkową wartość, której trudno oczekiwać przy klasycznym przetwarzaniu wsadowym.

[ciekawe informacje](https://aws.amazon.com/streaming-data/)

### Analiza danych w czasie rzeczywistym a przetwarzanie strumienia zdarzeń

Łatwo jest połączyć analizę w czasie rzeczywistym z analizą strumieniową (lub przetwarzaniem strumienia zdarzeń). Choć technologie analizy strumieniowej mogą umożliwiać analizę w czasie rzeczywistym, to nie to samo!

Analiza strumieniowa polega na przetwarzaniu danych w ruchu, natomiast analityka w czasie rzeczywistym odnosi się do każdej metody przetwarzania danych, w której opóźnienie jest na tyle krótkie, że spełnia określone wymagania biznesowe dotyczące „czasu rzeczywistego”.

Zazwyczaj systemy analizy w czasie rzeczywistym dzieli się na twarde i miękkie systemy czasu rzeczywistego:
-	Twarde systemy czasu rzeczywistego (np. systemy sterujące w samolotach) wymagają ścisłego dotrzymania terminów – każde opóźnienie może mieć katastrofalne skutki.
-	Miękkie systemy czasu rzeczywistego (np. stacje pogodowe) mogą tolerować pewne opóźnienia, choć w skrajnych przypadkach może to prowadzić do utraty wartości danych.

Co więcej, podczas gdy analiza strumieniowa zakłada istnienie dedykowanej architektury strumieniowej, analiza w czasie rzeczywistym nie jest powiązana z żadną konkretną architekturą.

Kluczowe jest to, że analityka w czasie rzeczywistym oznacza przetwarzanie danych w okresie, który dana firma uznaje za „czas rzeczywisty” – może to być zarówno kilka milisekund, jak i kilka sekund, w zależności od potrzeb biznesowych.

### Źródła danych przesyłanych strumieniowo obejmują:
	•	czujniki sprzętu,
	•	strumienie kliknięć,
	•	śledzenie lokalizacji,
	•	interakcję z użytkownikiem (np. działania użytkowników na Twojej witrynie),
	•	kanały mediów społecznościowych,
	•	notowania giełdowe,
	•	aktywność w aplikacjach,
	•	inne.

Firmy wykorzystują analitykę strumieniową do wykrywania i interpretowania wzorców, tworzenia wizualizacji, generowania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub bliskim rzeczywistemu.

### Uzasadnienie biznesowe

Analityka służy do identyfikowania istotnych wzorców w danych i odkrywania nowej wiedzy – zarówno w przypadku transmisji strumieniowych, jak i tradycyjnych analiz.

Jednak w dzisiejszym świecie natura „znajdowania sensownych wzorców w danych” uległa zmianie, ponieważ zmienił się charakter samych danych. Ich szybkość, objętość i różnorodność eksplodowały.

Twitter generuje ponad 500 milionów tweetów dziennie, a według prognoz IDC do 2025 roku urządzenia IoT będą w stanie wytworzyć 79,4 zettabajta (ZB) danych. Trend ten wciąż przyspiesza.



### Przykładowe biznesowe zastosowania

1. Dane z sensorów IoT i detekcja anomalii
2. Stock Trading (problemy regresyjne) - czas reagowania na zmiany i czas zakupy i sprzedaży akcji. 
3. Clickstream for websites (problem klasyfikacji) - śledzenie i analiza gości na stronie serwisu internetowego - personalizacja strony i treści. 

[8 najlepszych przykładów analizy w czasie rzeczywistym](https://www.linkedin.com/pulse/8-best-examples-real-time-data-analytics-bernard-marr/)

[Biznesowe zastosowania](https://www.forbes.com/sites/forbestechcouncil/2021/10/26/how-to-build-a-strong-business-case-for-streaming-analytics/?sh=eee8eaa465d0)

## Definicje

### Zapoznaj się z tematem [danych strumieniowych](https://medium.com/cuelogic-technologies/analyzing-data-streaming-using-spark-vs-kafka-bcfdc33ac828)

> **Definicja 1** – *Zdarzenie* to wszystko, co można zaobserwować w danym momencie czasu. Jest generowane jako bezpośredni skutek działania.  
>
> **Definicja 2** – W kontekście danych *zdarzenie* to **niezmienialny** rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.  
>
> **Definicja 3** – *Ciągły strumień zdarzeń* to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie, np. logi z urządzeń.  
>
> **Definicja 4** – *Strumień danych* to dane tworzone przyrostowo w czasie, generowane ze źródeł statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).  
>
> **Przedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.**

---

### **Analityka strumieniowa**  

**Analityka strumieniowa** (ang. _stream analytics_) nazywana jest również **przetwarzaniem strumieniowym zdarzeń** (ang. _event stream processing_) – czyli przetwarzaniem dużych ilości danych już na etapie ich generowania.  

Niezależnie od zastosowanej technologii, wszystkie dane powstają jako **ciągły strumień zdarzeń** – obejmuje to m.in.:  
- działania użytkowników na stronach internetowych,  
- logi systemowe,  
- pomiary z sensorów.  

## Czas w analizie danych w czasie rzeczywistym

W przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma żadnego związku z momentem wystąpienia analizowanych zdarzeń.

Natomiast w przetwarzaniu strumieniowym wyróżniamy dwie koncepcje czasu:
1.	Czas zdarzenia (event time) – moment, w którym zdarzenie faktycznie miało miejsce.
2.	Czas przetwarzania (processing time) – moment, w którym system przetwarza zdarzenie.

Idealne przetwarzanie danych

W idealnej sytuacji przetwarzanie następuje natychmiast po wystąpieniu zdarzenia:

<img alt="Batch Processing" src="img/rys2_1.png" class="center" />

Rzeczywiste przetwarzanie danych

W praktyce przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co jest widoczne jako punkty poniżej linii idealnego przetwarzania (poniżej przekątnej na wykresie).

<img alt="Batch Processing" src="img/rys2_2.png" class="center" />

W aplikacjach przetwarzania strumieniowego istotna jest różnica między czasem powstania zdarzenia a czasem jego przetwarzania. Do najczęstszych przyczyn opóźnień należą:

- przesyłanie danych przez sieć,
- brak komunikacji między urządzeniem a siecią.

Przykładem jest śledzenie położenia samochodu przez aplikację GPS – przejazd przez tunel może spowodować chwilową utratę danych.

Obsługa opóźnień w przetwarzaniu strumieniowym

Opóźnienia w przetwarzaniu zdarzeń można obsłużyć na dwa sposoby:
1.	Monitorowanie liczby pominiętych zdarzeń i wyzwalanie alarmu w przypadku zbyt dużej liczby odrzuceń.
2.	Zastosowanie korekty za pomocą watermarkingu, czyli dodatkowego mechanizmu uwzględniającego opóźnione zdarzenia.

Proces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić jako funkcję schodkową:

<img alt="Batch Processing" src="img/rys2_3.png" class="center" />

Nie wszystkie zdarzenia wnoszą wkład do analizy – niektóre mogą zostać odrzucone ze względu na zbyt duże opóźnienie.

Wykorzystanie watermarkingu pozwala na uwzględnienie dodatkowego czasu na pojawienie się opóźnionych zdarzeń. Proces ten obejmuje wszystkie zdarzenia powyżej przerywanej linii. Mimo to nadal mogą zdarzyć się przypadki, w których niektóre punkty zostaną pominięte.

<img alt="Batch Processing" src="img/rys2_4.png" class="center" />


Przedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych.
Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie. 

## **Okna czasowe w analizie strumieniowej**  

W przetwarzaniu strumieniowym **okna czasowe** pozwalają na grupowanie danych w ograniczone czasowo segmenty, co umożliwia analizę zdarzeń w określonych przedziałach czasowych. W zależności od zastosowania stosuje się różne typy okien, dostosowane do charakterystyki danych i wymagań analitycznych.  

---

### **1. Okno rozłączne (_Tumbling Window_)**  
Jest to okno o stałej długości, które **nie nakłada się na siebie** – każde zdarzenie należy tylko do jednego okna.  

✅ **Charakterystyka:**  
- Stała długość okna  
- Brak nakładania się na siebie  
- Idealne do podziału danych na równe segmenty czasowe  

📌 **Przykład:** Analiza liczby zamówień w sklepie internetowym co 5 minut.  

<img alt="Tumbling Window" src="img/rys2_5.png" class="center" />  

---

### **2. Okno przesuwne (_Sliding Window_)**  
Obejmuje wszystkie zdarzenia następujące w określonym przedziale czasu, gdzie okno **przesuwa się w sposób ciągły**.  

✅ **Charakterystyka:**  
- Każde zdarzenie może należeć do kilku okien  
- Okno przesuwa się o zadany interwał  
- Przydatne do wykrywania trendów i anomalii  

📌 **Przykład:** Śledzenie średniej temperatury w ciągu ostatnich 10 minut, aktualizowane co 2 minuty.  

<img alt="Sliding Window" src="img/rys2_6.png" class="center" />  

---

### **3. Okno skokowe (_Hopping Window_)**  
Jest podobne do okna rozłącznego, ale pozwala na **nakładanie się okien** na siebie, dzięki czemu jedno zdarzenie może należeć do kilku okien. Jest stosowane do wygładzania danych.  

✅ **Charakterystyka:**  
- Stała długość okna  
- Możliwość nakładania się na siebie  
- Przydatne do redukcji szumów w danych  

📌 **Przykład:** Analiza liczby odwiedzających stronę co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywać trendy.  

<img alt="Hopping Window" src="img/rys2_7.png" class="center" />  

---

### **4. Okno sesyjne (_Session Window_)**  
Okno sesyjne grupuje zdarzenia na podstawie **okresów aktywności** i zamyka się po określonym czasie braku aktywności.  

✅ **Charakterystyka:**  
- Dynamiczna długość okna  
- Definiowane przez aktywność użytkownika  
- Stosowane w analizie sesji użytkowników  

📌 **Przykład:** Analiza sesji użytkowników na stronie internetowej – sesja trwa tak długo, jak długo użytkownik wykonuje akcje, ale kończy się po 15 minutach braku aktywności.  
---

### **Podsumowanie**  
Różne rodzaje okien czasowych są stosowane w zależności od specyfiki danych i celów analizy. Wybór odpowiedniego okna wpływa na dokładność wyników i efektywność systemu analitycznego.  

| Typ okna | Charakterystyka | Zastosowanie |
|----------|---------------|--------------|
| **Rozłączne (_Tumbling_)** | Stała długość, brak nakładania | Raporty okresowe |
| **Przesuwne (_Sliding_)** | Stała długość, nakładające się okna | Trendy, wykrywanie anomalii |
| **Skokowe (_Hopping_)** | Stała długość, częściowe nakładanie | Wygładzanie danych |
| **Sesyjne (_Session_)** | Dynamiczna długość, zależna od aktywności | Analiza sesji użytkowników |

Każdy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. Wybór właściwej metody zależy od potrzeb biznesowych i charakterystyki analizowanych danych.


W analizie danych strumieniowych interpretacja czasu jest złożonym zagadnieniem, ponieważ:
1. Różne systemy mają różne zegary, co może prowadzić do niespójności,
2. Dane mogą docierać z opóźnieniem, co wymaga technik watermarkingu i okien czasowych,
3. Różne podejścia do analizy czasu zdarzenia i czasu przetwarzania wpływają na dokładność wyników.