{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie\n",
    "\n",
    "Apache Kafka to system przetwarzania strumieniowego (event streaming), który działa jako rozproszony broker wiadomości. \n",
    "Pozwala na przesyłanie i przetwarzanie danych w czasie rzeczywistym.\n",
    "\n",
    "Domyślnym adresem naszego brokera jest `broker:9092`.\n",
    "\n",
    "W Apache Kafka dane są przechowywane w strukturach zwanych **topicami**, które pełnią funkcję kolejek komunikacyjnych.\n",
    "\n",
    "Zarządzanie Kafką odbywa się za pomocą skryptów. W naszym przypadku będą to skrypty `.sh`.\n",
    "\n",
    "## 1️⃣ Sprawdź listę topiców\n",
    "Pamiętaj, aby przejść do katalogu domowego:\n",
    "```sh\n",
    "cd ~\n",
    "kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "```\n",
    "\n",
    "## 2️⃣ Utwórz nowy topic o nazwie `mytopic`\n",
    "```sh\n",
    "kafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092\n",
    "```\n",
    "\n",
    "## 3️⃣ Utwórz producenta w terminalu\n",
    "Ten skrypt pozwoli Ci wprowadzać eventy ręcznie przez terminal. Opcje `--property` są dodatkowe i służą do analizy w tym przykładzie.\n",
    "```sh\n",
    "kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"\n",
    "```\n",
    "\n",
    "## 4️⃣ Consumer w Sparku\n",
    "Otwórz nowy terminal w miejscu, gdzie znajduje się plik `test_key_value.py`, i uruchom program **Consumer**a w Sparku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file test_key_value.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BROKER = 'broker:9092'\n",
    "KAFKA_TOPIC = 'mytopic'\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (spark.readStream.format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "      .option(\"subscribe\", KAFKA_TOPIC)\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .load()\n",
    "     )\n",
    "\n",
    "# Konwersja danych binarnych na stringi\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    " .writeStream \\\n",
    " .format(\"console\") \\\n",
    " .outputMode(\"append\") \\\n",
    " .start() \\\n",
    " .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pamiętaj, że Apache Spark nie posiada domyślnego konektora do Kafki, dlatego uruchom proces za pomocą `spark-submit` i pobierz odpowiedni pakiet w Scali:\n",
    "\n",
    "```sh\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py\n",
    "```\n",
    "\n",
    "## 5️⃣ Przetestuj przesyłanie danych\n",
    "W terminalu z uruchomionym producentem wpisz tekst w postaci:\n",
    "```bash\n",
    "jan:45\n",
    "alicja:20\n",
    "```\n",
    "\n",
    "Sprawdź, co pojawia się w oknie aplikacji Consumera.\n",
    "\n",
    "## 6️⃣ Zakończenie procesu\n",
    "Po zakończeniu pokazu użyj `Ctrl+C`, aby zamknąć zarówno okno producenta, jak i aplikację Spark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
