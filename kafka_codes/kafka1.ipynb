{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache kafka - wprowadzenie\n",
    "\n",
    "Apache kafka to ,,serwer'' oraz ,,baza danych''.\n",
    "\n",
    "Domyślnym adresem naszego serwera jest `broker:9092`\n",
    "\n",
    "Nasza baza danych będzie posługiwała się tabelami, które w Apache Kafka nazywamy `topicami`.\n",
    "\n",
    "Zarządzanie Kafką odbywa się z wykorzystaniem skryptów. W naszym przypadku będą to skrypty  `*.sh`.\n",
    "\n",
    "\n",
    "### Sprawdź listę topiców\n",
    "\n",
    "Pamiętaj aby przejść do katalogu domowego.\n",
    "\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "```\n",
    "\n",
    "### Utwórz nowy topic o nazwie `mytopic`\n",
    "\n",
    "```bash\n",
    "kafka/bin/kafka-topics.sh --create --topic mytopic  --bootstrap-server broker:9092\n",
    "```\n",
    "\n",
    "### Utwórz producenta \n",
    "\n",
    "Ten skrypt pozwoli Ci wprowadzać eventy ręcznie przez terminal.\n",
    "Opcje `--property` są dodatkowe i wprowadzone tylko do analizy przeprowadzanej w tym przykładzie.\n",
    "\n",
    "```bash\n",
    "kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"\n",
    "```\n",
    "\n",
    "### Consument w Sparku\n",
    "\n",
    "Otwórz nowy terminal w miejscu gdzie znajduje się plik test_key_value.py i uruchom program Consumenta na Sparku.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BROKER = 'broker:9092'\n",
    "KAFKA_TOPIC = 'mytopic'\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (spark.readStream.format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "      .option(\"subscribe\", KAFKA_TOPIC)\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .load()\n",
    "     )\n",
    "# to co trafia z kafki przychodzi w postaci binarnej więc trzeba przetworzyć to na string\n",
    "\n",
    "(df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    " .writeStream\n",
    " .format(\"console\")\n",
    " .outputMode(\"append\")\n",
    " .start()\n",
    " .awaitTermination()\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Pamiętaj, iż Apache Spark nie posiada domyślnego konektora do Kafki dlatego uruchom proces poprzez komendę spark-submit i pobierz odpowiedni pakiet w scali.\n",
    "\n",
    "```bash\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0  test_key_value.py\n",
    "```\n",
    "\n",
    "5. W terminalu z uruchominionym producentem wpisz teskt w postaci: \n",
    "```bash\n",
    "jan:45\n",
    "alicja:20\n",
    "```\n",
    "i sprawdz co pojawia się w oknie aplikacji consumenta.\n",
    "\n",
    "Po zakończeniu pokazu ctrl+c zamyka zarówno okno producenta jak i aplikacje sparkową"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Kafka - Zadanie \n",
    "\n",
    "1. Sprawdź listę tematów udostępnionych na serwerze Kafki `broker:9092`\n",
    "2. Dodaj topic o nazwie `streaming##` gdzie w miejsce ## wstaw swój numer przydzielony na początku zajęć. \n",
    "3. Ponownie sprawdź listę tematów weryfikująć czy nowy teamt został dodany do listy.\n",
    "4. Otwórz nowy terminal na notatniku i utwórz producenta w pliku `producer.py`\n",
    "    - ustal nazwę topicu \n",
    "    - ustal bieżący czas i dane w postaci słownika z trzema kluczami i  wartościami: \n",
    "        - klucz_1 == `time` i przypisuje bierzącą datę\n",
    "        - klucz_2 == `id` i wybiera losową wartość z listy `[\"a\", \"b\", \"c\", \"d\"]`\n",
    "        - kliucz_3  ==  `values` - zwraca losową wartość z zakresu 0, 1000\n",
    "\n",
    "\n",
    "```python\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "# USTAL NAZWE TOPICU ZGODNIE Z TWOIM NUMEREM SRODOWISKA\n",
    "TOPIC = \"streaming##\"\n",
    "producer = KafkaProducer(\n",
    "        bootstrap_servers=[SERVER],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "    )\n",
    "    \n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # t = aktualna data i czas       \n",
    "        t = ...\n",
    "            \n",
    "        message = { pass } \n",
    "            \n",
    "            \n",
    "        producer.send(TOPIC, value=message)\n",
    "        sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uruchom nowego consumera w terminalu: \n",
    "\n",
    "```bash\n",
    "kafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming## --from-beginning\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
