{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e569a8a8",
   "metadata": {},
   "source": [
    "# Wprowadzenie do Apache Spark \n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab6\").getOrCreate()\n",
    "```\n",
    "\n",
    "```python\n",
    "spark\n",
    "\n",
    "SparkSession - in-memory\n",
    "\n",
    "SparkContext\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "## Przyk≈Çad 1 - dane ralizujƒÖce szereg czasowy\n",
    "\n",
    "```python\n",
    "czujnik_temperatury = ((12.5, \"2019-01-02 12:00:00\"),\n",
    "(17.6, \"2019-01-02 12:00:20\"),\n",
    "(14.6,  \"2019-01-02 12:00:30\"),\n",
    "(22.9,  \"2019-01-02 12:01:15\"),\n",
    "(17.4,  \"2019-01-02 12:01:30\"),\n",
    "(25.8,  \"2019-01-02 12:03:25\"),\n",
    "(27.1,  \"2019-01-02 12:02:40\"),\n",
    ")\n",
    "```\n",
    "Dane realizujƒÖce pomiar temperatury w czasie.\n",
    "\n",
    "Aby wygenerowaƒá DataFrame nale≈ºy u≈ºyƒá metod `createDataFrame`. \n",
    "Jednak nale≈ºy pamiƒôtaƒá aby zdefiniowaƒá typy danych. \n",
    "\n",
    "W nastƒôpnych laboratoriach szerzej opiszemy typy danych w Sparku.\n",
    "\n",
    "Zdefiuniujmy schemat naszych danych.\n",
    "```python\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"temperatura\", DoubleType(), True),\n",
    "    StructField(\"czas\", StringType(), True),\n",
    "])\n",
    "```\n",
    "Jak widaƒá praktycznie wszystkie elementy poza nazwƒÖ kolumny oraz parametrem True sƒÖ przedstawione jako obiekty. \n",
    "\n",
    "```python\n",
    "df = (spark.createDataFrame(czujnik_temperatury, schema=schema)\n",
    "      .withColumn(\"czas\", to_timestamp(\"czas\")))\n",
    "```\n",
    "\n",
    "Sprawd≈∫my jak wyglƒÖda schemat utworzonej tabeli.\n",
    "```python\n",
    "df.printSchema()\n",
    "\n",
    "root\n",
    " |-- temperatura: double (nullable = true)\n",
    " |-- czas: timestamp (nullable = true)\n",
    "\n",
    "\n",
    "``` \n",
    "\n",
    "Nastƒôpnie mo≈ºemy sprawdziƒá jak przedstawia siƒô sama tabela.\n",
    "```python\n",
    "df.show()\n",
    "+-----------+-------------------+\n",
    "|temperatura|               czas|\n",
    "+-----------+-------------------+\n",
    "|       12.5|2019-01-02 12:00:00|\n",
    "|       17.6|2019-01-02 12:00:20|\n",
    "|       14.6|2019-01-02 12:00:30|\n",
    "|       22.9|2019-01-02 12:01:15|\n",
    "|       17.4|2019-01-02 12:01:30|\n",
    "|       25.8|2019-01-02 12:03:25|\n",
    "|       27.1|2019-01-02 12:02:40|\n",
    "+-----------+-------------------+\n",
    "```\n",
    "\n",
    "### Spark jako SQL\n",
    "\n",
    "Ramki danych w sparku pozwalajƒÖ wykorzystaƒá jƒôzyk sql:\n",
    "```python\n",
    "df.createOrReplaceTempView(\"czujnik_temperatury\")\n",
    "spark.sql(\"SELECT * FROM czujnik_temperatury where temperatura > 21\").show()\n",
    "+-------------------+-----------+\n",
    "|               czas|temperatura|\n",
    "+-------------------+-----------+\n",
    "|2019-01-02 12:01:15|       22.9|\n",
    "|2019-01-02 12:03:25|       25.8|\n",
    "|2019-01-02 12:02:40|       27.1|\n",
    "+-------------------+-----------+\n",
    "```\n",
    "\n",
    "### Grupowanie danych\n",
    "\n",
    "Standardowy grupowanie danych w sparku po zmiennej \"czas\" wygeneruje nam liczbƒô wierszy w ka≈ºdym grupie. Ze wzglƒôdu, i≈º zmienne czasowe majƒÖ r√≥≈ºne warto≈õci,  ilo≈õƒá otrzymanych grup bƒôdzie r√≥wna ilo≈õci wierszy w tabeli.\n",
    " \n",
    "```python\n",
    "df2 = df.groupBy(\"czas\").count()\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "WykorzystujƒÖc funkcjƒô `window` mo≈ºemy wygenerowaƒá grupy czasowe w zale≈ºno≈õci od wybranego okna czasowego. \n",
    "\n",
    "```python\n",
    "# Thumbling window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df.groupBy(F.window(\"czas\",\"30 seconds\")).count()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "+------------------------------------------+-----+\n",
    "|window                                    |count|\n",
    "+------------------------------------------+-----+\n",
    "|{2019-01-02 12:00:00, 2019-01-02 12:00:30}|2    |\n",
    "|{2019-01-02 12:00:30, 2019-01-02 12:01:00}|1    |\n",
    "|{2019-01-02 12:01:00, 2019-01-02 12:01:30}|1    |\n",
    "|{2019-01-02 12:01:30, 2019-01-02 12:02:00}|1    |\n",
    "|{2019-01-02 12:03:00, 2019-01-02 12:03:30}|1    |\n",
    "|{2019-01-02 12:02:30, 2019-01-02 12:03:00}|1    |\n",
    "+------------------------------------------+-----+\n",
    "```\n",
    "Sprawd≈∫my schemat\n",
    "```python\n",
    "df2.printSchema()\n",
    "root\n",
    " |-- window: struct (nullable = false)\n",
    " |    |-- start: timestamp (nullable = true)\n",
    " |    |-- end: timestamp (nullable = true)\n",
    " |-- count: long (nullable = false)\n",
    "``` \n",
    "Podstawowa r√≥≈ºnica miƒôdzy pandasowymi ramkami danych i sparkowymi jest taka, ≈ºe w kom√≥rce danych sparkowych mo≈ºna u≈ºywaƒá typ√≥w z≈Ço≈ºonych - np `struct`.\n",
    "\n",
    "# üîå ≈πr√≥d≈Ça danych w Spark Structured Streaming\n",
    "\n",
    "Spark Structured Streaming pozwala na przetwarzanie danych w czasie rzeczywistym z r√≥≈ºnych ≈∫r√≥de≈Ç strumieniowych. Najpopularniejsze z nich to:\n",
    "\n",
    "### ‚úÖ rate ‚Äî ≈∫r√≥d≈Ço testowe\n",
    "\n",
    "-\tAutomatycznie generuje dane: co sekundƒô dodaje wiersz.\n",
    "-\tKa≈ºdy wiersz zawiera:\n",
    "-\ttimestamp ‚Äì znacznik czasu,\n",
    "-\tvalue ‚Äì licznik rosnƒÖcy (0, 1, 2, ‚Ä¶).\n",
    "-\tU≈ºywane do testowania logiki strumieniowania bez konieczno≈õci podpinania zewnƒôtrznych ≈∫r√≥de≈Ç.\n",
    "\n",
    "```python\n",
    "df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "```\n",
    "### üì° Inne ≈∫r√≥d≈Ça strumieni:\n",
    "\n",
    "`Socket` (do test√≥w np. nc -lk 9999): To ≈∫r√≥d≈Ço nas≈Çuchuje na wskazanym porcie gniazda (socket) i wczytuje dowolne dane do Spark Streaming. R√≥wnie≈º s≈Çu≈ºy wy≈ÇƒÖcznie do cel√≥w testowych.\n",
    "\n",
    "`Plik` (File): Nas≈Çuchuje okre≈õlonego katalogu i traktuje pojawiajƒÖce siƒô tam pliki jako dane strumieniowe. Obs≈Çuguje formaty takie jak CSV, JSON, ORC oraz Parquet (np. .csv, .json, .parquet).\n",
    "\n",
    "`Kafka`: Odczytuje dane z Apache Kafka¬Æ i jest kompatybilne z brokerami w wersji 0.10.0 lub wy≈ºszej.\n",
    "\n",
    "\n",
    "# üì§ Output Modes ‚Äì tryby wypisywania wynik√≥w\n",
    "\n",
    "outputMode okre≈õla jak Spark wypisuje dane po ka≈ºdej mikroserii (micro-batch). Dostƒôpne tryby to:\n",
    "\n",
    "`append` Wypisuje tylko nowe wiersze, kt√≥re zosta≈Çy dodane w tej mikroserii. Najczƒô≈õciej u≈ºywany.\n",
    "\n",
    "`update` Wypisuje zmienione wiersze - czyli zaktualizowane agregaty.\n",
    "\n",
    "`complete` Wypisuje ca≈ÇƒÖ tabelƒô agregacji po ka≈ºdej mikroserii. Wymaga pe≈Çnej agregacji (np. groupBy).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Utw√≥rzmy nasz pierwszy strumieniowy DataFrame w Sparku, korzystajƒÖc ze ≈∫r√≥d≈Ça danych typu `rate`. \n",
    "\n",
    "Mo≈ºemy zrealizowaƒá kod skryptu\n",
    "```python\n",
    "%%file streamrate.py\n",
    "## uruchom przez spark-submit streamrate.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 1)\n",
    "      .load()\n",
    ")\n",
    "\n",
    "\n",
    "query = (df.writeStream \n",
    "    .format(\"console\") \n",
    "    .outputMode(\"append\") \n",
    "    .option(\"truncate\", False) \n",
    "    .start()\n",
    ") \n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Albo uruchomiƒá kod w notatniku \n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "def process_batch(df, batch_id, tstop=5):\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_id == tstop:\n",
    "        df.stop()\n",
    "\n",
    "\n",
    "df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 1)\n",
    "      .load()\n",
    ")\n",
    "\n",
    "query = (df.writeStream \n",
    "    .format(\"console\") \n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"truncate\", False) \n",
    "    .start()\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
