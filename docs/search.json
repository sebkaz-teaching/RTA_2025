[
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Lecture 1: Introduction to Real-Time Data Analysis\nLecture 2: Data Ingestion and Processing for Real-Time Analysis\nLecture 3: Real-Time Data Analysis Techniques\nLecture 4: Real-Time Data Visualization and Communication\nLecture 5: Case Studies and Implementation\nThroughout the lectures, I’ll incorporate interactive elements, such as:\nThis comprehensive lecture plan will provide students with a solid foundation in real-time data analysis, covering the technical aspects of data ingestion, processing, and visualization, as well as practical applications and best practices."
  },
  {
    "objectID": "plan.html#nowy-program-przedmiotu",
    "href": "plan.html#nowy-program-przedmiotu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Nowy program przedmiotu",
    "text": "Nowy program przedmiotu\n\nBatch vs. Real-Time vs. Streaming Analytics – Różnice między trybami przetwarzania danych, kluczowe koncepcje i zastosowania.\nModele przetwarzania danych w Big Data – Od plików płaskich do Data Lake, wady i zalety podejścia real-time. Mity i fakty o przetwarzaniu w czasie rzeczywistym.\nArchitektura IT dla przetwarzania w czasie rzeczywistym – Omówienie architektur Lambda i Kappa w kontekście strumieniowego przetwarzania danych.\nSystemy przetwarzania danych w czasie rzeczywistym – Przegląd technologii: Apache Kafka, Apache Spark Streaming, Apache Flink i ich zastosowania w Pythonie.\nPodstawy uczenia maszynowego w czasie rzeczywistym – Porównanie offline learning vs. online learning, problemy związane z przyrostowym uczeniem maszynowym.\n\n🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej?\n2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce.\n3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym.\n4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów.\n5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych."
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli III bud G\n\n22-02-2025 (sobota) 08:00-09:30 - Wykład 1\n08-03-2025 (sobota) 08:00-09:30 - Wykład 2\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć) 20 pytań.\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "wyklad2.html",
    "href": "wyklad2.html",
    "title": "Wykład 2",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, jak dane ewoluowały w różnych branżach i jakie narzędzia są dziś wykorzystywane do ich analizy.\nW tym wstępie przedstawimy ewolucję analizy danych, pokazując, jak zmieniały się technologie i podejścia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aż po nowoczesne podejście do strumieniowego przetwarzania danych."
  },
  {
    "objectID": "wyklad2.html#ewolucja-danych",
    "href": "wyklad2.html#ewolucja-danych",
    "title": "Wykład 2",
    "section": "Ewolucja danych:",
    "text": "Ewolucja danych:\n\nDane tabelaryczne (tabele SQL):\nPoczątkowo dane były przechowywane w postaci tabel, gdzie każda tabela zawierała zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL). Modele takie doskonale nadawały się do danych ustrukturyzowanych.\nDane ustrukturyzowane zorganizowane są w kolumnach cech charakteryzujących każdą obserwację (wiersze). Kolumny posiadają etykietę, która wskazuje na ich interpretację.\n\n\nDane grafowe:\nWraz z rozwojem potrzeb biznesowych pojawiły się dane grafowe, w których relacje między obiektami (np. użytkownikami, produktami) są reprezentowane jako wierzchołki i krawędzie grafu. Przykłady takich danych to sieci społecznościowe, linki internetowe czy zależności między produktami.\n\n\nDane tekstowe:\nKolejnym etapem była analiza danych tekstowych, które zaczęły odgrywać istotną rolę w analizie danych z mediów społecznościowych, e-maili, forów czy opinii. Technologie takie jak przetwarzanie języka naturalnego Natural Language Processing (NLP) pozwalają wydobywać sens z nieustrukturyzowanego tekstu.\n\n\nDane strumieniowe:\nObecnie najbardziej dynamicznie rozwija się analiza danych strumieniowych, gdzie dane są analizowane na bieżąco, w miarę ich napływania. Przykładem takiego podejścia jest monitorowanie zdarzeń w czasie rzeczywistym, np. transakcje finansowe, analiza social media, detekcja oszustw."
  },
  {
    "objectID": "wyklad2.html#popularne-narzędzia-do-przechowywania-i-przetwarzania-danych",
    "href": "wyklad2.html#popularne-narzędzia-do-przechowywania-i-przetwarzania-danych",
    "title": "Wykład 2",
    "section": "Popularne narzędzia do przechowywania i przetwarzania danych:",
    "text": "Popularne narzędzia do przechowywania i przetwarzania danych:\n\nSQL i bazy tranzakcyjne OLTP ()\nTradycyjne bazy danych SQL są idealne do pracy z danymi ustrukturyzowanymi, które wymagają ścisłej struktury tabel (np. dane o transakcjach, klientach). SQL jest standardem w analizie takich danych, ale nie jest wystarczający w przypadku pracy z dużymi ilościami danych nieustrukturyzowanych.\n\n\nNoSQL\nBazy danych NoSQL, takie jak MongoDB, Cassandra, pozwalają na bardziej elastyczne podejście do danych, które mogą przybierać różne formy (np. dokumenty JSON, dane z sensorów, obrazy). NoSQL świetnie sprawdza się w przypadku danych, które nie pasują do tradycyjnych struktur tabelarycznych.\n\n\nData Lake\nData Lake to systemy przechowywania ogromnych zbiorów danych, zarówno ustrukturyzowanych, jak i nieustrukturyzowanych. Dzięki Data Lake organizacje mogą przechowywać dane w pierwotnej formie i analizować je później w miarę potrzeb, bez potrzeby wcześniejszego przetwarzania.\n\n\nApache Kafka\nKafka to system przetwarzania strumieniowego, który umożliwia zbieranie, przechowywanie i przetwarzanie danych w czasie rzeczywistym. Jest szeroko wykorzystywany w aplikacjach wymagających szybkiej reakcji na dane napływające w czasie rzeczywistym.\n\n\nApache Flink\nApache Flink to narzędzie do przetwarzania strumieniowego, które umożliwia analizowanie danych w czasie rzeczywistym z minimalnym opóźnieniem. W przeciwieństwie do Kafki, Flink nie tylko zbiera dane, ale także je przetwarza, co jest szczególnie przydatne w analizach złożonych.\n\n\nPrzykład: Jak Netflix analizuje dane w czasie rzeczywistym?\nNetflix to doskonały przykład firmy, która z powodzeniem wykorzystuje dane strumieniowe w celu dostarczania użytkownikom spersonalizowanych rekomendacji i analizowania ich zachowań. Dzięki technologii strumieniowej, Netflix monitoruje działania swoich użytkowników (co oglądają, jakie treści przewijają, jak długo oglądają), analizując te dane w czasie rzeczywistym. Na podstawie tej analizy Netflix jest w stanie dostarczać rekomendacje filmów i seriali w czasie rzeczywistym, dostosowując je do zmieniających się preferencji użytkownika."
  },
  {
    "objectID": "wyklad2.html#przykład-strumieniowego-przetwarzania-w-netflix",
    "href": "wyklad2.html#przykład-strumieniowego-przetwarzania-w-netflix",
    "title": "Wykład 2",
    "section": "Przykład strumieniowego przetwarzania w Netflix:",
    "text": "Przykład strumieniowego przetwarzania w Netflix:\nKiedy użytkownik zaczyna oglądać film, system śledzi jego reakcje (np. przerwanie filmu, przewijanie) i dostosowuje rekomendacje do jego preferencji. Takie dane mogą być przesyłane do systemu analitycznego za pomocą narzędzi takich jak Apache Kafka i przetwarzane na żywo w Apache Flink."
  },
  {
    "objectID": "wyklad2.html#podsumowanie",
    "href": "wyklad2.html#podsumowanie",
    "title": "Wykład 2",
    "section": "Podsumowanie",
    "text": "Podsumowanie\nDzięki ewolucji technologii, analiza danych przeszła długą drogę, od prostych tabel SQL, przez złożone dane grafowe i tekstowe, aż do analiz strumieniowych. Zrozumienie różnic między tymi podejściami pozwala na lepsze dobieranie narzędzi do odpowiednich typów analiz, w tym wykorzystania nowoczesnych systemów przetwarzania danych w czasie rzeczywistym, takich jak Kafka i Flink."
  },
  {
    "objectID": "wyklad2.html#ewolucja-analizy-danych-od-struktur-tabelarycznych-do-strumieni",
    "href": "wyklad2.html#ewolucja-analizy-danych-od-struktur-tabelarycznych-do-strumieni",
    "title": "Wykład 2",
    "section": "Ewolucja analizy danych: Od struktur tabelarycznych do strumieni",
    "text": "Ewolucja analizy danych: Od struktur tabelarycznych do strumieni\nRozwój technologii informatycznych stworzył nowe możliwości przetwarzania ogromnych ilości danych, zarówno ustrukturyzowanych, jak i nieustrukturyzowanych. W ciągu ostatnich kilku dekad obserwujemy wzrost dostępnych danych oraz technologii do ich przechowywania i analizy. Dzięki temu dane stały się jednym z najcenniejszych zasobów współczesnych organizacji."
  },
  {
    "objectID": "wyklad2.html#dane-ustrukturyzowane",
    "href": "wyklad2.html#dane-ustrukturyzowane",
    "title": "Wykład 2",
    "section": "Dane ustrukturyzowane",
    "text": "Dane ustrukturyzowane\nDane ustrukturyzowane to dane, które są przechowywane w sposób zorganizowany i jednoznacznie określony. Przykładami takich danych są tabele w bazach SQL, w których każda kolumna reprezentuje jedną cechę, a każdy wiersz to pojedynczy rekord. Tego typu dane są najczęściej wykorzystywane w klasycznych algorytmach uczenia maszynowego, takich jak regresja logistyczna, XGBoost czy modele klasyfikacji.\nPrzykład: Załóżmy, że mamy tabelę, w której każda linia przedstawia dane o kliencie: jego płeć, wzrost, ilość kredytów itd.\n\n\nCode\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nNa tej podstawie możemy przewidywać prawdopodobieństwo spłaty kredytu (regresja logistyczna). Takie przewidywanie również oznaczane jest jako cecha (ang. target).\n\n\nCode\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0"
  },
  {
    "objectID": "wyklad2.html#dane-nieustrukturyzowane",
    "href": "wyklad2.html#dane-nieustrukturyzowane",
    "title": "Wykład 2",
    "section": "Dane nieustrukturyzowane",
    "text": "Dane nieustrukturyzowane\nDane nieustrukturyzowane to dane, które nie mają określonej, tabelarycznej formy. Należą do nich obrazy, dźwięki, wideo i tekst. Choć te dane wydają się chaotyczne, są one również cennym źródłem informacji, które można przetwarzać za pomocą odpowiednich algorytmów.\nPrzykład: Analiza obrazów (np. weryfikacja, czy zdjęcie przedstawia psa czy kota) lub analizy tekstu (np. sentymentu w tweetach) są przykładami pracy z danymi nieustrukturyzowanymi.\n\nJakie wyzwania niesie analiza danych w różnych formach?\n\nZwiększający się wolumen danych, a także ich różnorodność, stawia przed nami kolejne wyzwania związane z przetwarzaniem i analizowaniem tych danych.\n\nKluczowe pytanie brzmi: jak efektywnie przetwarzać ogromne ilości danych w czasie rzeczywistym?"
  },
  {
    "objectID": "wyklad2.html#przetwarzanie-wsadowe-vs.-przetwarzanie-strumieniowe",
    "href": "wyklad2.html#przetwarzanie-wsadowe-vs.-przetwarzanie-strumieniowe",
    "title": "Wykład 2",
    "section": "Przetwarzanie wsadowe vs. przetwarzanie strumieniowe",
    "text": "Przetwarzanie wsadowe vs. przetwarzanie strumieniowe\nW tradycyjnych systemach analizy danych, takich jak bazy danych SQL, przetwarzanie danych odbywa się w trybie wsadowym (batch processing). Oznacza to, że dane są zbierane przez pewien czas, a następnie przetwarzane w partiach (np. raz dziennie, raz w tygodniu). Jest to podejście, które świetnie sprawdza się w analizach historycznych i podejmowaniu decyzji na podstawie dużych zbiorów danych.\n\nPrzykład batch processing:\nPrzetwarzanie wszystkich transakcji dokonanych przez klientów w ciągu dnia, aby na końcu dnia wyciągnąć raporty i wnioski dotyczące aktywności.\nNatomiast przetwarzanie strumieniowe (stream processing) pozwala na bieżące analizowanie danych w momencie ich napływania. Dzięki temu, dane mogą być przetwarzane i analizowane w czasie rzeczywistym, co jest niezwykle ważne w przypadku takich zastosowań jak wykrywanie nadużyć w czasie rzeczywistym, personalizowanie treści czy monitorowanie urządzeń IoT.\n\n\nPrzykład stream processing:\nSystem detekcji oszustw w kartach kredytowych, który monitoruje transakcje na żywo, analizując je pod kątem podejrzanych działań (np. przekroczenie typowego wzorca wydatków).\nWspółczesna analiza danych nie ogranicza się do prostych zbiorów danych w formie tabelarycznej. Przechodzimy od danych ustrukturyzowanych do zaawansowanego przetwarzania strumieniowego, które pozwala na podejmowanie decyzji w czasie rzeczywistym. Zrozumienie różnicy między przetwarzaniem wsadowym a strumieniowym to klucz do pracy z nowoczesnymi technologiami i narzędziami, które napędzają innowacje w wielu dziedzinach."
  },
  {
    "objectID": "wyklad2.html#źródła-danych",
    "href": "wyklad2.html#źródła-danych",
    "title": "Wykład 2",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\ndane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics)."
  },
  {
    "objectID": "wyklad2.html#architektura-systemów-real-time",
    "href": "wyklad2.html#architektura-systemów-real-time",
    "title": "Wykład 2",
    "section": "Architektura systemów real-time",
    "text": "Architektura systemów real-time\nWykorzystanie systemów real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, która będzie mogła szybko przetwarzać ogromne ilości danych oraz reagować na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, ponieważ umożliwia szybsze podejmowanie decyzji, monitorowanie procesów w czasie rzeczywistym i reagowanie na zdarzenia bez opóźnienia.\nOmówimy główne elementy architektury systemów real-time, popularne wzorce architektoniczne oraz technologie, które są wykorzystywane do budowy takich systemów.\n\nPodstawowe elementy systemu real-time\nSystemy real-time muszą spełniać szereg wymagań związanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentów w architekturze systemu, które zapewniają jego prawidłowe funkcjonowanie.\n\nProducent danych (Data Producer)\nDane w systemie real-time pochodzą z różnych źródeł, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urządzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z giełdy.\nDane użytkowników: np. logi użytkowników w aplikacjach mobilnych, dane z mediów społecznościowych.\n\n\n\nPrzesyłanie danych (Data Transport)\nDane muszą być szybko przesyłane do systemów, które mogą je analizować. W tym celu wykorzystywane są technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyłania danych w czasie rzeczywistym, zapewniający wysoką wydajność i niezawodność.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z dużą ilością subskrybentów.\n\n\n\nPrzetwarzanie danych (Data Processing)\nDane w systemach real-time są często przetwarzane w strumieniu. Dwa główne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, które może mieć opóźnienie, ale przetwarza dane w sposób efektywny. Może być wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opóźnień, w którym dane są natychmiastowo analizowane i przetwarzane.\n\n\n\nSkładowanie danych (Data Storage)\nPrzechowywanie danych w systemie real-time zależy od wymagań aplikacji. Dwa główne rodzaje przechowywania to:\n\nData Lake: składowanie ogromnych ilości nieprzetworzonych danych w postaci surowych plików. Bazy danych NoSQL: takie jak Cassandra, które umożliwiają szybki dostęp do danych w czasie rzeczywistym.\nData Warehouse: składowanie przetworzonych danych w celu ich analizy.\n\n\n\nAnaliza i wizualizacja danych (Data Analytics and Visualization)\nPo przetworzeniu danych w czasie rzeczywistym należy wykonać ich analizę i prezentację w sposób zrozumiały dla użytkownika:\n\nDashboardy: narzędzia takie jak Grafana lub Kibana, które służą do wizualizacji wyników w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmów uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\nPopularne architektury systemów real-time\n\nLambda Architecture\nLambda Architecture to popularna koncepcja przetwarzania danych, która łączy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura używana w systemach przetwarzania Big Data, która zakłada dwie warstwy:\n\nBatch Layer: przetwarzanie (dużych ilości) danych wsadowych, które są później wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napływające dane strumieniowe, np. z sensorów, social media, transakcji, w celu uzyskania natychmiastowych wyników.\nServing Layer: warstwa, która łączy wyniki obu poprzednich warstw i dostarcza je do użytkownika np. za pomocą API.\n\n \n\n\nZalety i Wady Lambda Architecture:\n\n✅ Możliwość łączenia przetwarzania wsadowego i strumieniowego,\n✅ wsparcie dla dużych zbiorów danych,\n✅ elastyczność w przetwarzaniu złożonych zapytań.\n❌ Wymaga utrzymywania dwóch oddzielnych systemów do przetwarzania danych (batch i stream), co prowadzi do złożoności implementacji i utrzymania.\n\n\n\nKappa Architecture\nKappa Architecture jest uproszczoną wersją Lambda Architecture. Zamiast używać dwóch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jedną warstwę przetwarzania strumieniowego, co upraszcza cały system.\nJest to bardziej elastyczne podejście do budowy systemów real-time, zwłaszcza w przypadku, gdy dane są przetwarzane tylko w jednym trybie (streaming).\n \n\n\nZalety i Wady Kappa Architecture:\n\n✅ Prostota: Jako że przetwarzanie danych odbywa się tylko w jednym strumieniu, cały system jest prostszy i bardziej spójny.\n✅ Skalowalność: Dzięki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekście analizy danych w czasie rzeczywistym.\n✅ Idealne dla ML: Kappa Architecture świetnie sprawdza się w zastosowaniach związanych z Machine Learning, ponieważ przetwarzanie danych odbywa się na bieżąco, co pozwala na szybsze uczenie i wdrażanie modeli ML w czasie rzeczywistym.\n❌ Może być mniej wydajna przy bardzo dużych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\nMicroservices Architecture\nArchitektura mikroserwisów jest powszechnie wykorzystywana w systemach real-time, ponieważ umożliwia:\n\nPodział aplikacji na mniejsze, autonomiczne jednostki.\nElastyczność i skalowalność systemu.\nMożliwość przetwarzania różnych rodzajów danych przez różne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomości.\n\n\n\nPrzykład\nUber to przykład firmy, która skutecznie wykorzystuje narzędzia do przetwarzania strumieniowego, by monitorować ruch drogowy w czasie rzeczywistym. Dzięki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdów oraz czasach oczekiwania na przejazd, które są następnie analizowane na żywo.\nDane wejściowe: Informacje o czasie i miejscu podróży, dane GPS z pojazdów, natężenie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyłania tych danych w czasie rzeczywistym do systemów takich jak Apache Flink lub Spark Streaming, które analizują je na bieżąco.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasę w czasie rzeczywistym.\nWynik: Użytkownicy Ubera otrzymują prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowców), co umożliwia optymalizację transportu."
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: Szkoła Główna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJęzyk prowadzenia: polski\nPoziom przedmiotu: średnio-zaawansowany\nProwadzący: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspółczesny biznes opiera się na podejmowaniu decyzji opartych na danych. Coraz większa ilość informacji, rosnące wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiają, że analiza danych w czasie rzeczywistym staje się kluczowym elementem nowoczesnych procesów biznesowych.\nNa zajęciach studenci zapoznają się z metodami i technologiami umożliwiającymi przetwarzanie danych w czasie rzeczywistym. Szczególną uwagę poświęcimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz głębokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretować zjawiska biznesowe, ale także podejmować szybkie i trafne decyzje.\nW ramach kursu omówimy zarówno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dźwięk, strumieniowanie wideo). Studenci poznają architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a także wyzwania związane z modelowaniem danych w czasie rzeczywistym na dużą skalę.\nKurs obejmuje część teoretyczną oraz praktyczne laboratoria, podczas których studenci będą pracować z rzeczywistymi danymi w środowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. Dzięki temu studenci nie tylko zdobędą wiedzę na temat metod analitycznych, ale także nauczą się korzystać z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plików płaskich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (Wykład)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametrów modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykładzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego środowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego środowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 1.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 2.\nPrzygotowanie środowiska Microsoft Azure. Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzędzia IT do szybkiej analizy logów.\nNarzędzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-kształcenia",
    "href": "sylabus.html#efekty-kształcenia",
    "title": "Sylabus",
    "section": "Efekty kształcenia",
    "text": "Efekty kształcenia\n\nWiedza:\n\n\nZna historię i filozofię modeli przetwarzania danych\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiązania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nUmie wybrać strukturę IT dla danego problemu biznesowego\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejętności:\n\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiązania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować, przetwarzać oraz zachowywać dane generowane w czasie rzeczywistym\n\nPowiązania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\nPowiązania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosować i skonstruować system do przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem\n\nPowiązania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiązania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1️⃣ Zając S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2️⃣ Frątczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3️⃣ Bellemare A., Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę, O’Reilly 2021.\n4️⃣ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, O’Reilly 2022.\n5️⃣ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. Rozwiązania typowych problemów dotyczących przygotowania danych, konstruowania modeli i MLOps, O’Reilly 2021.\n6️⃣ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, O’Reilly 2022.\n7️⃣ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly 2018.\n8️⃣ Sebastián Ramírez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostępna online).\n9️⃣ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\n🔟 Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, O’Reilly 2019."
  },
  {
    "objectID": "wyklad1.html",
    "href": "wyklad1.html",
    "title": "Wykład 1",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h\n🎯 Cel wykładu\nZapoznanie studentów z podstawami real-time analytics, różnicami między trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami."
  },
  {
    "objectID": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez konieczności przechowywania i oczekiwania na późniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wniosków i reakcji na zmieniające się warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) – dane są analizowane w ciągu milisekund lub sekund od ich wygenerowania.\nStreaming vs. Batch Processing – analiza danych może odbywać się w sposób ciągły (streaming) lub w z góry określonych interwałach (batch).\nIntegracja z IoT, AI i ML – real-time analytics często współpracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym – np. natychmiastowa detekcja oszustw w transakcjach bankowych."
  },
  {
    "objectID": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "Wykład 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowość\n\nWykrywanie oszustw – analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujących na oszustwa.\nAutomatyczny trading – systemy HFT (High-Frequency Trading) analizują miliony danych w ułamkach sekundy.\nDynamiczne oceny kredytowe – natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym – dynamiczne rekomendacje produktów na podstawie aktualnego zachowania użytkownika.\nDynamiczne ceny – np. Uber, Amazon i hotele stosują dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediów społecznościowych – analiza nastrojów klientów i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej – analiza logów w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystąpieniem.\nSmart Cities – analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji świetlnej.\nAnalityka IoT – urządzenia IoT generują strumienie danych, które można analizować w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentów – analiza sygnałów z urządzeń medycznych w celu natychmiastowego wykrycia zagrożenia życia.\nAnalityka epidemiologiczna – śledzenie rozprzestrzeniania się chorób na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemów informatycznych, który umożliwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branżach – od finansów, przez e-commerce, aż po ochronę zdrowia i IoT."
  },
  {
    "objectID": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "Wykład 1",
    "section": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstnieją trzy główne podejścia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKażde z nich różni się szybkością przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing – Przetwarzanie wsadowe\n📌 Definicja:\nBatch Processing polega na zbieraniu dużych ilości danych i ich przetwarzaniu w określonych odstępach czasu (np. co godzinę, codziennie, co tydzień).\n📌 Cechy:\n\n✅ Wysoka wydajność dla dużych zbiorów danych\n✅ Przetwarzanie danych po ich zgromadzeniu\n✅ Nie wymaga natychmiastowej analizy\n✅ Zwykle tańsze niż przetwarzanie w czasie rzeczywistym\n❌ Opóźnienia – wyniki są dostępne dopiero po zakończeniu przetwarzania\n\n📌 Przykłady zastosowań:\n\nGenerowanie raportów finansowych na koniec dnia/miesiąca\nAnaliza trendów sprzedaży na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\n📌 Przykładowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiąca\n\n# Agregacja danych - miesięczne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wyników do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyś chciał utworzyć dane do przykładu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics – Analiza niemal w czasie rzeczywistym\n📌 Definicja:\nNear Real-Time Analytics to analiza danych, która odbywa się z minimalnym opóźnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie pełna analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duże opóźnienia mogą wpłynąć na biznes.\n📌 Cechy:\n\n✅ Przetwarzanie danych w krótkich odstępach czasu (kilka sekund – minut)\n✅ Umożliwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\n✅ Optymalny balans między kosztami a szybkością\n❌ Nie nadaje się do systemów wymagających natychmiastowej reakcji\n\n📌 Przykłady zastosowań:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciągu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowań użytkowników\nAnaliza logów serwerów i sieci w celu wykrycia anomalii\n\n📌 Przykładowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logów IT)\nAmazon Kinesis\n\nPrzykład producenta danych realizującego tranzakcje wysyłane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generująca przykładowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota między 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# Zakończenie działania producenta\nproducer.flush()\nproducer.close()\nPrzykład consumenta - programu sparawdzającego zbyt duże transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\nPrzykładowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics – Analiza w czasie rzeczywistym\n📌 Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w ułamku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagających reakcji w czasie rzeczywistym, np. w transakcjach giełdowych, systemach IoT czy cyberbezpieczeństwie.\n📌 Cechy:\n\n✅ Bardzo niskie opóźnienie (milliseconds-seconds)\n✅ Umożliwia natychmiastową reakcję systemu\n✅ Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\n❌ Droższe i bardziej złożone technologicznie niż batch processing\n\n📌 Przykłady zastosowań:\n\nHigh-Frequency Trading (HFT) – analiza i podejmowanie decyzji w transakcjach giełdowych w milisekundach\nAutonomiczne samochody – analiza strumieni danych z kamer i sensorów w czasie rzeczywistym\nCyberbezpieczeństwo – detekcja ataków w sieciach komputerowych w ułamku sekundy\nAnalityka IoT – np. natychmiastowa detekcja anomalii w danych z czujników przemysłowych\n\n📌 Przykładowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\n🔎 Porównanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpóźnienie\nMinuty – godziny – dni\nSekundy – minuty\nMilisekundy – sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w pełni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\n📉 Niski\n📈 Średni\n📈📈 Wysoki\n\n\nZłożoność implementacji\n📉 Prosta\n📈 Średnia\n📈📈 Trudna\n\n\nPrzykłady zastosowań\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\n📌 Kiedy stosować Batch Processing?\n\n✅ Gdy nie wymagasz natychmiastowej analizy\n✅ Gdy masz duże ilości danych, ale przetwarzane są one okresowo\n✅ Gdy chcesz obniżyć koszty\n\n📌 Kiedy stosować Near Real-Time Analytics?\n\n✅ Gdy wymagasz analizy w krótkim czasie (sekundy – minuty)\n✅ Gdy potrzebujesz bardziej aktualnych danych, ale nie w pełnym real-time\n✅ Gdy szukasz kompromisu między wydajnością a kosztami\n\n📌 Kiedy stosować Real-Time Analytics?\n\n✅ Gdy każda milisekunda ma znaczenie (np. giełda, autonomiczne pojazdy)\n✅ Gdy chcesz wykrywać oszustwa, anomalie lub incydenty natychmiast\n✅ Gdy system musi natychmiast reagować na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne – w wielu przypadkach near real-time jest wystarczające i bardziej opłacalne. Kluczowe jest zrozumienie wymagań biznesowych przed wyborem odpowiedniego rozwiązania."
  },
  {
    "objectID": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "href": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "title": "Wykład 1",
    "section": "Dlaczego Real-Time Analytics jest ważne?",
    "text": "Dlaczego Real-Time Analytics jest ważne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje się coraz bardziej istotna w wielu branżach, ponieważ umożliwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodów, dla których real-time analytics jest ważne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagować na zmiany i wydarzenia w czasie rzeczywistym. Dzięki temu można podejmować decyzje szybciej, co jest kluczowe w dynamicznych środowiskach, takich jak:\n\nMarketing: Reklamy mogą być dostosowane do zachowań użytkowników w czasie rzeczywistym (np. personalizacja treści reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie każda minuta może oznaczać różnicę w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogą monitorować kluczowe wskaźniki operacyjne na bieżąco. Przykłady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urządzeń w fabrykach, aby natychmiast wykrywać awarie i zapobiegać przestojom.\nHealthtech: Śledzenie parametrów życiowych pacjentów i wykrywanie anomalii, co może ratować życie.\n\n\n\nZwiększenie efektywności operacyjnej\nReal-time analytics umożliwia natychmiastowe wykrywanie i eliminowanie problemów operacyjnych, zanim staną się poważniejsze. Przykłady:\n\nLogistyka: Śledzenie przesyłek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywność i zmniejsza opóźnienia.\nRetail: Monitorowanie poziomu zapasów na bieżąco i dostosowywanie zamówień do aktualnych potrzeb.\n\n\n\nKonkurencyjność\nOrganizacje, które wykorzystują analitykę w czasie rzeczywistym, mają przewagę nad konkurencją, ponieważ mogą szybciej reagować na zmiany na rynku, nowe potrzeby klientów i sytuacje kryzysowe. Dzięki natychmiastowym informacjom:\n\nMożna podejmować decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywać lepsze relacje z klientami, reagując na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doświadczenia użytkowników (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z użytkownikami w trakcie ich trwania. Przykłady:\n\nE-commerce: Analiza koszyka zakupowego użytkownika w czasie rzeczywistym, aby np. zaoferować rabat lub przypomnieć o porzuconych produktach.\nStreaming: Optymalizacja jakości usługi wideo/streamingowej w zależności od dostępnej przepustowości łącza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym świecie pełnym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeństwa. Przykłady:\n\nCyberbezpieczeństwo: Real-time analytics umożliwia wykrywanie podejrzanych działań w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztów\nDzięki analizie w czasie rzeczywistym można optymalizować zasoby i zmniejszać koszty. Na przykład:\n\nZarządzanie energią: Analiza zużycia energii w czasie rzeczywistym, umożliwiająca optymalizację wydatków na energię w firmach.\nOptymalizacja łańcucha dostaw: Dzięki bieżącemu śledzeniu zapasów i dostaw można lepiej zarządzać kosztami magazynowania i transportu.\n\n\n\nZdolność do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, które mogą przewidywać przyszłe zachowania lub problemy, a także je eliminować zanim się pojawią. Na przykład:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w połączeniu z modelami predykcyjnymi pozwala przewidywać awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym można dostosowywać produkcję lub zapasy na podstawie bieżących trendów.\n\nReal-time analytics to nie tylko analiza danych – to kluczowy element strategii firm w świecie, który wymaga szybkich reakcji, elastyczności i dostosowywania się do zmieniającego się otoczenia. Firmy, które wdrażają te technologie, mogą znacząco poprawić swoje wyniki finansowe, obsługę klienta, wydajność operacyjną, a także przewagę konkurencyjną."
  },
  {
    "objectID": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiąże się z wieloma wyzwaniami i trudnościami, które trzeba rozwiązać, aby systemy real-time działały efektywnie i niezawodnie. Pomimo ogromnego potencjału, jaki daje możliwość natychmiastowego przetwarzania danych, realizacja tych procesów w praktyce wiąże się z licznymi problemami technologicznymi, organizacyjnymi i dotyczącymi zarządzania danymi.\nPoniżej przedstawiamy najważniejsze wyzwania oraz możliwe rozwiązania, które należy uwzględnić podczas implementacji systemów analizy danych w czasie rzeczywistym.\n\nSkalowalność systemów\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadań. W miarę jak ilość generowanych danych rośnie, systemy muszą być w stanie obsługiwać większe obciążenie bez opóźnienia w przetwarzaniu.\nZwiększona ilość danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, ilość generowanych danych może być olbrzymia. Potrzebna jest elastyczność: System musi automatycznie dostosowywać zasoby w zależności od obciążenia.\n\n\nRozwiązanie:\nWykorzystanie skalowalnych systemów chmurowych, które pozwalają na dynamiczne zwiększanie zasobów obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarządzania kontenerami i automatycznego skalowania mikroserwisów. Technologie strumieniowe (Apache Kafka, Apache Flink) umożliwiające przetwarzanie danych w sposób wydajny i rozproszony.\n\n\n\nOpóźnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, każde opóźnienie w przetwarzaniu danych może mieć poważne konsekwencje. Dotyczy to zwłaszcza obszarów takich jak:\nWykrywanie oszustw: W przypadku systemów płatności online, opóźnienie w analizie transakcji może oznaczać przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentów: Opóźnienia mogą wpłynąć na skuteczność reakcji w sytuacjach kryzysowych.\n\n\nRozwiązanie:\nUżywanie algorytmów optymalizujących czas przetwarzania, np. stream processing z wykorzystaniem systemów takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliżej źródła (np. urządzenia IoT), aby zmniejszyć opóźnienia w transmisji danych do chmury.\n\n\n\nJakość danych i zarządzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizować dane w czasie rzeczywistym, ale także zapewnić ich wysoką jakość. W przeciwnym razie analizy mogą prowadzić do błędnych wniosków lub opóźnień w reagowaniu na nieprawidłowe dane.\nZanieczyszczone dane: W systemach real-time dane często są niepełne, brudne, błędne lub nieuporządkowane. Zmiana charakterystyki danych: Dane mogą zmieniać się w czasie, co może utrudniać ich przetwarzanie i analizę. #### Rozwiązanie:\nData cleansing i data validation na wstępnym etapie procesu. Automatyczne systemy monitorowania jakości danych w celu wykrywania błędów w czasie rzeczywistym. Zarządzanie danymi w strumieniu: Narzędzia takie jak Apache Kafka pozwalają na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZłożoność integracji systemów\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym często muszą współpracować z istniejącymi systemami IT i źródłami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemów, zwłaszcza w rozproszonej architekturze, może być skomplikowana.\n\n\nRozwiązanie:\nUżywanie API do łatwiejszej integracji z zewnętrznymi systemami. Mikroserwisy i konteneryzacja z pomocą narzędzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, które umożliwia łatwą integrację różnych źródeł danych oraz zapewnia elastyczność w dostosowywaniu systemów do rosnących potrzeb.\n\n\n\nBezpieczeństwo i prywatność\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiąże się z ogromną ilością wrażliwych informacji, szczególnie w branżach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, że dane są odpowiednio chronione przed nieautoryzowanym dostępem, jest kluczowe.\nOchrona danych w czasie transmisji: Muszą być szyfrowane zarówno podczas przesyłania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym może być celem ataków, takich jak DDoS czy SQL injection.\n\n\nRozwiązanie:\nSzyfrowanie danych zarówno w spoczynku, jak i podczas przesyłania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeństwa. Zgodność z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarządzanie błędami i awariami\n\nWyzwanie:\nBłędy i awarie w systemach real-time mogą prowadzić do poważnych konsekwencji, w tym utraty danych, opóźnień w analizach czy nawet usunięcia usług. W systemach rozproszonych trudno jest osiągnąć pełną niezawodność.\n\n\nRozwiązanie:\nRedundancja: Tworzenie kopii zapasowych systemów i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), które pozwalają na szybkie wykrycie i naprawienie problemów. Zarządzanie stanem: Dzięki użyciu narzędzi jak Apache Kafka, można ponownie przetwarzać dane, jeśli wystąpił błąd w transmisji.\n\n\n\nKoszty związane z infrastrukturą\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, która zapewni odpowiednią moc obliczeniową i pamięć. To może wiązać się z dużymi kosztami, szczególnie gdy dane muszą być przechowywane i przetwarzane w czasie rzeczywistym na dużą skalę.\n\n\nRozwiązanie:\nChmura obliczeniowa: Możliwość elastycznego skalowania zasobów w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalają na uruchamianie procesów bez potrzeby utrzymywania stałej infrastruktury.\nChociaż analiza danych w czasie rzeczywistym oferuje ogromne korzyści, wiąże się także z wieloma wyzwaniami. Właściwa architektura, narzędzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogą pomóc w przezwyciężeniu wielu z tych trudności. Warto również pamiętać o konieczności zapewnienia wysokiej jakości danych, ich bezpieczeństwa, a także elastyczności i skalowalności systemów, które będą w stanie sprostać rosnącym wymaganiom."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli VI bud G\n\n\n18-02-2025 (wtorek) 13:30-15:10 - Wykład 1\n\n25-02-2025 (wtorek) 13:30-15:10 - Wykład 2\n04-03-2025 (wtorek) 13:30-15:10 - Wykład 3\n11-03-2025 (wtorek) 13:30-15:10 - Wykład 4\n18-03-2025 (wtorek) 13:30-15:10 - Wykład 5\n\nWykład 5 kończy się TESTEM: 20 pytań - 30 minut. Test przeprowadzany jest za pośrednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć).\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "plan_wyklady.html",
    "href": "plan_wyklady.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#plan-wykładu",
    "href": "plan_wyklady.html#plan-wykładu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#moje",
    "href": "plan_wyklady.html#moje",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Moje",
    "text": "Moje\n\nwprowadzenie\nBatch processing\n\n\ntypy danych\nBig data\nETL\nMAP Reduce\nSparkowe przetwarzanie klastrowe\nBazy SQL - OLTP, OLAP\n\n\nAPI online\n\n\nwystawienie serwisu LLM\nbatching\n\n\nNear Real-Time i Real Time\n\n\nStrumienie danych, definicje, biznes,\nLambda/Kappa\nPub Sub, Kafka"
  }
]