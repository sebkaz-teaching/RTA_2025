[
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Lecture 1: Introduction to Real-Time Data Analysis\nLecture 2: Data Ingestion and Processing for Real-Time Analysis\nLecture 3: Real-Time Data Analysis Techniques\nLecture 4: Real-Time Data Visualization and Communication\nLecture 5: Case Studies and Implementation\nThroughout the lectures, Iâ€™ll incorporate interactive elements, such as:\nThis comprehensive lecture plan will provide students with a solid foundation in real-time data analysis, covering the technical aspects of data ingestion, processing, and visualization, as well as practical applications and best practices."
  },
  {
    "objectID": "plan.html#nowy-program-przedmiotu",
    "href": "plan.html#nowy-program-przedmiotu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Nowy program przedmiotu",
    "text": "Nowy program przedmiotu\n\nBatch vs.Â Real-Time vs.Â Streaming Analytics â€“ RÃ³Å¼nice miÄ™dzy trybami przetwarzania danych, kluczowe koncepcje i zastosowania.\nModele przetwarzania danych w Big Data â€“ Od plikÃ³w pÅ‚askich do Data Lake, wady i zalety podejÅ›cia real-time. Mity i fakty o przetwarzaniu w czasie rzeczywistym.\nArchitektura IT dla przetwarzania w czasie rzeczywistym â€“ OmÃ³wienie architektur Lambda i Kappa w kontekÅ›cie strumieniowego przetwarzania danych.\nSystemy przetwarzania danych w czasie rzeczywistym â€“ PrzeglÄ…d technologii: Apache Kafka, Apache Spark Streaming, Apache Flink i ich zastosowania w Pythonie.\nPodstawy uczenia maszynowego w czasie rzeczywistym â€“ PorÃ³wnanie offline learning vs.Â online learning, problemy zwiÄ…zane z przyrostowym uczeniem maszynowym.\n\nğŸ§‘â€ğŸ« WykÅ‚ady (teoria + case study z biznesu)\n1ï¸âƒ£ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs.Â nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs.Â strumieniowe (stream processing). Case study: Jak firmy przechodzÄ… od tabel do analizy strumieniowej?\n2ï¸âƒ£ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture â€“ rÃ³Å¼nice i zastosowania. Case study: Rekomendacje produktowe w e-commerce.\n3ï¸âƒ£ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs.Â przyrostowe (online learning). Stochastic Gradient Descent (SGD) â€“ podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym.\n4ï¸âƒ£ Obiektowe programowanie w Pythonie w kontekÅ›cie ML Struktury klasowe dla modeli ML. Tworzenie pipelineâ€™Ã³w ML w Pythonie. Case study: Klasyfikacja wiadomoÅ›ci jako SPAM/NON-SPAM w strumieniu tekstÃ³w.\n5ï¸âƒ£ Tworzenie API z reguÅ‚ami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych."
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli III bud G\n\n\n22-02-2025 (sobota) 08:00-09:30 - WykÅ‚ad 1\n\n\n08-03-2025 (sobota) 08:00-09:30 - WykÅ‚ad 2\n\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡) 20 pytaÅ„.\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "wyklad5.html",
    "href": "wyklad5.html",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n\nAlgorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\n\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania: - Analiza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS). - Monitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\nAlgorytmy dokonujÄ…ce wielu obliczeÅ„\n\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA) - Algorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA. - Proces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych. - CzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\nğŸ”¹ Inne zastosowania: - Symulacje fizyczne (np. prognozowanie pogody, modele klimatyczne). - Algorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\nAlgorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\n\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch) - Algorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym. - Wymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych. - MoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\nğŸ”¹ Inne zastosowania: - Autonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym). - Wyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia."
  },
  {
    "objectID": "wyklad5.html#algorytmy",
    "href": "wyklad5.html#algorytmy",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n\nAlgorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\n\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania: - Analiza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS). - Monitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\nAlgorytmy dokonujÄ…ce wielu obliczeÅ„\n\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA) - Algorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA. - Proces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych. - CzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\nğŸ”¹ Inne zastosowania: - Symulacje fizyczne (np. prognozowanie pogody, modele klimatyczne). - Algorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\nAlgorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\n\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch) - Algorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym. - Wymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych. - MoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\nğŸ”¹ Inne zastosowania: - Autonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym). - Wyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia."
  },
  {
    "objectID": "wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "href": "wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "title": "WykÅ‚ad 5",
    "section": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w",
    "text": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w\nW wielu przypadkach modelowanie jest wykorzystywane w sytuacjach krytycznych, np. w oprogramowaniu do podawania lekÃ³w. W takich sytuacjach kluczowe staje siÄ™ wyjaÅ›nienie przyczyny kaÅ¼dego wyniku dziaÅ‚ania algorytmu. Jest to konieczne, aby zapewniÄ‡, Å¼e decyzje podejmowane na jego podstawie sÄ… wolne od bÅ‚Ä™dÃ³w i uprzedzeÅ„.\nZdolnoÅ›Ä‡ algorytmu do wskazania mechanizmÃ³w generujÄ…cych wyniki nazywamy moÅ¼liwoÅ›ciÄ… wyjaÅ›nienia. Analiza etyczna stanowi standardowy element procesu walidacji algorytmu.\nUzyskanie wysokiej wyjaÅ›nialnoÅ›ci jest szczegÃ³lnie trudne w przypadku algorytmÃ³w uczenia maszynowego (ML) i gÅ‚Ä™bokiego uczenia (DL). Na przykÅ‚ad banki korzystajÄ…ce z algorytmÃ³w do podejmowania decyzji kredytowych muszÄ… zapewniÄ‡ transparentnoÅ›Ä‡ i wskazaÄ‡ powody wydanej decyzji.\nJednÄ… z metod poprawy wyjaÅ›nialnoÅ›ci algorytmÃ³w jest LIME (Local Interpretable Model-Agnostic Explanations), opublikowana w 2016 roku. Metoda ta polega na wprowadzaniu niewielkich zmian w danych wejÅ›ciowych i analizowaniu ich wpÅ‚ywu na wynik, co pozwala okreÅ›liÄ‡ lokalne zasady podejmowania decyzji przez model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# PodziaÅ‚ na zbiÃ³r treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykÅ‚adu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # WybÃ³r losowego przykÅ‚adu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# WyÅ›wietlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nJak dziaÅ‚a ten kod? 1. Åadowanie danych i trenowanie modelu - UÅ¼ywamy zbioru Iris, ktÃ³ry zawiera 150 przykÅ‚adÃ³w kwiatÃ³w z trzema gatunkami: - Setosa - Versicolor - Virginica - Model RandomForestClassifier trenuje siÄ™ na tych danych.\n\nTworzenie interpretowalnego modelu za pomocÄ… LIME\n\nLIME generuje lokalne wyjaÅ›nienia, czyli interpretuje model dla pojedynczych predykcji.\nWybieramy losowy przykÅ‚ad z danych testowych.\n\nEksploracja wyniku dla jednego przykÅ‚adu\n\nLIME modyfikuje lekko wartoÅ›ci wejÅ›ciowe i obserwuje, jak zmienia siÄ™ wynik predykcji.\nTworzy â€lokalnyâ€ model liniowy, ktÃ³ry pokazuje, ktÃ³re cechy miaÅ‚y najwiÄ™kszy wpÅ‚yw na decyzjÄ™.\n\n\nZaÅ‚Ã³Å¼my, Å¼e nasz model wybraÅ‚ przykÅ‚adowÄ… roÅ›linÄ™ i sklasyfikowaÅ‚ jÄ… jako Virginica.\nOto interpretacja wynikÃ³w: 1. NajwaÅ¼niejsze cechy wpÅ‚ywajÄ…ce na decyzjÄ™ modelu: - DÅ‚ugoÅ›Ä‡ pÅ‚atka (petal length): najwiÄ™kszy wpÅ‚yw na predykcjÄ™ (np. im wiÄ™ksza, tym wiÄ™ksze prawdopodobieÅ„stwo, Å¼e to Virginica). - SzerokoÅ›Ä‡ pÅ‚atka (petal width): rÃ³wnieÅ¼ istotny czynnik (np. powyÅ¼ej pewnej wartoÅ›ci sugeruje Virginica). - DÅ‚ugoÅ›Ä‡ kielicha (sepal length): mniejszy wpÅ‚yw, ale nadal istotny. - SzerokoÅ›Ä‡ kielicha (sepal width): zwykle najmniej istotna cecha.\n\nWizualizacja wynikÃ³w\n\nLIME generuje wykres sÅ‚upkowy, ktÃ³ry pokazuje wpÅ‚yw kaÅ¼dej cechy na klasyfikacjÄ™.\nNa wykresie widaÄ‡, ktÃ³re cechy zwiÄ™kszaÅ‚y, a ktÃ³re zmniejszaÅ‚y prawdopodobieÅ„stwo przypisania do danej klasy.\n\nCo oznacza wynik?\n\nJeÅ›li model przewidziaÅ‚ klasÄ™ Virginica z wysokim prawdopodobieÅ„stwem, oznacza to, Å¼e kluczowe cechy (np. dÅ‚ugi pÅ‚atek) mocno wskazujÄ… na ten gatunek.\nJeÅ›li cechy miaÅ‚y zrÃ³Å¼nicowany wpÅ‚yw, oznacza to, Å¼e model miaÅ‚ pewne trudnoÅ›ci w klasyfikacji (np. szerokoÅ›Ä‡ pÅ‚atka nie byÅ‚a jednoznaczna)."
  },
  {
    "objectID": "wyklad5.html#detekcja-anomalii",
    "href": "wyklad5.html#detekcja-anomalii",
    "title": "WykÅ‚ad 5",
    "section": "Detekcja anomalii",
    "text": "Detekcja anomalii\n\nWartoÅ›Ä‡ odstajÄ…ca (Outlier)\nWartoÅ›Ä‡ odstajÄ…ca (ang. outlier) to obserwacja (wiersz w tabeli danych), ktÃ³ra jest znacznie oddalona od pozostaÅ‚ych elementÃ³w prÃ³bki. Oznacza to, Å¼e zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi niezaleÅ¼nymi i zaleÅ¼nymi dla tej obserwacji moÅ¼e rÃ³Å¼niÄ‡ siÄ™ od pozostaÅ‚ych przypadkÃ³w.\nDla pojedynczych zmiennych wartoÅ›ci odstajÄ…ce moÅ¼na okreÅ›liÄ‡, korzystajÄ…c z wykresu pudeÅ‚kowego (box plot). Wykres ten bazuje na kwartylach:\n\nPierwszy kwartyl ( Q_1 ) i trzeci kwartyl ( Q_3 ) wyznaczajÄ… boki pudeÅ‚ka,\nDrugi kwartyl ( Q_2 ) (mediana) jest zaznaczony wewnÄ…trz pudeÅ‚ka,\n\nWartoÅ›ci odstajÄ…ce speÅ‚niajÄ… zaleÅ¼noÅ›Ä‡:\n[ x_{out} &lt; Q_1 - 1.5 IQR x_{out} &gt; Q_3 + 1.5 IQR ]\nGdzie: [ IQR = Q_3 - Q_1 ]\nPrzykÅ‚adem wartoÅ›ci odstajÄ…cej moÅ¼e byÄ‡ bolid FormuÅ‚y 1 â€“ pod wzglÄ™dem prÄ™dkoÅ›ci jest on anomaliÄ… wÅ›rÃ³d zwykÅ‚ych samochodÃ³w.\n\n\nWykorzystanie detekcji anomalii\nWykrywanie wartoÅ›ci odstajÄ…cych ma szerokie zastosowanie, np.: - Finanse â€“ wykrywanie transakcji fraudowych w analizie danych bankowych, - CyberbezpieczeÅ„stwo â€“ identyfikacja intruzÃ³w w sieci na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w, - Medycyna â€“ monitorowanie parametrÃ³w zdrowotnych i wykrywanie nieprawidÅ‚owoÅ›ci, - PrzemysÅ‚ â€“ wykrywanie wadliwych komponentÃ³w poprzez analizÄ™ obrazu.\n\n\nMetody wykrywania anomalii\n\n1. Metody nadzorowane (supervised learning)\nStosowane, gdy mamy oznaczone dane (np. przypadki oszustw w transakcjach). - Sieci neuronowe, - Algorytm K-najbliÅ¼szych sÄ…siadÃ³w (KNN), - Sieci Bayesowskie.\n\n\n2. Metody nienadzorowane (unsupervised learning)\nZakÅ‚adajÄ…, Å¼e wiÄ™kszoÅ›Ä‡ danych jest poprawna, a anomalie to niewielki odsetek przypadkÃ³w. - Klasteryzacja metodÄ… K-Å›rednich (K-Means), - Autoenkodery w sieciach neuronowych, - Testy statystyczne.\n\n\n\nMetoda klasyczna â€“ detekcja na podstawie prawdopodobieÅ„stwa\nAby okreÅ›liÄ‡, czy dana obserwacja jest anomaliÄ…, moÅ¼na uÅ¼yÄ‡ prawdopodobieÅ„stwa ( p(x) ):\n\nJeÅ›li ( p(x) &lt; ), uznajemy wartoÅ›Ä‡ za odstajÄ…cÄ….\nW praktyce zakÅ‚adamy, Å¼e dane majÄ… rozkÅ‚ad normalny ( N(, ) ).\nSzacujemy parametry ( ) (Å›rednia) i ( ^2 ) (wariancja) na podstawie prÃ³bki.\nNastÄ™pnie dla kaÅ¼dej wartoÅ›ci obliczamy prawdopodobieÅ„stwo jej wystÄ…pienia i porÃ³wnujemy z ( ).\n\nPrzykÅ‚ad: Analiza wynagrodzeÅ„ w firmie\nWykrywamy, czy w danej firmie sÄ… osoby o wynagrodzeniach znacznie odbiegajÄ…cych od Å›redniej.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PrzykÅ‚adowe wynagrodzenia w firmie (w tysiÄ…cach)\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\n# Obliczenie kwartylÃ³w\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\n# Definicja wartoÅ›ci odstajÄ…cych\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\n# Znalezienie outlierÃ³w\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\n# Wizualizacja\nsns.boxplot(salaries)\nplt.title(\"Wykres pudeÅ‚kowy wynagrodzeÅ„\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nWynik: Na wykresie pudeÅ‚kowym widaÄ‡, Å¼e 150 tys. to anomalia.\n\n\nIsolation Forest â€“ detekcja anomalii za pomocÄ… lasu izolacyjnego\nIsolation Forest to algorytm bazujÄ…cy na drzewach decyzyjnych, zaproponowany przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku. Identyfikuje anomalie poprzez izolowanie wartoÅ›ci odstajÄ…cych w procesie podziaÅ‚u danych:\n\nWybiera losowo cechÄ™ oraz wartoÅ›Ä‡ podziaÅ‚u,\nWartoÅ›ci odstajÄ…ce szybciej zostajÄ… odizolowane (sÄ… bliÅ¼ej korzenia drzewa),\nWynik jest agregowany na podstawie wielu drzew.\n\nJego zalety to niskie wymagania obliczeniowe i skutecznoÅ›Ä‡ w analizie wielowymiarowych danych.\nMetody detekcji anomalii sklearn\nPrzykÅ‚ad: Wykrywanie oszustw bankowych\nBank analizuje transakcje kartÄ… kredytowÄ… i wykrywa te, ktÃ³re mogÄ… byÄ‡ nieautoryzowane.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# PrzykÅ‚adowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duÅ¼a kwota, rzadkoÅ›Ä‡)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nWynik: Transakcja 5000 zÅ‚ zostanie wykryta jako anomalia."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "href": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z serwisu GitHub",
    "text": "Zacznij korzystaÄ‡ z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystaÄ‡ z serwisu github\nPracujÄ…c nad projektem np. praca magisterska, (samodzielnie lub w zespole) czÄ™sto potrzebujesz sprawdziÄ‡ jakie zmiany, kiedy i przez kogo zostaÅ‚y wprowadzone do projektu. W zadaniu tym Å›wietnie sprawdza siÄ™ system kontroli wersji czyli GIT.\nGit moÅ¼esz pobraÄ‡ i zainstalowaÄ‡ jak zwykÅ‚y program na dowolnym komputerze. Jednak najczÄ™Å›ciej (maÅ‚e projekty) korzysta siÄ™ z serwisÃ³w z jakimÅ› systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dziÄ™ki ktÃ³remu moÅ¼esz korzystaÄ‡ z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki moÅ¼esz przechowywaÄ‡ w publicznych (dostÄ™p majÄ… wszyscy) repozytoriach.\nSkupimy siÄ™ wyÅ‚Ä…cznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyÅ¼szym poziomie znajdujÄ… siÄ™ konta indywidualne np http://github.com/sebkaz, bÄ…dÅº zakÅ‚adane przez organizacje. UÅ¼ytkownicy indywidualni mogÄ… tworzyÄ‡ repozytoria publiczne (public ) bÄ…dÅº prywatne (private).\nJeden plik nie powinien przekraczaÄ‡ 100 MB.\nRepo (skrÃ³t do repozytorium) tworzymy za pomocÄ… Create a new repository. KaÅ¼de repo powinno mieÄ‡ swojÄ… indywidualnÄ… nazwÄ™.\n\n\nBranche\nGÅ‚Ã³wna (tworzona domyÅ›lnie) gaÅ‚Ä…Åº rapozytorium ma nazwÄ™ master.\n\n\nNajwaÅ¼niejsze polecnia do zapamiÄ™tania\n\nÅ›ciÄ…ganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba moÅ¼esz pobraÄ‡ repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejÅ›cie do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawiÄ‡ siÄ™ ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPoÅ‚Ä…cz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsÅ‚uga w 3 krokach\n\n# sprawdÅº zmiany jakie zostaÅ‚y dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzÄ…cy stan wraz z informacjÄ… co zrobiÅ‚eÅ›\ngit commit -m \" opis \"\n# 3. potem juÅ¼ zostaje tylko\ngit push origin master\nWarto obejrzeÄ‡ Youtube course.\nCiekawe i proste wprowadzenie mozna znaleÅºÄ‡ tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "href": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z Dockera",
    "text": "Zacznij korzystaÄ‡ z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swÃ³j system przejdÅº do strony.\nJeÅ¼li wszystko zainstalowaÅ‚o siÄ™ prawidÅ‚owo wykonaj nastÄ™pujÄ…ce polecenia:\n\nSprawdÅº zainstalowanÄ… wersjÄ™\n\ndocker --version\n\nÅšciÄ…gnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzeglÄ…d Å›ciÄ…gnietych obrazÃ³w:\n\ndocker image ls\n\ndocker images\n\nPrzeglÄ…d uruchomionych kontenerÃ³w:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsuniÄ™cie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam rÃ³wnieÅ¼ krÃ³tkie intro"
  },
  {
    "objectID": "wyklad3.html",
    "href": "wyklad3.html",
    "title": "WykÅ‚ad 3",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, podstawowych sposobÃ³w przetwarzania i analizowania danych strumieniowych."
  },
  {
    "objectID": "wyklad3.html#definicje",
    "href": "wyklad3.html#definicje",
    "title": "WykÅ‚ad 3",
    "section": "Definicje",
    "text": "Definicje\n\nZapoznaj siÄ™ z tematem danych strumieniowych\n\nDefinicja 1 â€“ Zdarzenie to wszystko, co moÅ¼na zaobserwowaÄ‡ w danym momencie czasu. Jest generowane jako bezpoÅ›redni skutek dziaÅ‚ania.\nDefinicja 2 â€“ W kontekÅ›cie danych zdarzenie to niezmienialny rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.\nDefinicja 3 â€“ CiÄ…gÅ‚y strumieÅ„ zdarzeÅ„ to nieskoÅ„czony zbiÃ³r pojedynczych zdarzeÅ„ uporzÄ…dkowanych w czasie, np. logi z urzÄ…dzeÅ„.\nDefinicja 4 â€“ StrumieÅ„ danych to dane tworzone przyrostowo w czasie, generowane ze ÅºrÃ³deÅ‚ statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).\nPrzedsiÄ™biorstwo to organizacja, ktÃ³ra generuje i odpowiada na ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„.\n\n\n\n\nAnalityka strumieniowa\nAnalityka strumieniowa (ang. stream analytics) nazywana jest rÃ³wnieÅ¼ przetwarzaniem strumieniowym zdarzeÅ„ (ang. event stream processing) â€“ czyli przetwarzaniem duÅ¼ych iloÅ›ci danych juÅ¼ na etapie ich generowania.\nNiezaleÅ¼nie od zastosowanej technologii, wszystkie dane powstajÄ… jako ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„ â€“ obejmuje to m.in.:\n- dziaÅ‚ania uÅ¼ytkownikÃ³w na stronach internetowych,\n- logi systemowe,\n- pomiary z sensorÃ³w."
  },
  {
    "objectID": "wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 3",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma Å¼adnego zwiÄ…zku z momentem wystÄ…pienia analizowanych zdarzeÅ„.\nNatomiast w przetwarzaniu strumieniowym wyrÃ³Å¼niamy dwie koncepcje czasu: 1. Czas zdarzenia (event time) â€“ moment, w ktÃ³rym zdarzenie faktycznie miaÅ‚o miejsce. 2. Czas przetwarzania (processing time) â€“ moment, w ktÃ³rym system przetwarza zdarzenie.\nIdealne przetwarzanie danych\nW idealnej sytuacji przetwarzanie nastÄ™puje natychmiast po wystÄ…pieniu zdarzenia:\n\nRzeczywiste przetwarzanie danych\nW praktyce przetwarzanie danych zawsze odbywa siÄ™ z pewnym opÃ³Åºnieniem, co jest widoczne jako punkty poniÅ¼ej linii idealnego przetwarzania (poniÅ¼ej przekÄ…tnej na wykresie).\n\nW aplikacjach przetwarzania strumieniowego istotna jest rÃ³Å¼nica miÄ™dzy czasem powstania zdarzenia a czasem jego przetwarzania. Do najczÄ™stszych przyczyn opÃ³ÅºnieÅ„ naleÅ¼Ä…:\n\nprzesyÅ‚anie danych przez sieÄ‡,\nbrak komunikacji miÄ™dzy urzÄ…dzeniem a sieciÄ….\n\nPrzykÅ‚adem jest Å›ledzenie poÅ‚oÅ¼enia samochodu przez aplikacjÄ™ GPS â€“ przejazd przez tunel moÅ¼e spowodowaÄ‡ chwilowÄ… utratÄ™ danych.\nObsÅ‚uga opÃ³ÅºnieÅ„ w przetwarzaniu strumieniowym\nOpÃ³Åºnienia w przetwarzaniu zdarzeÅ„ moÅ¼na obsÅ‚uÅ¼yÄ‡ na dwa sposoby: 1. Monitorowanie liczby pominiÄ™tych zdarzeÅ„ i wyzwalanie alarmu w przypadku zbyt duÅ¼ej liczby odrzuceÅ„. 2. Zastosowanie korekty za pomocÄ… watermarkingu, czyli dodatkowego mechanizmu uwzglÄ™dniajÄ…cego opÃ³Åºnione zdarzenia.\nProces przetwarzania zdarzeÅ„ w czasie rzeczywistym moÅ¼na przedstawiÄ‡ jako funkcjÄ™ schodkowÄ…:\n\nNie wszystkie zdarzenia wnoszÄ… wkÅ‚ad do analizy â€“ niektÃ³re mogÄ… zostaÄ‡ odrzucone ze wzglÄ™du na zbyt duÅ¼e opÃ³Åºnienie.\nWykorzystanie watermarkingu pozwala na uwzglÄ™dnienie dodatkowego czasu na pojawienie siÄ™ opÃ³Åºnionych zdarzeÅ„. Proces ten obejmuje wszystkie zdarzenia powyÅ¼ej przerywanej linii. Mimo to nadal mogÄ… zdarzyÄ‡ siÄ™ przypadki, w ktÃ³rych niektÃ³re punkty zostanÄ… pominiÄ™te.\n\nPrzedstawione na wykresach sytuacje jawnie wskazujÄ… dlaczego pojÄ™cie czasu jest istotnym czynnikiem i wymaga Å›cisÅ‚ego okreÅ›lenia juÅ¼ na poziomie definiowania potrzeb biznesowych. Przypisywanie znacznikÃ³w czasu do danych (zdarzeÅ„) to trudne zadanie."
  },
  {
    "objectID": "wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "href": "wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "title": "WykÅ‚ad 3",
    "section": "Okna czasowe w analizie strumieniowej",
    "text": "Okna czasowe w analizie strumieniowej\nW przetwarzaniu strumieniowym okna czasowe pozwalajÄ… na grupowanie danych w ograniczone czasowo segmenty, co umoÅ¼liwia analizÄ™ zdarzeÅ„ w okreÅ›lonych przedziaÅ‚ach czasowych. W zaleÅ¼noÅ›ci od zastosowania stosuje siÄ™ rÃ³Å¼ne typy okien, dostosowane do charakterystyki danych i wymagaÅ„ analitycznych.\n\n\n1. Okno rozÅ‚Ä…czne (Tumbling Window)\nJest to okno o staÅ‚ej dÅ‚ugoÅ›ci, ktÃ³re nie nakÅ‚ada siÄ™ na siebie â€“ kaÅ¼de zdarzenie naleÅ¼y tylko do jednego okna.\nâœ… Charakterystyka:\n- StaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n- Brak nakÅ‚adania siÄ™ na siebie\n- Idealne do podziaÅ‚u danych na rÃ³wne segmenty czasowe\nğŸ“Œ PrzykÅ‚ad: Analiza liczby zamÃ³wieÅ„ w sklepie internetowym co 5 minut.\n\n\n\n\n2. Okno przesuwne (Sliding Window)\nObejmuje wszystkie zdarzenia nastÄ™pujÄ…ce w okreÅ›lonym przedziale czasu, gdzie okno przesuwa siÄ™ w sposÃ³b ciÄ…gÅ‚y.\nâœ… Charakterystyka:\n- KaÅ¼de zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien\n- Okno przesuwa siÄ™ o zadany interwaÅ‚\n- Przydatne do wykrywania trendÃ³w i anomalii\nğŸ“Œ PrzykÅ‚ad: Åšledzenie Å›redniej temperatury w ciÄ…gu ostatnich 10 minut, aktualizowane co 2 minuty.\n\n\n\n\n3. Okno skokowe (Hopping Window)\nJest podobne do okna rozÅ‚Ä…cznego, ale pozwala na nakÅ‚adanie siÄ™ okien na siebie, dziÄ™ki czemu jedno zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien. Jest stosowane do wygÅ‚adzania danych.\nâœ… Charakterystyka:\n- StaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n- MoÅ¼liwoÅ›Ä‡ nakÅ‚adania siÄ™ na siebie\n- Przydatne do redukcji szumÃ³w w danych\nğŸ“Œ PrzykÅ‚ad: Analiza liczby odwiedzajÄ…cych stronÄ™ co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywaÄ‡ trendy.\n\n\n\n\n4. Okno sesyjne (Session Window)\nOkno sesyjne grupuje zdarzenia na podstawie okresÃ³w aktywnoÅ›ci i zamyka siÄ™ po okreÅ›lonym czasie braku aktywnoÅ›ci.\nâœ… Charakterystyka:\n- Dynamiczna dÅ‚ugoÅ›Ä‡ okna\n- Definiowane przez aktywnoÅ›Ä‡ uÅ¼ytkownika\n- Stosowane w analizie sesji uÅ¼ytkownikÃ³w\nğŸ“Œ PrzykÅ‚ad: Analiza sesji uÅ¼ytkownikÃ³w na stronie internetowej â€“ sesja trwa tak dÅ‚ugo, jak dÅ‚ugo uÅ¼ytkownik wykonuje akcje, ale koÅ„czy siÄ™ po 15 minutach braku aktywnoÅ›ci.\n\n\n\nPodsumowanie\nRÃ³Å¼ne rodzaje okien czasowych sÄ… stosowane w zaleÅ¼noÅ›ci od specyfiki danych i celÃ³w analizy. WybÃ³r odpowiedniego okna wpÅ‚ywa na dokÅ‚adnoÅ›Ä‡ wynikÃ³w i efektywnoÅ›Ä‡ systemu analitycznego.\n\n\n\n\n\n\n\n\nTyp okna\nCharakterystyka\nZastosowanie\n\n\n\n\nRozÅ‚Ä…czne (Tumbling)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, brak nakÅ‚adania\nRaporty okresowe\n\n\nPrzesuwne (Sliding)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, nakÅ‚adajÄ…ce siÄ™ okna\nTrendy, wykrywanie anomalii\n\n\nSkokowe (Hopping)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, czÄ™Å›ciowe nakÅ‚adanie\nWygÅ‚adzanie danych\n\n\nSesyjne (Session)\nDynamiczna dÅ‚ugoÅ›Ä‡, zaleÅ¼na od aktywnoÅ›ci\nAnaliza sesji uÅ¼ytkownikÃ³w\n\n\n\nKaÅ¼dy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. WybÃ³r wÅ‚aÅ›ciwej metody zaleÅ¼y od potrzeb biznesowych i charakterystyki analizowanych danych.\nW analizie danych strumieniowych interpretacja czasu jest zÅ‚oÅ¼onym zagadnieniem, poniewaÅ¼: 1. RÃ³Å¼ne systemy majÄ… rÃ³Å¼ne zegary, co moÅ¼e prowadziÄ‡ do niespÃ³jnoÅ›ci, 2. Dane mogÄ… docieraÄ‡ z opÃ³Åºnieniem, co wymaga technik watermarkingu i okien czasowych, 3. RÃ³Å¼ne podejÅ›cia do analizy czasu zdarzenia i czasu przetwarzania wpÅ‚ywajÄ… na dokÅ‚adnoÅ›Ä‡ wynikÃ³w."
  },
  {
    "objectID": "wyklad2.html",
    "href": "wyklad2.html",
    "title": "WykÅ‚ad 2",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, jak dane ewoluowaÅ‚y w rÃ³Å¼nych branÅ¼ach i jakie narzÄ™dzia sÄ… dziÅ› wykorzystywane do ich analizy.\nNa tym wykÅ‚adzie przedstawimy ewolucjÄ™ analizy danych, pokazujÄ…c, jak zmieniaÅ‚y siÄ™ technologie i podejÅ›cia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aÅ¼ po nowoczesne podejÅ›cie do strumieniowego przetwarzania danych."
  },
  {
    "objectID": "wyklad2.html#dane-tabelaryczne-tabele-sql",
    "href": "wyklad2.html#dane-tabelaryczne-tabele-sql",
    "title": "WykÅ‚ad 2",
    "section": "1. Dane tabelaryczne (tabele SQL)",
    "text": "1. Dane tabelaryczne (tabele SQL)\nPoczÄ…tkowo dane byÅ‚y przechowywane w postaci tabel, gdzie kaÅ¼da tabela zawieraÅ‚a zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).\nModele takie doskonale nadawaÅ‚y siÄ™ do danych ustrukturyzowanych.\n\nğŸ“Œ Cechy:\nâœ… Dane podzielone na kolumny o staÅ‚ej strukturze.\nâœ… MoÅ¼liwoÅ›Ä‡ stosowania operacji CRUD (Create, Read, Update, Delete).\nâœ… ÅšcisÅ‚e reguÅ‚y spÃ³jnoÅ›ci i normalizacji.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Systemy bankowe, e-commerce, ERP, systemy CRM.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()"
  },
  {
    "objectID": "wyklad2.html#dane-grafowe",
    "href": "wyklad2.html#dane-grafowe",
    "title": "WykÅ‚ad 2",
    "section": "2. Dane grafowe",
    "text": "2. Dane grafowe\nWraz z rozwojem potrzeb biznesowych pojawiÅ‚y siÄ™ dane grafowe, w ktÃ³rych relacje miÄ™dzy obiektami sÄ… reprezentowane jako wierzchoÅ‚ki i krawÄ™dzie.\n\nğŸ“Œ Cechy:\nâœ… Dane opisujÄ…ce relacje i powiÄ…zania.\nâœ… Elastyczna struktura (grafy zamiast tabel).\nâœ… MoÅ¼liwoÅ›Ä‡ analizy poÅ‚Ä…czeÅ„ (np. algorytmy PageRank, centralnoÅ›Ä‡).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Sieci spoÅ‚ecznoÅ›ciowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Graf Karate - NetworkX):\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)"
  },
  {
    "objectID": "wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "href": "wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "title": "WykÅ‚ad 2",
    "section": "3. Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)",
    "text": "3. Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)\nDane te nie sÄ… w peÅ‚ni ustrukturyzowane jak w bazach SQL, ale majÄ… pewien schemat.\n\nğŸ“Œ Cechy:\nâœ… Hierarchiczna struktura (np. klucz-wartoÅ›Ä‡, obiekty zagnieÅ¼dÅ¼one).\nâœ… Brak Å›cisÅ‚ego schematu (moÅ¼liwoÅ›Ä‡ dodawania nowych pÃ³l).\nâœ… PopularnoÅ›Ä‡ w systemach NoSQL i API.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logÃ³w.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}"
  },
  {
    "objectID": "wyklad2.html#dane-tekstowe-nlp",
    "href": "wyklad2.html#dane-tekstowe-nlp",
    "title": "WykÅ‚ad 2",
    "section": "4. Dane tekstowe (NLP)",
    "text": "4. Dane tekstowe (NLP)\nTekst staÅ‚ siÄ™ kluczowym ÅºrÃ³dÅ‚em informacji, szczegÃ³lnie w analizie opinii, chatbotach czy wyszukiwarkach.\n\nğŸ“Œ Cechy:\nâœ… Nieustrukturyzowane dane wymagajÄ…ce przeksztaÅ‚cenia.\nâœ… Stosowanie embeddingÃ³w (np. Word2Vec, BERT, GPT).\nâœ… DuÅ¼e zastosowanie w analizie sentymentu i chatbotach.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Media spoÅ‚ecznoÅ›ciowe, e-maile, chatboty, tÅ‚umaczenie maszynowe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie:\n\n\nCode\nimport ollama\n\n# PrzykÅ‚adowe zdanie\nsentence = \"Sztuczna inteligencja zmienia Å›wiat.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-1.6779385805130005, 3.0364203453063965, -6.6012187004089355, -1.7487436532974243]"
  },
  {
    "objectID": "wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "href": "wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "title": "WykÅ‚ad 2",
    "section": "5. Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)",
    "text": "5. Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)\nNowoczesne systemy analizy danych wykorzystujÄ… rÃ³wnieÅ¼ obrazy i dÅºwiÄ™k.\n\nğŸ“Œ Cechy:\nâœ… WymagajÄ… duÅ¼ej mocy obliczeniowej (sztuczna inteligencja, deep learning).\nâœ… Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dÅºwiÄ™k).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treÅ›ci wideo.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Obraz - OpenCV):\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "wyklad2.html#dane-strumieniowe",
    "href": "wyklad2.html#dane-strumieniowe",
    "title": "WykÅ‚ad 2",
    "section": "6. Dane strumieniowe",
    "text": "6. Dane strumieniowe\nObecnie najbardziej dynamicznie rozwija siÄ™ analiza danych strumieniowych, gdzie dane sÄ… analizowane na bieÅ¼Ä…co, w miarÄ™ ich napÅ‚ywania.\n\nğŸ“Œ Cechy:\nâœ… Przetwarzanie w czasie rzeczywistym.\nâœ… Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Strumieniowe transakcje bankowe):\n\n\nCode\nimport time\ntransactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]\nfor transaction in transactions:\n    print(f\"Processing transaction: {transaction}\")\n    time.sleep(1)\n\n\nProcessing transaction: {'id': 1, 'amount': 100}\nProcessing transaction: {'id': 2, 'amount': 200}"
  },
  {
    "objectID": "wyklad2.html#dane-sensoryczne-i-iot",
    "href": "wyklad2.html#dane-sensoryczne-i-iot",
    "title": "WykÅ‚ad 2",
    "section": "7. Dane sensoryczne i IoT",
    "text": "7. Dane sensoryczne i IoT\nDane z czujnikÃ³w i urzÄ…dzeÅ„ IoT sÄ… kolejnym krokiem w ewolucji.\n\nğŸ“Œ Cechy:\nâœ… CzÄ™sto pochodzÄ… z miliardÃ³w urzÄ…dzeÅ„ (big data).\nâœ… WymagajÄ… analizy brzegowej (edge computing).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Smart home, wearables, samochody autonomiczne, systemy przemysÅ‚owe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Sensor - temperatura):\n\n\nCode\nimport random\ndef get_temperature():\n    return round(random.uniform(20.0, 25.0), 2)\nprint(f\"Current temperature: {get_temperature()}Â°C\")\n\n\nCurrent temperature: 21.0Â°C"
  },
  {
    "objectID": "wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "href": "wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "title": "WykÅ‚ad 2",
    "section": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data",
    "text": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data\nKiedy mÃ³wimy o skalowalnym przetwarzaniu danych, pierwszym skojarzeniem moÅ¼e byÄ‡ Google.\nAle co tak naprawdÄ™ sprawia, Å¼e moÅ¼emy wyszukiwaÄ‡ informacje w uÅ‚amku sekundy, przetwarzajÄ…c petabajty danych?\nğŸ‘‰ Czy wiesz, Å¼e nazwa â€œGoogleâ€ pochodzi od sÅ‚owa â€œGoogolâ€, czyli liczby rÃ³wnej 10Â¹â°â°?\nTo wiÄ™cej niÅ¼ liczba atomÃ³w w znanym WszechÅ›wiecie! ğŸŒŒ\n\nğŸ”¥ Wyzwanie: Czy uda Ci siÄ™ zapisaÄ‡ liczbÄ™ Googol do koÅ„ca zajÄ™Ä‡?"
  },
  {
    "objectID": "wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "href": "wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?",
    "text": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?\nTradycyjne bazy danych SQL czy jednowÄ…tkowe algorytmy zawodzÄ…, gdy skala danych przekracza pojedynczy komputer.\nW tym miejscu pojawia siÄ™ MapReduce â€“ rewolucyjny model obliczeniowy stworzony przez Google.\n\nğŸ› ï¸ RozwiÄ…zania Google dla Big Data:\nâœ… Google File System (GFS) â€“ rozproszony system plikÃ³w.\nâœ… Bigtable â€“ system do przechowywania ogromnych iloÅ›ci ustrukturyzowanych danych.\nâœ… MapReduce â€“ algorytm podziaÅ‚u pracy na wiele maszyn."
  },
  {
    "objectID": "wyklad2.html#graficzne-przedstawienie-mapreduce",
    "href": "wyklad2.html#graficzne-przedstawienie-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ–¼ï¸ Graficzne przedstawienie MapReduce",
    "text": "ğŸ–¼ï¸ Graficzne przedstawienie MapReduce\n\n1. Mapowanie rozdziela zadania (Map)\nKaÅ¼de wejÅ›cie dzielone jest na mniejsze czÄ™Å›ci i przetwarzane rÃ³wnolegle.\nğŸŒ WyobraÅº sobie, Å¼e masz ksiÄ…Å¼kÄ™ telefonicznÄ… i chcesz znaleÅºÄ‡ wszystkie osoby o nazwisku â€œNowakâ€.\nâ¡ï¸ Podziel ksiÄ…Å¼kÄ™ na fragmenty i daj kaÅ¼demu do przeanalizowania jeden fragment.\n\n\n2. Redukcja zbiera wyniki (Reduce)\nWszystkie czÄ™Å›ciowe wyniki sÄ… Å‚Ä…czone w jednÄ…, koÅ„cowÄ… odpowiedÅº.\nğŸ”„ Wszyscy uczniowie zgÅ‚aszajÄ… swoje wyniki, a jeden student zbiera i podsumowuje odpowiedÅº."
  },
  {
    "objectID": "wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "href": "wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie",
    "text": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie\nZaÅ‚Ã³Å¼my, Å¼e mamy miliony ksiÄ…Å¼ek i chcemy policzyÄ‡, ile razy wystÄ™puje kaÅ¼de sÅ‚owo.\n\nğŸ–¥ï¸ Kod MapReduce w Pythonie (z uÅ¼yciem multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Funkcja Map (podziaÅ‚ tekstu na sÅ‚owa)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Funkcja Reduce (sumowanie wynikÃ³w)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\nğŸ”¹ Co tu siÄ™ dzieje?\nâœ… KaÅ¼dy fragment tekstu jest przetwarzany niezaleÅ¼nie (map).\nâœ… Wyniki sÄ… zbierane i sumowane (reduce).\nâœ… Efekt: MoÅ¼emy przetwarzaÄ‡ terabajty tekstu rÃ³wnolegle!"
  },
  {
    "objectID": "wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "href": "wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce",
    "text": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce\nğŸ“Š Stare podejÅ›cie â€“ Jeden komputer wykonuje wszystko sekwencyjnie.\nğŸ“Š Nowe podejÅ›cie (MapReduce) â€“ KaÅ¼da maszyna liczy fragment i wyniki sÄ… agregowane."
  },
  {
    "objectID": "wyklad2.html#wyzwanie-dla-ciebie",
    "href": "wyklad2.html#wyzwanie-dla-ciebie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸš€ Wyzwanie dla Ciebie!",
    "text": "ğŸš€ Wyzwanie dla Ciebie!\nğŸ”¹ ZnajdÅº i uruchom swÃ³j wÅ‚asny algorytm MapReduce w dowolnym jÄ™zyku!\nğŸ”¹ Czy potrafisz zaimplementowaÄ‡ wÅ‚asny MapReduce do innego zadania? (np. analiza logÃ³w, zliczanie klikniÄ™Ä‡ na stronie)"
  },
  {
    "objectID": "wyklad2.html#big-data",
    "href": "wyklad2.html#big-data",
    "title": "WykÅ‚ad 2",
    "section": "Big Data",
    "text": "Big Data\nSystemy Big data mogÄ… byÄ‡ czÄ™Å›ciÄ… (ÅºrÃ³dÅ‚em) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie sÄ… systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsÅ‚uÅ¼y do rÃ³Å¼norodnych celÃ³w opartych na danych (analityka, data science â€¦)\nponiÅ¼ej 100% accuracy\n\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.â€™â€™ â€” Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, â€¦ four V\n\nVolume (ObjÄ™toÅ›Ä‡) - rozmiar danych produkowanych na caÅ‚ym Å›wiecie przyrasta w tempie wykÅ‚adniczym.\nVelocity (SzybkoÅ›Ä‡) - tempo produkowania danych, szybkoÅ›ci ich przesyÅ‚ania i przetwarzania.\nVariety (ZrÃ³Å¼nicowanie) - tradycyjne dane kojarzÄ… siÄ™ nam z postaciÄ… alfanumerycznÄ… zÅ‚oÅ¼onÄ… z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dÅºwiÄ™ki, pliki wideo, strumienie danych z IoT\nVeracity (WiarygodnoÅ›Ä‡) - Czy dane sÄ… kompletne i poprawne? Czy obiektywnie odzwierciedlajÄ… rzeczywistoÅ›Ä‡? Czy sÄ… podstawÄ… do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, itâ€™s all about cost and benefits.\n\n\nCelem obliczeÅ„ nie sÄ… liczby, lecz ich zrozumienie R.W. Hamming 1962."
  },
  {
    "objectID": "wyklad2.html#modele-przetwarzania-danych",
    "href": "wyklad2.html#modele-przetwarzania-danych",
    "title": "WykÅ‚ad 2",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane sÄ… praktycznie od zawsze. W ciÄ…gu ostatnich dziesiÄ™cioleci iloÅ›Ä‡ przetwarzanych danych systematycznie roÅ›nie co wpÅ‚ywa na proces przygotowania i przetwarzania danych.\n\nTrochÄ™ historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : PoczÄ…tek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPÃ³Åºniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiÄ™kszoÅ›Ä‡ danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostÄ™p do danych sprowadza siÄ™ najczÄ™Å›ciej do realizacji zapytaÅ„ poprzez aplikacjÄ™.\nSposÃ³b wykorzystania i realizacji procesu dostÄ™pu do bazy danych nazywamy modelem przetwarzania. NajczÄ™Å›ciej uÅ¼ywane sÄ… dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Åšwietnie sprawdza siÄ™ w przypadku obsÅ‚ugi bieÅ¼Ä…cej np. obsÅ‚uga klienta, rejestr zamÃ³wieÅ„, obsÅ‚uga sprzedaÅ¼y itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiÄ…zaÅ„ m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostÄ™pu do danych,\nzarzÄ…dzania wspÃ³Å‚bieÅ¼noÅ›ciÄ…,\nprzetwarzania zdarzeÅ„ -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemÃ³w (np. dla wielu sklepÃ³w),\nraportowanie i podsumowania danych,\noptymalizacja zÅ‚oÅ¼onych zapytaÅ„,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziÅ‚y do sformuÅ‚owania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesÃ³w analizy i dostarczanie narzÄ™dzi umoÅ¼liwiajÄ…cych analizÄ™ wielowymiarowÄ… (czas, miejsce, produkt).\nProces zrzucania danych z rÃ³Å¼nych systemÃ³w do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatÃ³w (podsumowaÅ„) dotyczÄ…cych wymiarÃ³w hurtowni. Proces ten jest caÅ‚kowicie sterowany przez uÅ¼ytkownika.\nPrzykÅ‚ad\nZaÅ‚Ã³Å¼my, Å¼e mamy dostÄ™p do hurtowni danych gdzie przechowywane sÄ… informacje dotyczÄ…ce sprzedaÅ¼y produktÃ³w w supermarkecie. Jak przeanalizowaÄ‡ zapytania:\n\nJaka jest Å‚Ä…czna sprzedaÅ¼ produktÃ³w w kolejnych kwartaÅ‚ach, miesiÄ…cach, tygodniach ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na rodzaje produktÃ³w ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na oddziaÅ‚y supermarketu ?\n\nOdpowiedzi na te pytania pozwalajÄ… okreÅ›liÄ‡ wÄ…skie gardÅ‚a sprzedaÅ¼y produktÃ³w przynoszÄ…cych deficyt, zaplanowaÄ‡ zapasy w magazynach czy porÃ³wnaÄ‡ sprzedaÅ¼ rÃ³Å¼nych grup w rÃ³Å¼nych oddziaÅ‚ach supermarketu.\nW ramach Hurtowni Danych najczÄ™Å›ciej wykonuje siÄ™ dwa rodzaje zapytaÅ„(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczajÄ…ce biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagajÄ…ce krytyczne decyzje biznesowe."
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#ksiÄ…Å¼ki",
    "href": "ksiazki.html#ksiÄ…Å¼ki",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatnikÃ³w\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI donâ€™t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJÄ™zyk prowadzenia: polski\nPoziom przedmiotu: Å›rednio-zaawansowany\nProwadzÄ…cy: Sebastian ZajÄ…c, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspÃ³Å‚czesny biznes opiera siÄ™ na podejmowaniu decyzji opartych na danych. Coraz wiÄ™ksza iloÅ›Ä‡ informacji, rosnÄ…ce wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiajÄ…, Å¼e analiza danych w czasie rzeczywistym staje siÄ™ kluczowym elementem nowoczesnych procesÃ³w biznesowych.\nNa zajÄ™ciach studenci zapoznajÄ… siÄ™ z metodami i technologiami umoÅ¼liwiajÄ…cymi przetwarzanie danych w czasie rzeczywistym. SzczegÃ³lnÄ… uwagÄ™ poÅ›wiÄ™cimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz gÅ‚Ä™bokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretowaÄ‡ zjawiska biznesowe, ale takÅ¼e podejmowaÄ‡ szybkie i trafne decyzje.\nW ramach kursu omÃ³wimy zarÃ³wno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dÅºwiÄ™k, strumieniowanie wideo). Studenci poznajÄ… architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a takÅ¼e wyzwania zwiÄ…zane z modelowaniem danych w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\nKurs obejmuje czÄ™Å›Ä‡ teoretycznÄ… oraz praktyczne laboratoria, podczas ktÃ³rych studenci bÄ™dÄ… pracowaÄ‡ z rzeczywistymi danymi w Å›rodowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. DziÄ™ki temu studenci nie tylko zdobÄ™dÄ… wiedzÄ™ na temat metod analitycznych, ale takÅ¼e nauczÄ… siÄ™ korzystaÄ‡ z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plikoÌw pÅ‚askich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (WykÅ‚ad)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametroÌw modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykÅ‚adzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego sÌrodowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego sÌrodowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 1.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 2.\nPrzygotowanie sÌrodowiska Microsoft Azure. Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzeÌ¨dzia IT do szybkiej analizy logoÌw.\nNarzeÌ¨dzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-ksztaÅ‚cenia",
    "href": "sylabus.html#efekty-ksztaÅ‚cenia",
    "title": "Sylabus",
    "section": "Efekty ksztaÅ‚cenia",
    "text": "Efekty ksztaÅ‚cenia\n\nWiedza:\n\n\nZna historieÌ¨ i filozofieÌ¨ modeli przetwarzania danych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna mozÌ‡liwosÌci i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nUmie wybracÌ struktureÌ¨ IT dla danego problemu biznesowego\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo kroÌtkim czasie\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejÄ™tnoÅ›ci:\n\n\nRozroÌzÌ‡nia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiaÌ¨zania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ, przetwarzacÌ oraz zachowywacÌ dane generowane w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikajaÌ¨ce z czasu przetwarzania przez urzaÌ¨dzenia oraz systemy informatyczne\n\nPowiaÌ¨zania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosowacÌ i skonstruowacÌ system do przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormuÅ‚uje problem analityczny wraz z jego informatycznym rozwiaÌ¨zaniem\n\nPowiaÌ¨zania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejeÌ¨tnosÌcÌ samodzielnego uzupeÅ‚niania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiaÌ¨zania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1ï¸âƒ£ ZajÄ…c S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzÄ™dzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2ï¸âƒ£ FrÄ…tczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3ï¸âƒ£ Bellemare A., MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™, Oâ€™Reilly 2021.\n4ï¸âƒ£ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, Oâ€™Reilly 2022.\n5ï¸âƒ£ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. RozwiÄ…zania typowych problemÃ³w dotyczÄ…cych przygotowania danych, konstruowania modeli i MLOps, Oâ€™Reilly 2021.\n6ï¸âƒ£ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, Oâ€™Reilly 2022.\n7ï¸âƒ£ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, Oâ€™Reilly 2018.\n8ï¸âƒ£ SebastiÃ¡n RamÃ­rez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostÄ™pna online).\n9ï¸âƒ£ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\nğŸ”Ÿ Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, Oâ€™Reilly 2019."
  },
  {
    "objectID": "w3.html",
    "href": "w3.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Wykorzystanie systemÃ³w real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, ktÃ³ra bÄ™dzie mogÅ‚a szybko przetwarzaÄ‡ ogromne iloÅ›ci danych oraz reagowaÄ‡ na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, poniewaÅ¼ umoÅ¼liwia szybsze podejmowanie decyzji, monitorowanie procesÃ³w w czasie rzeczywistym i reagowanie na zdarzenia bez opÃ³Åºnienia.\nOmÃ³wimy gÅ‚Ã³wne elementy architektury systemÃ³w real-time, popularne wzorce architektoniczne oraz technologie, ktÃ³re sÄ… wykorzystywane do budowy takich systemÃ³w.\n\n\nSystemy real-time muszÄ… speÅ‚niaÄ‡ szereg wymagaÅ„ zwiÄ…zanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentÃ³w w architekturze systemu, ktÃ³re zapewniajÄ… jego prawidÅ‚owe funkcjonowanie.\n\n\nDane w systemie real-time pochodzÄ… z rÃ³Å¼nych ÅºrÃ³deÅ‚, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urzÄ…dzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z gieÅ‚dy.\nDane uÅ¼ytkownikÃ³w: np. logi uÅ¼ytkownikÃ³w w aplikacjach mobilnych, dane z mediÃ³w spoÅ‚ecznoÅ›ciowych.\n\n\n\n\nDane muszÄ… byÄ‡ szybko przesyÅ‚ane do systemÃ³w, ktÃ³re mogÄ… je analizowaÄ‡. W tym celu wykorzystywane sÄ… technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyÅ‚ania danych w czasie rzeczywistym, zapewniajÄ…cy wysokÄ… wydajnoÅ›Ä‡ i niezawodnoÅ›Ä‡.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z duÅ¼Ä… iloÅ›ciÄ… subskrybentÃ³w.\n\n\n\n\nDane w systemach real-time sÄ… czÄ™sto przetwarzane w strumieniu. Dwa gÅ‚Ã³wne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, ktÃ³re moÅ¼e mieÄ‡ opÃ³Åºnienie, ale przetwarza dane w sposÃ³b efektywny. MoÅ¼e byÄ‡ wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opÃ³ÅºnieÅ„, w ktÃ³rym dane sÄ… natychmiastowo analizowane i przetwarzane.\n\n\n\n\nPrzechowywanie danych w systemie real-time zaleÅ¼y od wymagaÅ„ aplikacji. Dwa gÅ‚Ã³wne rodzaje przechowywania to:\n\nData Lake: skÅ‚adowanie ogromnych iloÅ›ci nieprzetworzonych danych w postaci surowych plikÃ³w. Bazy danych NoSQL: takie jak Cassandra, ktÃ³re umoÅ¼liwiajÄ… szybki dostÄ™p do danych w czasie rzeczywistym.\nData Warehouse: skÅ‚adowanie przetworzonych danych w celu ich analizy.\n\n\n\n\nPo przetworzeniu danych w czasie rzeczywistym naleÅ¼y wykonaÄ‡ ich analizÄ™ i prezentacjÄ™ w sposÃ³b zrozumiaÅ‚y dla uÅ¼ytkownika:\n\nDashboardy: narzÄ™dzia takie jak Grafana lub Kibana, ktÃ³re sÅ‚uÅ¼Ä… do wizualizacji wynikÃ³w w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmÃ³w uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\n\n\n\nLambda Architecture to popularna koncepcja przetwarzania danych, ktÃ³ra Å‚Ä…czy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura uÅ¼ywana w systemach przetwarzania Big Data, ktÃ³ra zakÅ‚ada dwie warstwy:\n\nBatch Layer: przetwarzanie (duÅ¼ych iloÅ›ci) danych wsadowych, ktÃ³re sÄ… pÃ³Åºniej wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napÅ‚ywajÄ…ce dane strumieniowe, np. z sensorÃ³w, social media, transakcji, w celu uzyskania natychmiastowych wynikÃ³w.\nServing Layer: warstwa, ktÃ³ra Å‚Ä…czy wyniki obu poprzednich warstw i dostarcza je do uÅ¼ytkownika np. za pomocÄ… API.\n\n \n\n\n\n\nâœ… MoÅ¼liwoÅ›Ä‡ Å‚Ä…czenia przetwarzania wsadowego i strumieniowego,\nâœ… wsparcie dla duÅ¼ych zbiorÃ³w danych,\nâœ… elastycznoÅ›Ä‡ w przetwarzaniu zÅ‚oÅ¼onych zapytaÅ„.\nâŒ Wymaga utrzymywania dwÃ³ch oddzielnych systemÃ³w do przetwarzania danych (batch i stream), co prowadzi do zÅ‚oÅ¼onoÅ›ci implementacji i utrzymania.\n\n\n\n\nKappa Architecture jest uproszczonÄ… wersjÄ… Lambda Architecture. Zamiast uÅ¼ywaÄ‡ dwÃ³ch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jednÄ… warstwÄ™ przetwarzania strumieniowego, co upraszcza caÅ‚y system.\nJest to bardziej elastyczne podejÅ›cie do budowy systemÃ³w real-time, zwÅ‚aszcza w przypadku, gdy dane sÄ… przetwarzane tylko w jednym trybie (streaming).\n \n\n\n\n\nâœ… Prostota: Jako Å¼e przetwarzanie danych odbywa siÄ™ tylko w jednym strumieniu, caÅ‚y system jest prostszy i bardziej spÃ³jny.\nâœ… SkalowalnoÅ›Ä‡: DziÄ™ki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekÅ›cie analizy danych w czasie rzeczywistym.\nâœ… Idealne dla ML: Kappa Architecture Å›wietnie sprawdza siÄ™ w zastosowaniach zwiÄ…zanych z Machine Learning, poniewaÅ¼ przetwarzanie danych odbywa siÄ™ na bieÅ¼Ä…co, co pozwala na szybsze uczenie i wdraÅ¼anie modeli ML w czasie rzeczywistym.\nâŒ MoÅ¼e byÄ‡ mniej wydajna przy bardzo duÅ¼ych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\n\nArchitektura mikroserwisÃ³w jest powszechnie wykorzystywana w systemach real-time, poniewaÅ¼ umoÅ¼liwia:\n\nPodziaÅ‚ aplikacji na mniejsze, autonomiczne jednostki.\nElastycznoÅ›Ä‡ i skalowalnoÅ›Ä‡ systemu.\nMoÅ¼liwoÅ›Ä‡ przetwarzania rÃ³Å¼nych rodzajÃ³w danych przez rÃ³Å¼ne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomoÅ›ci.\n\n\n\n\nUber to przykÅ‚ad firmy, ktÃ³ra skutecznie wykorzystuje narzÄ™dzia do przetwarzania strumieniowego, by monitorowaÄ‡ ruch drogowy w czasie rzeczywistym. DziÄ™ki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdÃ³w oraz czasach oczekiwania na przejazd, ktÃ³re sÄ… nastÄ™pnie analizowane na Å¼ywo.\nDane wejÅ›ciowe: Informacje o czasie i miejscu podrÃ³Å¼y, dane GPS z pojazdÃ³w, natÄ™Å¼enie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyÅ‚ania tych danych w czasie rzeczywistym do systemÃ³w takich jak Apache Flink lub Spark Streaming, ktÃ³re analizujÄ… je na bieÅ¼Ä…co.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasÄ™ w czasie rzeczywistym.\nWynik: UÅ¼ytkownicy Ubera otrzymujÄ… prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowcÃ³w), co umoÅ¼liwia optymalizacjÄ™ transportu."
  },
  {
    "objectID": "w3.html#architektura-systemÃ³w-real-time",
    "href": "w3.html#architektura-systemÃ³w-real-time",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Wykorzystanie systemÃ³w real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, ktÃ³ra bÄ™dzie mogÅ‚a szybko przetwarzaÄ‡ ogromne iloÅ›ci danych oraz reagowaÄ‡ na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, poniewaÅ¼ umoÅ¼liwia szybsze podejmowanie decyzji, monitorowanie procesÃ³w w czasie rzeczywistym i reagowanie na zdarzenia bez opÃ³Åºnienia.\nOmÃ³wimy gÅ‚Ã³wne elementy architektury systemÃ³w real-time, popularne wzorce architektoniczne oraz technologie, ktÃ³re sÄ… wykorzystywane do budowy takich systemÃ³w.\n\n\nSystemy real-time muszÄ… speÅ‚niaÄ‡ szereg wymagaÅ„ zwiÄ…zanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentÃ³w w architekturze systemu, ktÃ³re zapewniajÄ… jego prawidÅ‚owe funkcjonowanie.\n\n\nDane w systemie real-time pochodzÄ… z rÃ³Å¼nych ÅºrÃ³deÅ‚, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urzÄ…dzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z gieÅ‚dy.\nDane uÅ¼ytkownikÃ³w: np. logi uÅ¼ytkownikÃ³w w aplikacjach mobilnych, dane z mediÃ³w spoÅ‚ecznoÅ›ciowych.\n\n\n\n\nDane muszÄ… byÄ‡ szybko przesyÅ‚ane do systemÃ³w, ktÃ³re mogÄ… je analizowaÄ‡. W tym celu wykorzystywane sÄ… technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyÅ‚ania danych w czasie rzeczywistym, zapewniajÄ…cy wysokÄ… wydajnoÅ›Ä‡ i niezawodnoÅ›Ä‡.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z duÅ¼Ä… iloÅ›ciÄ… subskrybentÃ³w.\n\n\n\n\nDane w systemach real-time sÄ… czÄ™sto przetwarzane w strumieniu. Dwa gÅ‚Ã³wne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, ktÃ³re moÅ¼e mieÄ‡ opÃ³Åºnienie, ale przetwarza dane w sposÃ³b efektywny. MoÅ¼e byÄ‡ wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opÃ³ÅºnieÅ„, w ktÃ³rym dane sÄ… natychmiastowo analizowane i przetwarzane.\n\n\n\n\nPrzechowywanie danych w systemie real-time zaleÅ¼y od wymagaÅ„ aplikacji. Dwa gÅ‚Ã³wne rodzaje przechowywania to:\n\nData Lake: skÅ‚adowanie ogromnych iloÅ›ci nieprzetworzonych danych w postaci surowych plikÃ³w. Bazy danych NoSQL: takie jak Cassandra, ktÃ³re umoÅ¼liwiajÄ… szybki dostÄ™p do danych w czasie rzeczywistym.\nData Warehouse: skÅ‚adowanie przetworzonych danych w celu ich analizy.\n\n\n\n\nPo przetworzeniu danych w czasie rzeczywistym naleÅ¼y wykonaÄ‡ ich analizÄ™ i prezentacjÄ™ w sposÃ³b zrozumiaÅ‚y dla uÅ¼ytkownika:\n\nDashboardy: narzÄ™dzia takie jak Grafana lub Kibana, ktÃ³re sÅ‚uÅ¼Ä… do wizualizacji wynikÃ³w w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmÃ³w uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\n\n\n\nLambda Architecture to popularna koncepcja przetwarzania danych, ktÃ³ra Å‚Ä…czy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura uÅ¼ywana w systemach przetwarzania Big Data, ktÃ³ra zakÅ‚ada dwie warstwy:\n\nBatch Layer: przetwarzanie (duÅ¼ych iloÅ›ci) danych wsadowych, ktÃ³re sÄ… pÃ³Åºniej wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napÅ‚ywajÄ…ce dane strumieniowe, np. z sensorÃ³w, social media, transakcji, w celu uzyskania natychmiastowych wynikÃ³w.\nServing Layer: warstwa, ktÃ³ra Å‚Ä…czy wyniki obu poprzednich warstw i dostarcza je do uÅ¼ytkownika np. za pomocÄ… API.\n\n \n\n\n\n\nâœ… MoÅ¼liwoÅ›Ä‡ Å‚Ä…czenia przetwarzania wsadowego i strumieniowego,\nâœ… wsparcie dla duÅ¼ych zbiorÃ³w danych,\nâœ… elastycznoÅ›Ä‡ w przetwarzaniu zÅ‚oÅ¼onych zapytaÅ„.\nâŒ Wymaga utrzymywania dwÃ³ch oddzielnych systemÃ³w do przetwarzania danych (batch i stream), co prowadzi do zÅ‚oÅ¼onoÅ›ci implementacji i utrzymania.\n\n\n\n\nKappa Architecture jest uproszczonÄ… wersjÄ… Lambda Architecture. Zamiast uÅ¼ywaÄ‡ dwÃ³ch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jednÄ… warstwÄ™ przetwarzania strumieniowego, co upraszcza caÅ‚y system.\nJest to bardziej elastyczne podejÅ›cie do budowy systemÃ³w real-time, zwÅ‚aszcza w przypadku, gdy dane sÄ… przetwarzane tylko w jednym trybie (streaming).\n \n\n\n\n\nâœ… Prostota: Jako Å¼e przetwarzanie danych odbywa siÄ™ tylko w jednym strumieniu, caÅ‚y system jest prostszy i bardziej spÃ³jny.\nâœ… SkalowalnoÅ›Ä‡: DziÄ™ki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekÅ›cie analizy danych w czasie rzeczywistym.\nâœ… Idealne dla ML: Kappa Architecture Å›wietnie sprawdza siÄ™ w zastosowaniach zwiÄ…zanych z Machine Learning, poniewaÅ¼ przetwarzanie danych odbywa siÄ™ na bieÅ¼Ä…co, co pozwala na szybsze uczenie i wdraÅ¼anie modeli ML w czasie rzeczywistym.\nâŒ MoÅ¼e byÄ‡ mniej wydajna przy bardzo duÅ¼ych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\n\nArchitektura mikroserwisÃ³w jest powszechnie wykorzystywana w systemach real-time, poniewaÅ¼ umoÅ¼liwia:\n\nPodziaÅ‚ aplikacji na mniejsze, autonomiczne jednostki.\nElastycznoÅ›Ä‡ i skalowalnoÅ›Ä‡ systemu.\nMoÅ¼liwoÅ›Ä‡ przetwarzania rÃ³Å¼nych rodzajÃ³w danych przez rÃ³Å¼ne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomoÅ›ci.\n\n\n\n\nUber to przykÅ‚ad firmy, ktÃ³ra skutecznie wykorzystuje narzÄ™dzia do przetwarzania strumieniowego, by monitorowaÄ‡ ruch drogowy w czasie rzeczywistym. DziÄ™ki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdÃ³w oraz czasach oczekiwania na przejazd, ktÃ³re sÄ… nastÄ™pnie analizowane na Å¼ywo.\nDane wejÅ›ciowe: Informacje o czasie i miejscu podrÃ³Å¼y, dane GPS z pojazdÃ³w, natÄ™Å¼enie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyÅ‚ania tych danych w czasie rzeczywistym do systemÃ³w takich jak Apache Flink lub Spark Streaming, ktÃ³re analizujÄ… je na bieÅ¼Ä…co.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasÄ™ w czasie rzeczywistym.\nWynik: UÅ¼ytkownicy Ubera otrzymujÄ… prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowcÃ³w), co umoÅ¼liwia optymalizacjÄ™ transportu."
  },
  {
    "objectID": "wyklad1.html",
    "href": "wyklad1.html",
    "title": "WykÅ‚ad 1",
    "section": "",
    "text": "â³ Czas trwania: 1,5h\nğŸ¯ Cel wykÅ‚adu\nZapoznanie studentÃ³w z podstawami real-time analytics, rÃ³Å¼nicami miÄ™dzy trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami."
  },
  {
    "objectID": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez koniecznoÅ›ci przechowywania i oczekiwania na pÃ³Åºniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wnioskÃ³w i reakcji na zmieniajÄ…ce siÄ™ warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) â€“ dane sÄ… analizowane w ciÄ…gu milisekund lub sekund od ich wygenerowania.\nStreaming vs.Â Batch Processing â€“ analiza danych moÅ¼e odbywaÄ‡ siÄ™ w sposÃ³b ciÄ…gÅ‚y (streaming) lub w z gÃ³ry okreÅ›lonych interwaÅ‚ach (batch).\nIntegracja z IoT, AI i ML â€“ real-time analytics czÄ™sto wspÃ³Å‚pracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym â€“ np. natychmiastowa detekcja oszustw w transakcjach bankowych."
  },
  {
    "objectID": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "WykÅ‚ad 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowoÅ›Ä‡\n\nWykrywanie oszustw â€“ analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujÄ…cych na oszustwa.\nAutomatyczny trading â€“ systemy HFT (High-Frequency Trading) analizujÄ… miliony danych w uÅ‚amkach sekundy.\nDynamiczne oceny kredytowe â€“ natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym â€“ dynamiczne rekomendacje produktÃ³w na podstawie aktualnego zachowania uÅ¼ytkownika.\nDynamiczne ceny â€“ np. Uber, Amazon i hotele stosujÄ… dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediÃ³w spoÅ‚ecznoÅ›ciowych â€“ analiza nastrojÃ³w klientÃ³w i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej â€“ analiza logÃ³w w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystÄ…pieniem.\nSmart Cities â€“ analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji Å›wietlnej.\nAnalityka IoT â€“ urzÄ…dzenia IoT generujÄ… strumienie danych, ktÃ³re moÅ¼na analizowaÄ‡ w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentÃ³w â€“ analiza sygnaÅ‚Ã³w z urzÄ…dzeÅ„ medycznych w celu natychmiastowego wykrycia zagroÅ¼enia Å¼ycia.\nAnalityka epidemiologiczna â€“ Å›ledzenie rozprzestrzeniania siÄ™ chorÃ³b na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemÃ³w informatycznych, ktÃ³ry umoÅ¼liwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branÅ¼ach â€“ od finansÃ³w, przez e-commerce, aÅ¼ po ochronÄ™ zdrowia i IoT."
  },
  {
    "objectID": "wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "WykÅ‚ad 1",
    "section": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstniejÄ… trzy gÅ‚Ã³wne podejÅ›cia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKaÅ¼de z nich rÃ³Å¼ni siÄ™ szybkoÅ›ciÄ… przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing â€“ Przetwarzanie wsadowe\nğŸ“Œ Definicja:\nBatch Processing polega na zbieraniu duÅ¼ych iloÅ›ci danych i ich przetwarzaniu w okreÅ›lonych odstÄ™pach czasu (np. co godzinÄ™, codziennie, co tydzieÅ„).\nğŸ“Œ Cechy:\n\nâœ… Wysoka wydajnoÅ›Ä‡ dla duÅ¼ych zbiorÃ³w danych\nâœ… Przetwarzanie danych po ich zgromadzeniu\nâœ… Nie wymaga natychmiastowej analizy\nâœ… Zwykle taÅ„sze niÅ¼ przetwarzanie w czasie rzeczywistym\nâŒ OpÃ³Åºnienia â€“ wyniki sÄ… dostÄ™pne dopiero po zakoÅ„czeniu przetwarzania\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nGenerowanie raportÃ³w finansowych na koniec dnia/miesiÄ…ca\nAnaliza trendÃ³w sprzedaÅ¼y na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiÄ…ca\n\n# Agregacja danych - miesiÄ™czne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wynikÃ³w do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyÅ› chciaÅ‚ utworzyÄ‡ dane do przykÅ‚adu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics â€“ Analiza niemal w czasie rzeczywistym\nğŸ“Œ Definicja:\nNear Real-Time Analytics to analiza danych, ktÃ³ra odbywa siÄ™ z minimalnym opÃ³Åºnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie peÅ‚na analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duÅ¼e opÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na biznes.\nğŸ“Œ Cechy:\n\nâœ… Przetwarzanie danych w krÃ³tkich odstÄ™pach czasu (kilka sekund â€“ minut)\nâœ… UmoÅ¼liwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\nâœ… Optymalny balans miÄ™dzy kosztami a szybkoÅ›ciÄ…\nâŒ Nie nadaje siÄ™ do systemÃ³w wymagajÄ…cych natychmiastowej reakcji\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciÄ…gu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w\nAnaliza logÃ³w serwerÃ³w i sieci w celu wykrycia anomalii\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logÃ³w IT)\nAmazon Kinesis\n\nPrzykÅ‚ad producenta danych realizujÄ…cego tranzakcje wysyÅ‚ane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generujÄ…ca przykÅ‚adowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota miÄ™dzy 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# ZakoÅ„czenie dziaÅ‚ania producenta\nproducer.flush()\nproducer.close()\nPrzykÅ‚ad consumenta - programu sparawdzajÄ…cego zbyt duÅ¼e transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"ğŸš¨ Wykryto duÅ¼Ä… transakcjÄ™: {transaction}\")\nPrzykÅ‚adowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics â€“ Analiza w czasie rzeczywistym\nğŸ“Œ Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w uÅ‚amku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagajÄ…cych reakcji w czasie rzeczywistym, np. w transakcjach gieÅ‚dowych, systemach IoT czy cyberbezpieczeÅ„stwie.\nğŸ“Œ Cechy:\n\nâœ… Bardzo niskie opÃ³Åºnienie (milliseconds-seconds)\nâœ… UmoÅ¼liwia natychmiastowÄ… reakcjÄ™ systemu\nâœ… Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\nâŒ DroÅ¼sze i bardziej zÅ‚oÅ¼one technologicznie niÅ¼ batch processing\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nHigh-Frequency Trading (HFT) â€“ analiza i podejmowanie decyzji w transakcjach gieÅ‚dowych w milisekundach\nAutonomiczne samochody â€“ analiza strumieni danych z kamer i sensorÃ³w w czasie rzeczywistym\nCyberbezpieczeÅ„stwo â€“ detekcja atakÃ³w w sieciach komputerowych w uÅ‚amku sekundy\nAnalityka IoT â€“ np. natychmiastowa detekcja anomalii w danych z czujnikÃ³w przemysÅ‚owych\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\nğŸ” PorÃ³wnanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpÃ³Åºnienie\nMinuty â€“ godziny â€“ dni\nSekundy â€“ minuty\nMilisekundy â€“ sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w peÅ‚ni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\nğŸ“‰ Niski\nğŸ“ˆ Åšredni\nğŸ“ˆğŸ“ˆ Wysoki\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ implementacji\nğŸ“‰ Prosta\nğŸ“ˆ Åšrednia\nğŸ“ˆğŸ“ˆ Trudna\n\n\nPrzykÅ‚ady zastosowaÅ„\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\nğŸ“Œ Kiedy stosowaÄ‡ Batch Processing?\n\nâœ… Gdy nie wymagasz natychmiastowej analizy\nâœ… Gdy masz duÅ¼e iloÅ›ci danych, ale przetwarzane sÄ… one okresowo\nâœ… Gdy chcesz obniÅ¼yÄ‡ koszty\n\nğŸ“Œ Kiedy stosowaÄ‡ Near Real-Time Analytics?\n\nâœ… Gdy wymagasz analizy w krÃ³tkim czasie (sekundy â€“ minuty)\nâœ… Gdy potrzebujesz bardziej aktualnych danych, ale nie w peÅ‚nym real-time\nâœ… Gdy szukasz kompromisu miÄ™dzy wydajnoÅ›ciÄ… a kosztami\n\nğŸ“Œ Kiedy stosowaÄ‡ Real-Time Analytics?\n\nâœ… Gdy kaÅ¼da milisekunda ma znaczenie (np. gieÅ‚da, autonomiczne pojazdy)\nâœ… Gdy chcesz wykrywaÄ‡ oszustwa, anomalie lub incydenty natychmiast\nâœ… Gdy system musi natychmiast reagowaÄ‡ na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne â€“ w wielu przypadkach near real-time jest wystarczajÄ…ce i bardziej opÅ‚acalne. Kluczowe jest zrozumienie wymagaÅ„ biznesowych przed wyborem odpowiedniego rozwiÄ…zania."
  },
  {
    "objectID": "wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "href": "wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "title": "WykÅ‚ad 1",
    "section": "Dlaczego Real-Time Analytics jest waÅ¼ne?",
    "text": "Dlaczego Real-Time Analytics jest waÅ¼ne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje siÄ™ coraz bardziej istotna w wielu branÅ¼ach, poniewaÅ¼ umoÅ¼liwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodÃ³w, dla ktÃ³rych real-time analytics jest waÅ¼ne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagowaÄ‡ na zmiany i wydarzenia w czasie rzeczywistym. DziÄ™ki temu moÅ¼na podejmowaÄ‡ decyzje szybciej, co jest kluczowe w dynamicznych Å›rodowiskach, takich jak:\n\nMarketing: Reklamy mogÄ… byÄ‡ dostosowane do zachowaÅ„ uÅ¼ytkownikÃ³w w czasie rzeczywistym (np. personalizacja treÅ›ci reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie kaÅ¼da minuta moÅ¼e oznaczaÄ‡ rÃ³Å¼nicÄ™ w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogÄ… monitorowaÄ‡ kluczowe wskaÅºniki operacyjne na bieÅ¼Ä…co. PrzykÅ‚ady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urzÄ…dzeÅ„ w fabrykach, aby natychmiast wykrywaÄ‡ awarie i zapobiegaÄ‡ przestojom.\nHealthtech: Åšledzenie parametrÃ³w Å¼yciowych pacjentÃ³w i wykrywanie anomalii, co moÅ¼e ratowaÄ‡ Å¼ycie.\n\n\n\nZwiÄ™kszenie efektywnoÅ›ci operacyjnej\nReal-time analytics umoÅ¼liwia natychmiastowe wykrywanie i eliminowanie problemÃ³w operacyjnych, zanim stanÄ… siÄ™ powaÅ¼niejsze. PrzykÅ‚ady:\n\nLogistyka: Åšledzenie przesyÅ‚ek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywnoÅ›Ä‡ i zmniejsza opÃ³Åºnienia.\nRetail: Monitorowanie poziomu zapasÃ³w na bieÅ¼Ä…co i dostosowywanie zamÃ³wieÅ„ do aktualnych potrzeb.\n\n\n\nKonkurencyjnoÅ›Ä‡\nOrganizacje, ktÃ³re wykorzystujÄ… analitykÄ™ w czasie rzeczywistym, majÄ… przewagÄ™ nad konkurencjÄ…, poniewaÅ¼ mogÄ… szybciej reagowaÄ‡ na zmiany na rynku, nowe potrzeby klientÃ³w i sytuacje kryzysowe. DziÄ™ki natychmiastowym informacjom:\n\nMoÅ¼na podejmowaÄ‡ decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywaÄ‡ lepsze relacje z klientami, reagujÄ…c na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doÅ›wiadczenia uÅ¼ytkownikÃ³w (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z uÅ¼ytkownikami w trakcie ich trwania. PrzykÅ‚ady:\n\nE-commerce: Analiza koszyka zakupowego uÅ¼ytkownika w czasie rzeczywistym, aby np. zaoferowaÄ‡ rabat lub przypomnieÄ‡ o porzuconych produktach.\nStreaming: Optymalizacja jakoÅ›ci usÅ‚ugi wideo/streamingowej w zaleÅ¼noÅ›ci od dostÄ™pnej przepustowoÅ›ci Å‚Ä…cza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym Å›wiecie peÅ‚nym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeÅ„stwa. PrzykÅ‚ady:\n\nCyberbezpieczeÅ„stwo: Real-time analytics umoÅ¼liwia wykrywanie podejrzanych dziaÅ‚aÅ„ w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztÃ³w\nDziÄ™ki analizie w czasie rzeczywistym moÅ¼na optymalizowaÄ‡ zasoby i zmniejszaÄ‡ koszty. Na przykÅ‚ad:\n\nZarzÄ…dzanie energiÄ…: Analiza zuÅ¼ycia energii w czasie rzeczywistym, umoÅ¼liwiajÄ…ca optymalizacjÄ™ wydatkÃ³w na energiÄ™ w firmach.\nOptymalizacja Å‚aÅ„cucha dostaw: DziÄ™ki bieÅ¼Ä…cemu Å›ledzeniu zapasÃ³w i dostaw moÅ¼na lepiej zarzÄ…dzaÄ‡ kosztami magazynowania i transportu.\n\n\n\nZdolnoÅ›Ä‡ do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, ktÃ³re mogÄ… przewidywaÄ‡ przyszÅ‚e zachowania lub problemy, a takÅ¼e je eliminowaÄ‡ zanim siÄ™ pojawiÄ…. Na przykÅ‚ad:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w poÅ‚Ä…czeniu z modelami predykcyjnymi pozwala przewidywaÄ‡ awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym moÅ¼na dostosowywaÄ‡ produkcjÄ™ lub zapasy na podstawie bieÅ¼Ä…cych trendÃ³w.\n\nReal-time analytics to nie tylko analiza danych â€“ to kluczowy element strategii firm w Å›wiecie, ktÃ³ry wymaga szybkich reakcji, elastycznoÅ›ci i dostosowywania siÄ™ do zmieniajÄ…cego siÄ™ otoczenia. Firmy, ktÃ³re wdraÅ¼ajÄ… te technologie, mogÄ… znaczÄ…co poprawiÄ‡ swoje wyniki finansowe, obsÅ‚ugÄ™ klienta, wydajnoÅ›Ä‡ operacyjnÄ…, a takÅ¼e przewagÄ™ konkurencyjnÄ…."
  },
  {
    "objectID": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z wieloma wyzwaniami i trudnoÅ›ciami, ktÃ³re trzeba rozwiÄ…zaÄ‡, aby systemy real-time dziaÅ‚aÅ‚y efektywnie i niezawodnie. Pomimo ogromnego potencjaÅ‚u, jaki daje moÅ¼liwoÅ›Ä‡ natychmiastowego przetwarzania danych, realizacja tych procesÃ³w w praktyce wiÄ…Å¼e siÄ™ z licznymi problemami technologicznymi, organizacyjnymi i dotyczÄ…cymi zarzÄ…dzania danymi.\nPoniÅ¼ej przedstawiamy najwaÅ¼niejsze wyzwania oraz moÅ¼liwe rozwiÄ…zania, ktÃ³re naleÅ¼y uwzglÄ™dniÄ‡ podczas implementacji systemÃ³w analizy danych w czasie rzeczywistym.\n\nSkalowalnoÅ›Ä‡ systemÃ³w\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadaÅ„. W miarÄ™ jak iloÅ›Ä‡ generowanych danych roÅ›nie, systemy muszÄ… byÄ‡ w stanie obsÅ‚ugiwaÄ‡ wiÄ™ksze obciÄ…Å¼enie bez opÃ³Åºnienia w przetwarzaniu.\nZwiÄ™kszona iloÅ›Ä‡ danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, iloÅ›Ä‡ generowanych danych moÅ¼e byÄ‡ olbrzymia. Potrzebna jest elastycznoÅ›Ä‡: System musi automatycznie dostosowywaÄ‡ zasoby w zaleÅ¼noÅ›ci od obciÄ…Å¼enia.\n\n\nRozwiÄ…zanie:\nWykorzystanie skalowalnych systemÃ³w chmurowych, ktÃ³re pozwalajÄ… na dynamiczne zwiÄ™kszanie zasobÃ³w obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarzÄ…dzania kontenerami i automatycznego skalowania mikroserwisÃ³w. Technologie strumieniowe (Apache Kafka, Apache Flink) umoÅ¼liwiajÄ…ce przetwarzanie danych w sposÃ³b wydajny i rozproszony.\n\n\n\nOpÃ³Åºnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, kaÅ¼de opÃ³Åºnienie w przetwarzaniu danych moÅ¼e mieÄ‡ powaÅ¼ne konsekwencje. Dotyczy to zwÅ‚aszcza obszarÃ³w takich jak:\nWykrywanie oszustw: W przypadku systemÃ³w pÅ‚atnoÅ›ci online, opÃ³Åºnienie w analizie transakcji moÅ¼e oznaczaÄ‡ przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentÃ³w: OpÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na skutecznoÅ›Ä‡ reakcji w sytuacjach kryzysowych.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie algorytmÃ³w optymalizujÄ…cych czas przetwarzania, np. stream processing z wykorzystaniem systemÃ³w takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliÅ¼ej ÅºrÃ³dÅ‚a (np. urzÄ…dzenia IoT), aby zmniejszyÄ‡ opÃ³Åºnienia w transmisji danych do chmury.\n\n\n\nJakoÅ›Ä‡ danych i zarzÄ…dzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizowaÄ‡ dane w czasie rzeczywistym, ale takÅ¼e zapewniÄ‡ ich wysokÄ… jakoÅ›Ä‡. W przeciwnym razie analizy mogÄ… prowadziÄ‡ do bÅ‚Ä™dnych wnioskÃ³w lub opÃ³ÅºnieÅ„ w reagowaniu na nieprawidÅ‚owe dane.\nZanieczyszczone dane: W systemach real-time dane czÄ™sto sÄ… niepeÅ‚ne, brudne, bÅ‚Ä™dne lub nieuporzÄ…dkowane. Zmiana charakterystyki danych: Dane mogÄ… zmieniaÄ‡ siÄ™ w czasie, co moÅ¼e utrudniaÄ‡ ich przetwarzanie i analizÄ™. #### RozwiÄ…zanie:\nData cleansing i data validation na wstÄ™pnym etapie procesu. Automatyczne systemy monitorowania jakoÅ›ci danych w celu wykrywania bÅ‚Ä™dÃ³w w czasie rzeczywistym. ZarzÄ…dzanie danymi w strumieniu: NarzÄ™dzia takie jak Apache Kafka pozwalajÄ… na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ integracji systemÃ³w\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym czÄ™sto muszÄ… wspÃ³Å‚pracowaÄ‡ z istniejÄ…cymi systemami IT i ÅºrÃ³dÅ‚ami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemÃ³w, zwÅ‚aszcza w rozproszonej architekturze, moÅ¼e byÄ‡ skomplikowana.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie API do Å‚atwiejszej integracji z zewnÄ™trznymi systemami. Mikroserwisy i konteneryzacja z pomocÄ… narzÄ™dzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, ktÃ³re umoÅ¼liwia Å‚atwÄ… integracjÄ™ rÃ³Å¼nych ÅºrÃ³deÅ‚ danych oraz zapewnia elastycznoÅ›Ä‡ w dostosowywaniu systemÃ³w do rosnÄ…cych potrzeb.\n\n\n\nBezpieczeÅ„stwo i prywatnoÅ›Ä‡\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z ogromnÄ… iloÅ›ciÄ… wraÅ¼liwych informacji, szczegÃ³lnie w branÅ¼ach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, Å¼e dane sÄ… odpowiednio chronione przed nieautoryzowanym dostÄ™pem, jest kluczowe.\nOchrona danych w czasie transmisji: MuszÄ… byÄ‡ szyfrowane zarÃ³wno podczas przesyÅ‚ania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym moÅ¼e byÄ‡ celem atakÃ³w, takich jak DDoS czy SQL injection.\n\n\nRozwiÄ…zanie:\nSzyfrowanie danych zarÃ³wno w spoczynku, jak i podczas przesyÅ‚ania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeÅ„stwa. ZgodnoÅ›Ä‡ z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarzÄ…dzanie bÅ‚Ä™dami i awariami\n\nWyzwanie:\nBÅ‚Ä™dy i awarie w systemach real-time mogÄ… prowadziÄ‡ do powaÅ¼nych konsekwencji, w tym utraty danych, opÃ³ÅºnieÅ„ w analizach czy nawet usuniÄ™cia usÅ‚ug. W systemach rozproszonych trudno jest osiÄ…gnÄ…Ä‡ peÅ‚nÄ… niezawodnoÅ›Ä‡.\n\n\nRozwiÄ…zanie:\nRedundancja: Tworzenie kopii zapasowych systemÃ³w i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), ktÃ³re pozwalajÄ… na szybkie wykrycie i naprawienie problemÃ³w. ZarzÄ…dzanie stanem: DziÄ™ki uÅ¼yciu narzÄ™dzi jak Apache Kafka, moÅ¼na ponownie przetwarzaÄ‡ dane, jeÅ›li wystÄ…piÅ‚ bÅ‚Ä…d w transmisji.\n\n\n\nKoszty zwiÄ…zane z infrastrukturÄ…\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, ktÃ³ra zapewni odpowiedniÄ… moc obliczeniowÄ… i pamiÄ™Ä‡. To moÅ¼e wiÄ…zaÄ‡ siÄ™ z duÅ¼ymi kosztami, szczegÃ³lnie gdy dane muszÄ… byÄ‡ przechowywane i przetwarzane w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\n\n\nRozwiÄ…zanie:\nChmura obliczeniowa: MoÅ¼liwoÅ›Ä‡ elastycznego skalowania zasobÃ³w w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalajÄ… na uruchamianie procesÃ³w bez potrzeby utrzymywania staÅ‚ej infrastruktury.\nChociaÅ¼ analiza danych w czasie rzeczywistym oferuje ogromne korzyÅ›ci, wiÄ…Å¼e siÄ™ takÅ¼e z wieloma wyzwaniami. WÅ‚aÅ›ciwa architektura, narzÄ™dzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogÄ… pomÃ³c w przezwyciÄ™Å¼eniu wielu z tych trudnoÅ›ci. Warto rÃ³wnieÅ¼ pamiÄ™taÄ‡ o koniecznoÅ›ci zapewnienia wysokiej jakoÅ›ci danych, ich bezpieczeÅ„stwa, a takÅ¼e elastycznoÅ›ci i skalowalnoÅ›ci systemÃ³w, ktÃ³re bÄ™dÄ… w stanie sprostaÄ‡ rosnÄ…cym wymaganiom."
  },
  {
    "objectID": "wyklad4.html",
    "href": "wyklad4.html",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym."
  },
  {
    "objectID": "wyklad4.html#historia-podejÅ›cia-do-architektury",
    "href": "wyklad4.html#historia-podejÅ›cia-do-architektury",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym."
  },
  {
    "objectID": "wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "href": "wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "title": "WykÅ‚ad 4",
    "section": "WpÅ‚yw technologii na systemy informatyczne",
    "text": "WpÅ‚yw technologii na systemy informatyczne\nKomunikacja sieciowa, relacyjne bazy danych, rozwiÄ…zania chmurowe oraz Big Data znaczÄ…co zmieniÅ‚y sposÃ³b budowania systemÃ³w informatycznych i wykonywania w nich pracy.\nPodobnie, narzÄ™dzia do przekazu informacji â€“ takie jak gazeta, radio, telewizja, internet, komunikatory i media spoÅ‚ecznoÅ›ciowe â€“ wpÅ‚ynÄ™Å‚y na interakcje miÄ™dzyludzkie oraz struktury spoÅ‚eczne.\nKaÅ¼de nowe medium technologiczne ksztaÅ‚tuje sposÃ³b, w jaki ludzie korzystajÄ… z informatyki i postrzegajÄ… jej rolÄ™ w codziennym Å¼yciu."
  },
  {
    "objectID": "wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "href": "wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "title": "WykÅ‚ad 4",
    "section": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT",
    "text": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT\nJednym z najpopularniejszych podejÅ›Ä‡ do budowy systemÃ³w informatycznych jest koncepcja mikrousÅ‚ug (microservices).\nJest ona szeroko stosowana zarÃ³wno w tworzeniu oprogramowania, jak i w prowadzeniu firm opartych na analizie danych (Data-Driven).\n\nGÅ‚Ã³wne zalety mikroserwisÃ³w:\n\nWydajnoÅ›Ä‡ â€“ kaÅ¼da usÅ‚uga realizuje jedno, dobrze okreÅ›lone zadanie (â€œrÃ³b jednÄ… rzecz, ale dobrzeâ€).\nElastycznoÅ›Ä‡ â€“ umoÅ¼liwiajÄ… Å‚atwe modyfikacje i skalowanie systemu.\nPrzejrzystoÅ›Ä‡ architektury â€“ system skÅ‚ada siÄ™ z niewielkich, niezaleÅ¼nych moduÅ‚Ã³w.\n\nMikroserwisy moÅ¼na porÃ³wnaÄ‡ do czystych funkcji w programowaniu funkcyjnym â€“ kaÅ¼da usÅ‚uga dziaÅ‚a niezaleÅ¼nie i posiada jasno okreÅ›lone wejÅ›cia oraz wyjÅ›cia.\nAby umoÅ¼liwiÄ‡ komunikacjÄ™ miÄ™dzy mikroserwisami, czÄ™sto wykorzystuje siÄ™ Application Programming Interfaces (API), ktÃ³re pozwalajÄ… na wymianÄ™ danych i integracjÄ™ rÃ³Å¼nych usÅ‚ug.\n\n\nPrzykÅ‚ad API w mikroserwisach â€“ Python & FastAPI\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy mikroserwis REST API w Pythonie z uÅ¼yciem FastAPI, ktÃ³ry zwraca informacje o uÅ¼ytkownikach:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# PrzykÅ‚adowe dane uÅ¼ytkownikÃ³w\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane uÅ¼ytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nJak to dziaÅ‚a?\n\nUruchamiamy serwer FastAPI.\nMoÅ¼emy uzyskaÄ‡ dane uÅ¼ytkownika, wysyÅ‚ajÄ…c Å¼Ä…danie GET do http://127.0.0.1:8000/users/1.\nAPI zwrÃ³ci dane w formacie JSON, np.:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousÅ‚ug jest wykorzystanie interfejsÃ³w API (Application Programming Interface).\nAPI umoÅ¼liwia komunikacjÄ™ i integracjÄ™ miÄ™dzy rÃ³Å¼nymi mikroserwisami, podobnie jak strona internetowa komunikuje siÄ™ z przeglÄ…darkÄ….\nGdy odwiedzasz stronÄ™ internetowÄ…, serwer wysyÅ‚a kod, ktÃ³ry Twoja przeglÄ…darka interpretuje i wyÅ›wietla jako stronÄ™ internetowÄ….\nPodobnie dziaÅ‚a API â€“ wysyÅ‚a odpowiedzi na zapytania klienta."
  },
  {
    "objectID": "wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "href": "wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "title": "WykÅ‚ad 4",
    "section": "Przypadek biznesowy: Model ML jako usÅ‚uga",
    "text": "Przypadek biznesowy: Model ML jako usÅ‚uga\nZaÅ‚Ã³Å¼my, Å¼e pracujesz w firmie zajmujÄ…cej siÄ™ sprzedaÅ¼Ä… nieruchomoÅ›ci w Bostonie.\nChcesz zwiÄ™kszyÄ‡ sprzedaÅ¼ i poprawiÄ‡ jakoÅ›Ä‡ obsÅ‚ugi klientÃ³w poprzez nowÄ… aplikacjÄ™ mobilnÄ…, z ktÃ³rej moÅ¼e korzystaÄ‡ nawet 1 000 000 uÅ¼ytkownikÃ³w jednoczeÅ›nie.\nJednym z rozwiÄ…zaÅ„ jest udostÄ™pnienie prognozy wartoÅ›ci domu w czasie rzeczywistym.\nGdy uÅ¼ytkownik prosi o wycenÄ™ nieruchomoÅ›ci, aplikacja wysyÅ‚a zapytanie do serwera, ktÃ³ry przetwarza je za pomocÄ… modelu Machine Learning (ML) i zwraca oszacowanÄ… wartoÅ›Ä‡.\n\nCzym jest serwowanie modelu ML?\nTrening dobrego modelu ML to dopiero pierwszy krok caÅ‚ego procesu.\nAby model byÅ‚ uÅ¼yteczny, musisz go udostÄ™pniÄ‡ uÅ¼ytkownikom koÅ„cowym, np. w formie API.\n\n\nJak to zrobiÄ‡?\n\nPotrzebujesz:\n\n\nwytrenowanego modelu ML,\ninterpreter modelu (np. TensorFlow, Scikit-Learn, PyTorch),\ndanych wejÅ›ciowych dla modelu.\n\n\nKluczowe metryki jakoÅ›ci serwowania modelu:\nCzas odpowiedzi (latency),\nKoszt uruchomienia modelu (infrastruktura serwerowa),\nLiczba zapytaÅ„ na sekundÄ™ (QPS â€“ Queries Per Second).\n\n\nUdostÄ™pnianie danych miÄ™dzy systemami zawsze byÅ‚o kluczowym wyzwaniem w tworzeniu oprogramowania.\nW obszarze tradycyjnego DevOps koncentrujemy siÄ™ na infrastrukturze, a w MLOps â€“ na wdraÅ¼aniu i utrzymaniu modeli uczenia maszynowego.\n\nZbudowanie systemu przygotowanego do Å›rodowiska produkcyjnego jest bardziej skomplikowane niÅ¼ wytrenowanie samego modelu:\n\nczyszczenie i zaÅ‚adowanie odpowiednich i zwalidowanych danych\nObliczenie zmiennych i ich serwowanie na wÅ‚aÅ›ciwym Å›rodowisku\nSerwowanie modelu w sposÃ³b najbardziej efektywny ze wzglÄ™du na koszty\nWersjonowanie, Å›ledzenie i udostÄ™pnianie danych i modeli oraz innych artefaktÃ³w\nMonitorowanie infrastruktury i modelu\nWdroÅ¼enie modelu na skalowalnej infrastrukturze\nAutomatyzacja procesu wdraÅ¼ania i treningu"
  },
  {
    "objectID": "wyklad4.html#jak-dziaÅ‚a-api",
    "href": "wyklad4.html#jak-dziaÅ‚a-api",
    "title": "WykÅ‚ad 4",
    "section": "Jak dziaÅ‚a API?",
    "text": "Jak dziaÅ‚a API?\nKiedy wywoÅ‚asz interfejs API, serwer otrzymuje Twoje Å¼Ä…danie, przetwarza je i generuje odpowiedÅº.\nJeÅ›li wszystko dziaÅ‚a poprawnie, otrzymasz wynik w formacie JSON lub XML.\nJeÅ›li wystÄ…pi bÅ‚Ä…d, API zwrÃ³ci kod bÅ‚Ä™du HTTP (np. 400 â€“ nieprawidÅ‚owe Å¼Ä…danie, 500 â€“ bÅ‚Ä…d serwera).\n\nKluczowe zasady REST API:\n\nKlient-Serwer â†’ Klient (np. aplikacja mobilna) wysyÅ‚a Å¼Ä…danie HTTP do API hostowanego na serwerze, ktÃ³ry zwraca odpowiedÅº.\nDziaÅ‚a to identycznie jak przeglÄ…darka internetowa, ktÃ³ra wysyÅ‚a Å¼Ä…danie do serwera WWW i otrzymuje stronÄ™ HTML.\nBezstanowoÅ›Ä‡ â†’ KaÅ¼de Å¼Ä…danie klienta musi zawieraÄ‡ wszystkie niezbÄ™dne informacje do przetworzenia odpowiedzi,\nAPI nie powinno przechowywaÄ‡ informacji o wczeÅ›niejszych Å¼Ä…daniach uÅ¼ytkownika.\n\n\n\nPrzykÅ‚ad: API serwujÄ…ce model ML\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy serwis API, ktÃ³ry udostÄ™pnia model ML do prognozowania ceny nieruchomoÅ›ci,\nz wykorzystaniem FastAPI oraz Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wczeÅ›niej wytrenowany model ML (np. regresjÄ™ liniowÄ…)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Prognoza ceny nieruchomoÅ›ci na podstawie cech:\n    - area (powierzchnia w mÂ²),\n    - bedrooms (liczba sypialni),\n    - age (wiek budynku w latach).\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n- domenÄ™, \n- port, \n- dodatkowe Å›cieÅ¼ki, \n- zapytanie\nMetody HTTP:\n- GET, \n- POST\nNagÅ‚Ã³wki HTTP zawierajÄ…:\n- informacje o autoryzacji, \n- cookies metadata\n\nCaÅ‚a informacja zawarta jest w Content-Type: application/json, text â€¦ Accept: application/json, Authorization: Basic abase64string, Tokens 4. CiaÅ‚o zapytania\nNajczÄ™Å›ciej wybieranym formatem dla wymiany informacji miÄ™dzy serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt sÅ‚ownika - â€œkluczâ€: â€œwartoÅ›Ä‡â€.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedÅº - Response\n\nTreÅ›Ä‡ odpowiedzi przekazywana jest razem z nagÅ‚Ã³wkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: â€œContent-Typeâ€ =&gt; â€application/json; charset=utf-8â€, â€Serverâ€ =&gt; â€Genie/Julia/1.8.5â€\nTreÅ›Ä‡ (ciaÅ‚o) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidÅ‚owe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API."
  },
  {
    "objectID": "wyklad4.html#publikujsubskrybuj",
    "href": "wyklad4.html#publikujsubskrybuj",
    "title": "WykÅ‚ad 4",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyÅ‚ania wiadomoÅ›ci â€Publikuj/Subskrybujâ€ ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujÄ…cy siÄ™ tym, Å¼e nadawca (publikujÄ…cy) fragmentu danych (wiadomoÅ›ci) nie kieruje go wprost do odbiorcy. pub/sub to systemy, ktÃ³re czÄ™sto posiadajÄ… brokera czyli centralny punkt, w ktÃ³rym znajdujÄ… siÄ™ wiadomoÅ›ci."
  },
  {
    "objectID": "wyklad4.html#apache-kafka",
    "href": "wyklad4.html#apache-kafka",
    "title": "WykÅ‚ad 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicjÄ™:\n\nRozproszona platforma streamingowa\nCo to jest â€platforma rozproszonego przesyÅ‚ania strumieniowegoâ€?\nNajpierw chcÄ™ przypomnieÄ‡, czym jest â€strumieÅ„â€. Strumienie to po prostu nieograniczone dane, dane, ktÃ³re nigdy siÄ™ nie koÅ„czÄ…. CiÄ…gle ich przybywa i moÅ¼esz przetwarzaÄ‡ je w czasie rzeczywistym.\nA â€rozproszoneâ€? Rozproszony oznacza, Å¼e â€‹â€‹Kafka dziaÅ‚a w klastrze, a kaÅ¼dy wÄ™zeÅ‚ w grupie nazywa siÄ™ Brokerem. Ci brokerzy to po prostu serwery wykonujÄ…ce kopiÄ™ Apache Kafka.\nTak wiÄ™c Kafka to zestaw wspÃ³Å‚pracujÄ…cych ze sobÄ… maszyn, aby mÃ³c obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ nieograniczone dane w czasie rzeczywistym.\nBrokerzy sprawiajÄ…, Å¼e jest niezawodny, skalowalny i odporny na bÅ‚Ä™dy. Ale dlaczego panuje bÅ‚Ä™dne przekonanie, Å¼e Kafka to kolejny â€kolejkowy system przesyÅ‚ania wiadomoÅ›ciâ€?\nAby odpowiedzieÄ‡ na tÄ™ odpowiedÅº, musimy najpierw wyjaÅ›niÄ‡, jak dziaÅ‚a kolejkowe przesyÅ‚anie wiadomoÅ›ci.\n\n\nKolejkowy system przesyÅ‚ania wiadomoÅ›ci\nPrzesyÅ‚anie wiadomoÅ›ci, to po prostu czynnoÅ›Ä‡ wysyÅ‚ania wiadomoÅ›ci z jednego miejsca do drugiego. Ma trzech gÅ‚Ã³wnych â€œaktorÃ³wâ€:\n\nProducent: KtÃ³ry tworzy i wysyÅ‚a komunikaty do jednej lub wiÄ™cej kolejek;\nKolejka: struktura danych bufora, ktÃ³ra odbiera (od producentÃ³w) i dostarcza komunikaty (do konsumentÃ³w) w sposÃ³b FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jednÄ… lub wiÄ™cej kolejek i otrzymuje ich wiadomoÅ›ci po opublikowaniu.\n\nI to jest to; tak dziaÅ‚a przesyÅ‚anie wiadomoÅ›ci. Jak widaÄ‡, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Apache Kafka\nWiÄ™cej informacji na temat Kafki znajdziesz w tym linku.\nTeraz, gdy rozumiemy podstawy przesyÅ‚ania wiadomoÅ›ci, zagÅ‚Ä™bmy siÄ™ w Å›wiat Apache Kafka.\nW Kafce mamy dwa kluczowe pojÄ™cia: ProducentÃ³w (Producers) i KonsumentÃ³w (Consumers),\nktÃ³rzy dziaÅ‚ajÄ… w podobny sposÃ³b jak w klasycznych systemach kolejkowych, produkujÄ…c i konsumujÄ…c wiadomoÅ›ci.\n\n\nJak widaÄ‡, Kafka przypomina klasyczny system przesyÅ‚ania wiadomoÅ›ci, jednak w odrÃ³Å¼nieniu od tradycyjnych kolejek,\nzamiast pojÄ™cia kolejki (queue) mamy Tematy (Topics).\n\n\nTematy (Topics) i Partycje (Partitions)\nTemat (Topic) to podstawowy kanaÅ‚ przesyÅ‚ania danych w Kafce.\nMoÅ¼na go porÃ³wnaÄ‡ do folderu, w ktÃ³rym przechowywane sÄ… wiadomoÅ›ci.\nKaÅ¼dy temat posiada jednÄ… lub wiÄ™cej partycji (Partitions) â€“ podziaÅ‚ ten wpÅ‚ywa na skalowalnoÅ›Ä‡ i rÃ³wnowaÅ¼enie obciÄ…Å¼enia.\nPodczas tworzenia tematu okreÅ›lamy liczbÄ™ partycji.\n\nKluczowe cechy tematÃ³w i partycji:\n\nTemat to logiczna jednostka, do ktÃ³rej producenci wysyÅ‚ajÄ… wiadomoÅ›ci, a konsumenci je odczytujÄ….\nPartycja to fizyczny podziaÅ‚ tematu. MoÅ¼na jÄ… porÃ³wnaÄ‡ do plikÃ³w w folderze.\nOffset â€“ kaÅ¼da wiadomoÅ›Ä‡ w partycji otrzymuje unikalny identyfikator (offset),\nktÃ³ry pozwala konsumentom Å›ledziÄ‡, ktÃ³re wiadomoÅ›ci zostaÅ‚y juÅ¼ przetworzone.\nKafka przechowuje wiadomoÅ›ci na dysku, dziÄ™ki czemu moÅ¼e je ponownie odczytaÄ‡ (w przeciwieÅ„stwie do klasycznych kolejek, gdzie wiadomoÅ›Ä‡ jest usuwana po przetworzeniu).\nKonsumenci odczytujÄ… wiadomoÅ›ci sekwencyjnie, od najstarszej do najnowszej.\nW przypadku awarii konsument moÅ¼e wznowiÄ‡ przetwarzanie od ostatniego zapisanego offsetu.\n\n\n\n\n\n\nBrokerzy (Brokers) i Klaster Kafka\nKafka dziaÅ‚a w sposÃ³b rozproszony â€“ oznacza to, Å¼e moÅ¼e skÅ‚adaÄ‡ siÄ™ z wielu brokerÃ³w (Brokers),\nktÃ³re wspÃ³Å‚pracujÄ… jako jeden klaster.\n\n\n\nKluczowe informacje o brokerach\n\nBroker to pojedynczy serwer w klastrze Kafki, odpowiedzialny za przechowywanie partycji tematÃ³w.\nKaÅ¼dy broker w klastrze ma unikalny identyfikator.\nAby zwiÄ™kszyÄ‡ dostÄ™pnoÅ›Ä‡ i niezawodnoÅ›Ä‡, Kafka wykorzystuje replikacjÄ™ danych.\nWspÃ³Å‚czynnik replikacji okreÅ›la, ile kopii danej partycji ma byÄ‡ przechowywane na rÃ³Å¼nych brokerach.\nJeÅ›li temat ma trzy partycje i wspÃ³Å‚czynnik replikacji rÃ³wny trzy,\noznacza to, Å¼e kaÅ¼da partycja zostanie powielona na trzech rÃ³Å¼nych brokerach.\n\nLiczba partycji powinna byÄ‡ dobrana w taki sposÃ³b, aby kaÅ¼dy broker miaÅ‚ co najmniej jednÄ… partycjÄ™ do obsÅ‚ugi.\n\n\n\nProducenci (Producers)\nW Kafka producenci to aplikacje lub usÅ‚ugi, ktÃ³re tworzÄ… i wysyÅ‚ajÄ… wiadomoÅ›ci do tematÃ³w.\nDziaÅ‚a to podobnie do systemÃ³w kolejkowych, z tÄ… rÃ³Å¼nicÄ…, Å¼e Kafka zapisuje wiadomoÅ›ci w partycjach.\n\nJak Kafka przypisuje wiadomoÅ›ci do partycji?\n\nWiadomoÅ›ci sÄ… rozsyÅ‚ane okrÄ™Å¼nie (round-robin) do dostÄ™pnych partycji.\nMoÅ¼emy okreÅ›liÄ‡ klucz wiadomoÅ›ci, a Kafka wyliczy jego hash,\naby okreÅ›liÄ‡, do ktÃ³rej partycji trafi wiadomoÅ›Ä‡.\nKlucz wiadomoÅ›ci determinuje przypisanie do partycji â€“ jeÅ›li temat zostaÅ‚ juÅ¼ utworzony,\nliczba partycji nie moÅ¼e byÄ‡ zmieniona bez zakÅ‚Ã³cenia tego mechanizmu.\n\nPrzykÅ‚ad przypisania wiadomoÅ›ci do partycji: - WiadomoÅ›Ä‡ 01 trafia do partycji 0 tematu Topic_1. - WiadomoÅ›Ä‡ 02 trafia do partycji 1 tego samego tematu. - Kolejna wiadomoÅ›Ä‡ moÅ¼e ponownie trafiÄ‡ do partycji 0, jeÅ›li stosujemy przypisanie round-robin.\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# WysyÅ‚amy wiadomoÅ›Ä‡ do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"Nowe mieszkanie na sprzedaÅ¼\"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"WiadomoÅ›Ä‡ wysÅ‚ana do tematu '{topic}'\")\n\n\n\nKonsumenci (Consumers)\nKonsumenci w Kafce odczytujÄ… i przetwarzajÄ… wiadomoÅ›ci z tematÃ³w. KaÅ¼dy konsument moÅ¼e naleÅ¼eÄ‡ do grupy konsumentÃ³w (Consumer Group), co pozwala na rÃ³wnolegÅ‚e przetwarzanie wiadomoÅ›ci. - JeÅ›li wielu konsumentÃ³w naleÅ¼y do tej samej grupy, Kafka rÃ³wnowaÅ¼y obciÄ…Å¼enie miÄ™dzy nimi. - JeÅ›li jeden konsument przestanie dziaÅ‚aÄ‡, Kafka automatycznie przypisze jego partycje do innego aktywnego konsumenta.\nPrzykÅ‚ad konsumenta w Pythonie:\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, ktÃ³ry nasÅ‚uchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Oczekiwanie na wiadomoÅ›ci...\")\n\nfor message in consumer:\n    print(f\"Otrzymano wiadomoÅ›Ä‡: {message.value.decode()}\")\nInnym waÅ¼nym pojÄ™ciem Kafki sÄ… â€Grupy konsumentÃ³wâ€. Jest to bardzo waÅ¼ne, gdy musimy skalowaÄ‡ odczytywanie wiadomoÅ›ci. Staje siÄ™ to bardzo kosztowne, gdy pojedynczy konsument musi czytaÄ‡ z wielu partycji, wiÄ™c musimy zrÃ³wnowaÅ¼yÄ‡ obciÄ…Å¼enie miÄ™dzy naszymi konsumentami, wtedy wchodzÄ… grupy konsumentÃ³w.\nDane z jednego tematu bÄ™dÄ… rÃ³wnowaÅ¼one obciÄ…Å¼eniem miÄ™dzy konsumentami, dziÄ™ki czemu moÅ¼emy zagwarantowaÄ‡, Å¼e nasi konsumenci bÄ™dÄ… w stanie obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ dane. IdeaÅ‚em jest posiadanie takiej samej liczby konsumentÃ³w w grupie, jakÄ… mamy jako partycje w temacie, w ten sposÃ³b kaÅ¼dy konsument czyta tylko z jednego. Podczas dodawania konsumentÃ³w do grupy naleÅ¼y uwaÅ¼aÄ‡, jeÅ›li liczba konsumentÃ³w jest wiÄ™ksza niÅ¼ liczba partycji, niektÃ³rzy konsumenci nie bÄ™dÄ… czytaÄ‡ z Å¼adnego tematu i pozostanÄ… bezczynni."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w."
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w."
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli VI bud G\n\n\n18-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 1\n\n\n25-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 2\n\n\n04-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 3 online\n\n11-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 4\n18-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 5\n\nWykÅ‚ad 5 koÅ„czy siÄ™ TESTEM: 20 pytaÅ„ - 30 minut. Test przeprowadzany jest za poÅ›rednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡).\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "plan_wyklady.html",
    "href": "plan_wyklady.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "ğŸ§‘â€ğŸ« WykÅ‚ady (teoria + case study z biznesu)\n1ï¸âƒ£ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs.Â nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs.Â strumieniowe (stream processing). Case study: Jak firmy przechodzÄ… od tabel do analizy strumieniowej? 2ï¸âƒ£ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture â€“ rÃ³Å¼nice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3ï¸âƒ£ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs.Â przyrostowe (online learning). Stochastic Gradient Descent (SGD) â€“ podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4ï¸âƒ£ Obiektowe programowanie w Pythonie w kontekÅ›cie ML Struktury klasowe dla modeli ML. Tworzenie pipelineâ€™Ã³w ML w Pythonie. Case study: Klasyfikacja wiadomoÅ›ci jako SPAM/NON-SPAM w strumieniu tekstÃ³w. 5ï¸âƒ£ Tworzenie API z reguÅ‚ami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. ğŸ›  Laboratoria (praktyka + implementacja w Pythonie)\nğŸ”¹ Lab 1: Struktury danych w Pythonie â€“ Pandas, SQL (PostgreSQL, SQLite). ğŸ”¹ Lab 2: Dane grafowe w analizie relacji â€“ NetworkX i algorytmy grafowe. ğŸ”¹ Lab 3: Analiza tekstÃ³w i NLP â€“ przetwarzanie danych tekstowych (spaCy, TF-IDF). ğŸ”¹ Lab 4: Strumieniowanie danych w Apache Kafka â€“ pierwsza aplikacja Python + Kafka. ğŸ”¹ Lab 5: Uczenie maszynowe na strumieniu â€“ klasyfikacja w czasie rzeczywistym (SGDClassifier). ğŸ”¹ Lab 6: Przygotowanie API w FastAPI â€“ serwowanie modelu ML. ğŸ”¹ Lab 7: Zastosowanie modelu ML w reguÅ‚ach decyzyjnych â€“ integracja API z logikÄ… biznesowÄ…. ğŸ”¹ Lab 8: Przetwarzanie obrazÃ³w w czasie rzeczywistym â€“ OpenCV + klasyfikacja wideo. ğŸ”¹ Lab 9: Wykrywanie oszustw w transakcjach finansowych â€“ online learning na Kafka. ğŸ”¹ Lab 10: Projekt koÅ„cowy â€“ budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#plan-wykÅ‚adu",
    "href": "plan_wyklady.html#plan-wykÅ‚adu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "ğŸ§‘â€ğŸ« WykÅ‚ady (teoria + case study z biznesu)\n1ï¸âƒ£ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs.Â nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs.Â strumieniowe (stream processing). Case study: Jak firmy przechodzÄ… od tabel do analizy strumieniowej? 2ï¸âƒ£ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture â€“ rÃ³Å¼nice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3ï¸âƒ£ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs.Â przyrostowe (online learning). Stochastic Gradient Descent (SGD) â€“ podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4ï¸âƒ£ Obiektowe programowanie w Pythonie w kontekÅ›cie ML Struktury klasowe dla modeli ML. Tworzenie pipelineâ€™Ã³w ML w Pythonie. Case study: Klasyfikacja wiadomoÅ›ci jako SPAM/NON-SPAM w strumieniu tekstÃ³w. 5ï¸âƒ£ Tworzenie API z reguÅ‚ami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. ğŸ›  Laboratoria (praktyka + implementacja w Pythonie)\nğŸ”¹ Lab 1: Struktury danych w Pythonie â€“ Pandas, SQL (PostgreSQL, SQLite). ğŸ”¹ Lab 2: Dane grafowe w analizie relacji â€“ NetworkX i algorytmy grafowe. ğŸ”¹ Lab 3: Analiza tekstÃ³w i NLP â€“ przetwarzanie danych tekstowych (spaCy, TF-IDF). ğŸ”¹ Lab 4: Strumieniowanie danych w Apache Kafka â€“ pierwsza aplikacja Python + Kafka. ğŸ”¹ Lab 5: Uczenie maszynowe na strumieniu â€“ klasyfikacja w czasie rzeczywistym (SGDClassifier). ğŸ”¹ Lab 6: Przygotowanie API w FastAPI â€“ serwowanie modelu ML. ğŸ”¹ Lab 7: Zastosowanie modelu ML w reguÅ‚ach decyzyjnych â€“ integracja API z logikÄ… biznesowÄ…. ğŸ”¹ Lab 8: Przetwarzanie obrazÃ³w w czasie rzeczywistym â€“ OpenCV + klasyfikacja wideo. ğŸ”¹ Lab 9: Wykrywanie oszustw w transakcjach finansowych â€“ online learning na Kafka. ğŸ”¹ Lab 10: Projekt koÅ„cowy â€“ budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#moje",
    "href": "plan_wyklady.html#moje",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Moje",
    "text": "Moje\n\nwprowadzenie\nBatch processing\n\n\ntypy danych\nBig data\nETL\nMAP Reduce\nSparkowe przetwarzanie klastrowe\nBazy SQL - OLTP, OLAP\n\n\nAPI online\n\n\nwystawienie serwisu LLM\nbatching\n\n\nNear Real-Time i Real Time\n\n\nStrumienie danych, definicje, biznes,\nLambda/Kappa\nPub Sub, Kafka"
  }
]