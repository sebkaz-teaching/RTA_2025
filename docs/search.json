[
  {
    "objectID": "kafka_codes/kafka1.html",
    "href": "kafka_codes/kafka1.html",
    "title": "Apache Kafka - Wprowadzenie",
    "section": "",
    "text": "Apache Kafka to system przetwarzania strumieniowego (event streaming), ktÃ³ry dziaÅ‚a jako rozproszony broker wiadomoÅ›ci. Pozwala na przesyÅ‚anie i przetwarzanie danych w czasie rzeczywistym.\nDomyÅ›lnym adresem naszego brokera jest broker:9092.\nW Apache Kafka dane sÄ… przechowywane w strukturach zwanych topicami, ktÃ³re peÅ‚niÄ… funkcjÄ™ kolejek komunikacyjnych.\nZarzÄ…dzanie KafkÄ… odbywa siÄ™ za pomocÄ… skryptÃ³w. W naszym przypadku bÄ™dÄ… to skrypty .sh.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Apache Kafka - Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#sprawdÅº-listÄ™-topicÃ³w",
    "href": "kafka_codes/kafka1.html#sprawdÅº-listÄ™-topicÃ³w",
    "title": "Apache Kafka - Wprowadzenie",
    "section": "1ï¸âƒ£ SprawdÅº listÄ™ topicÃ³w",
    "text": "1ï¸âƒ£ SprawdÅº listÄ™ topicÃ³w\nPamiÄ™taj, aby przejÅ›Ä‡ do katalogu domowego:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\n2ï¸âƒ£ UtwÃ³rz nowy topic o nazwie mytopic\nkafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092\n\n\n3ï¸âƒ£ UtwÃ³rz producenta\nTen skrypt pozwoli Ci wprowadzaÄ‡ eventy rÄ™cznie przez terminal. Opcje --property sÄ… dodatkowe i sÅ‚uÅ¼Ä… do analizy w tym przykÅ‚adzie.\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"\n\n\n4ï¸âƒ£ Consumer w Sparku\nOtwÃ³rz nowy terminal w miejscu, gdzie znajduje siÄ™ plik test_key_value.py, i uruchom program Consumera w Sparku.\n\n%%file test_key_value.py\nfrom pyspark.sql import SparkSession\n\nKAFKA_BROKER = 'broker:9092'\nKAFKA_TOPIC = 'mytopic'\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\ndf = (spark.readStream.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n      .option(\"subscribe\", KAFKA_TOPIC)\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n     )\n\n# Konwersja danych binarnych na stringi\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n .writeStream \\\n .format(\"console\") \\\n .outputMode(\"append\") \\\n .start() \\\n .awaitTermination()\n\nPamiÄ™taj, Å¼e Apache Spark nie posiada domyÅ›lnego konektora do Kafki, dlatego uruchom proces za pomocÄ… spark-submit i pobierz odpowiedni pakiet w Scali:\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py\n\n\n5ï¸âƒ£ Przetestuj przesyÅ‚anie danych\nW terminalu z uruchomionym producentem wpisz tekst w postaci:\njan:45\nalicja:20\nSprawdÅº, co pojawia siÄ™ w oknie aplikacji Consumera.\n\n\n6ï¸âƒ£ ZakoÅ„czenie procesu\nPo zakoÅ„czeniu pokazu uÅ¼yj Ctrl+C, aby zamknÄ…Ä‡ zarÃ³wno okno producenta, jak i aplikacjÄ™ Spark.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Apache Kafka - Wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html",
    "href": "lectures/wyklad5.html",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n1. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania:\n\nAnaliza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\n2. Algorytmy dokonujÄ…ce wielu obliczeÅ„\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA.\nProces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych.\nCzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\n\nğŸ”¹ Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\n3. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych.\nMoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\n\nğŸ”¹ Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#algorytmy",
    "href": "lectures/wyklad5.html#algorytmy",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n1. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania:\n\nAnaliza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\n2. Algorytmy dokonujÄ…ce wielu obliczeÅ„\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA.\nProces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych.\nCzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\n\nğŸ”¹ Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\n3. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych.\nMoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\n\nğŸ”¹ Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "href": "lectures/wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "title": "WykÅ‚ad 5",
    "section": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w",
    "text": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w\nW wielu przypadkach modelowanie jest wykorzystywane w sytuacjach krytycznych, np. w oprogramowaniu do podawania lekÃ³w. W takich sytuacjach kluczowe staje siÄ™ wyjaÅ›nienie przyczyny kaÅ¼dego wyniku dziaÅ‚ania algorytmu. Jest to konieczne, aby zapewniÄ‡, Å¼e decyzje podejmowane na jego podstawie sÄ… wolne od bÅ‚Ä™dÃ³w i uprzedzeÅ„.\nZdolnoÅ›Ä‡ algorytmu do wskazania mechanizmÃ³w generujÄ…cych wyniki nazywamy moÅ¼liwoÅ›ciÄ… wyjaÅ›nienia. Analiza etyczna stanowi standardowy element procesu walidacji algorytmu.\nUzyskanie wysokiej wyjaÅ›nialnoÅ›ci jest szczegÃ³lnie trudne w przypadku algorytmÃ³w uczenia maszynowego (ML) i gÅ‚Ä™bokiego uczenia (DL). Na przykÅ‚ad banki korzystajÄ…ce z algorytmÃ³w do podejmowania decyzji kredytowych muszÄ… zapewniÄ‡ transparentnoÅ›Ä‡ i wskazaÄ‡ powody wydanej decyzji.\nJednÄ… z metod poprawy wyjaÅ›nialnoÅ›ci algorytmÃ³w jest LIME (Local Interpretable Model-Agnostic Explanations), opublikowana w 2016 roku. Metoda ta polega na wprowadzaniu niewielkich zmian w danych wejÅ›ciowych i analizowaniu ich wpÅ‚ywu na wynik, co pozwala okreÅ›liÄ‡ lokalne zasady podejmowania decyzji przez model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# PodziaÅ‚ na zbiÃ³r treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykÅ‚adu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # WybÃ³r losowego przykÅ‚adu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# WyÅ›wietlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nJak dziaÅ‚a ten kod? 1. Åadowanie danych i trenowanie modelu\n\nUÅ¼ywamy zbioru Iris, ktÃ³ry zawiera 150 przykÅ‚adÃ³w kwiatÃ³w z trzema gatunkami:\nSetosa\nVersicolor\nVirginica\nModel RandomForestClassifier trenuje siÄ™ na tych danych.\n\n\nTworzenie interpretowalnego modelu za pomocÄ… LIME\n\n\nLIME generuje lokalne wyjaÅ›nienia, czyli interpretuje model dla pojedynczych predykcji.\nWybieramy losowy przykÅ‚ad z danych testowych.\n\n\nEksploracja wyniku dla jednego przykÅ‚adu\n\n\nLIME modyfikuje lekko wartoÅ›ci wejÅ›ciowe i obserwuje, jak zmienia siÄ™ wynik predykcji.\nTworzy â€lokalnyâ€ model liniowy, ktÃ³ry pokazuje, ktÃ³re cechy miaÅ‚y najwiÄ™kszy wpÅ‚yw na decyzjÄ™.\n\nZaÅ‚Ã³Å¼my, Å¼e nasz model wybraÅ‚ przykÅ‚adowÄ… roÅ›linÄ™ i sklasyfikowaÅ‚ jÄ… jako Virginica.\nOto interpretacja wynikÃ³w:\n\nNajwaÅ¼niejsze cechy wpÅ‚ywajÄ…ce na decyzjÄ™ modelu:\n\n\nDÅ‚ugoÅ›Ä‡ pÅ‚atka (petal length): najwiÄ™kszy wpÅ‚yw na predykcjÄ™ (np. im wiÄ™ksza, tym wiÄ™ksze prawdopodobieÅ„stwo, Å¼e to Virginica).\nSzerokoÅ›Ä‡ pÅ‚atka (petal width): rÃ³wnieÅ¼ istotny czynnik (np. powyÅ¼ej pewnej wartoÅ›ci sugeruje Virginica).\nDÅ‚ugoÅ›Ä‡ kielicha (sepal length): mniejszy wpÅ‚yw, ale nadal istotny.\nSzerokoÅ›Ä‡ kielicha (sepal width): zwykle najmniej istotna cecha.\n\n\nWizualizacja wynikÃ³w\n\n\nLIME generuje wykres sÅ‚upkowy, ktÃ³ry pokazuje wpÅ‚yw kaÅ¼dej cechy na klasyfikacjÄ™.\nNa wykresie widaÄ‡, ktÃ³re cechy zwiÄ™kszaÅ‚y, a ktÃ³re zmniejszaÅ‚y prawdopodobieÅ„stwo przypisania do danej klasy.\n\n\nCo oznacza wynik?\n\n\nJeÅ›li model przewidziaÅ‚ klasÄ™ Virginica z wysokim prawdopodobieÅ„stwem, oznacza to, Å¼e kluczowe cechy (np. dÅ‚ugi pÅ‚atek) mocno wskazujÄ… na ten gatunek.\nJeÅ›li cechy miaÅ‚y zrÃ³Å¼nicowany wpÅ‚yw, oznacza to, Å¼e model miaÅ‚ pewne trudnoÅ›ci w klasyfikacji (np. szerokoÅ›Ä‡ pÅ‚atka nie byÅ‚a jednoznaczna).",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#detekcja-anomalii",
    "href": "lectures/wyklad5.html#detekcja-anomalii",
    "title": "WykÅ‚ad 5",
    "section": "Detekcja anomalii",
    "text": "Detekcja anomalii\n\nWartoÅ›Ä‡ odstajÄ…ca (Outlier)\nWartoÅ›Ä‡ odstajÄ…ca (ang. outlier) to obserwacja (wiersz w tabeli danych), ktÃ³ra jest znacznie oddalona od pozostaÅ‚ych elementÃ³w prÃ³bki. Oznacza to, Å¼e zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi niezaleÅ¼nymi i zaleÅ¼nymi dla tej obserwacji moÅ¼e rÃ³Å¼niÄ‡ siÄ™ od pozostaÅ‚ych przypadkÃ³w.\nDla pojedynczych zmiennych wartoÅ›ci odstajÄ…ce moÅ¼na okreÅ›liÄ‡, korzystajÄ…c z wykresu pudeÅ‚kowego (box plot). Wykres ten bazuje na kwartylach:\n\nPierwszy kwartyl \\(Q_1\\) i trzeci kwartyl \\(Q_3\\) wyznaczajÄ… boki pudeÅ‚ka,\nDrugi kwartyl \\(Q_2\\) (mediana) jest zaznaczony wewnÄ…trz pudeÅ‚ka,\n\nWartoÅ›ci odstajÄ…ce speÅ‚niajÄ… zaleÅ¼noÅ›Ä‡:\n\\[\nx_{out} &lt; Q_1 - 1.5 \\times IQR \\quad \\text{lub} \\quad x_{out} &gt; Q_3 + 1.5 \\times IQR\n\\]\nGdzie: \\[\nIQR = Q_3 - Q_1\n\\]\nPrzykÅ‚adem wartoÅ›ci odstajÄ…cej moÅ¼e byÄ‡ bolid FormuÅ‚y 1 â€“ pod wzglÄ™dem prÄ™dkoÅ›ci jest on anomaliÄ… wÅ›rÃ³d zwykÅ‚ych samochodÃ³w.\n\n\nWykorzystanie detekcji anomalii\nWykrywanie wartoÅ›ci odstajÄ…cych ma szerokie zastosowanie, np.:\n\nFinanse â€“ wykrywanie transakcji fraudowych w analizie danych bankowych,\nCyberbezpieczeÅ„stwo â€“ identyfikacja intruzÃ³w w sieci na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w,\nMedycyna â€“ monitorowanie parametrÃ³w zdrowotnych i wykrywanie nieprawidÅ‚owoÅ›ci,\nPrzemysÅ‚ â€“ wykrywanie wadliwych komponentÃ³w poprzez analizÄ™ obrazu.\n\n\n\nMetody wykrywania anomalii\n\n1. Metody nadzorowane (supervised learning)\nStosowane, gdy mamy oznaczone dane (np. przypadki oszustw w transakcjach).\n\nSieci neuronowe,\nAlgorytm K-najbliÅ¼szych sÄ…siadÃ³w (KNN),\nSieci Bayesowskie.\n\n\n\n2. Metody nienadzorowane (unsupervised learning)\nZakÅ‚adajÄ…, Å¼e wiÄ™kszoÅ›Ä‡ danych jest poprawna, a anomalie to niewielki odsetek przypadkÃ³w.\n\nKlasteryzacja metodÄ… K-Å›rednich (K-Means),\nAutoenkodery w sieciach neuronowych,\nTesty statystyczne.\n\n\n\n\nMetoda klasyczna â€“ detekcja na podstawie prawdopodobieÅ„stwa\nAby okreÅ›liÄ‡, czy dana obserwacja jest anomaliÄ…, moÅ¼na uÅ¼yÄ‡ prawdopodobieÅ„stwa \\(p(x)\\):\n\nJeÅ›li \\(p(x) &lt; \\epsilon\\), uznajemy wartoÅ›Ä‡ za odstajÄ…cÄ….\nW praktyce zakÅ‚adamy, Å¼e dane majÄ… rozkÅ‚ad normalny \\(N(\\mu, \\sigma)\\).\nSzacujemy parametry \\(\\mu\\) (Å›rednia) i \\(\\sigma^2\\) (wariancja) na podstawie prÃ³bki.\nNastÄ™pnie dla kaÅ¼dej wartoÅ›ci obliczamy prawdopodobieÅ„stwo jej wystÄ…pienia i porÃ³wnujemy z \\(\\epsilon\\).\n\nPrzykÅ‚ad: Analiza wynagrodzeÅ„ w firmie\nWykrywamy, czy w danej firmie sÄ… osoby o wynagrodzeniach znacznie odbiegajÄ…cych od Å›redniej.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PrzykÅ‚adowe wynagrodzenia w firmie (w tysiÄ…cach)\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\n# Obliczenie kwartylÃ³w\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\n# Definicja wartoÅ›ci odstajÄ…cych\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\n# Znalezienie outlierÃ³w\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\n# Wizualizacja\nsns.boxplot(salaries)\nplt.title(\"Wykres pudeÅ‚kowy wynagrodzeÅ„\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nWynik: Na wykresie pudeÅ‚kowym widaÄ‡, Å¼e 150 tys. to anomalia.\n\n\nIsolation Forest â€“ detekcja anomalii za pomocÄ… lasu izolacyjnego\nIsolation Forest to algorytm bazujÄ…cy na drzewach decyzyjnych, zaproponowany przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku. Identyfikuje anomalie poprzez izolowanie wartoÅ›ci odstajÄ…cych w procesie podziaÅ‚u danych:\n\nWybiera losowo cechÄ™ oraz wartoÅ›Ä‡ podziaÅ‚u,\nWartoÅ›ci odstajÄ…ce szybciej zostajÄ… odizolowane (sÄ… bliÅ¼ej korzenia drzewa),\nWynik jest agregowany na podstawie wielu drzew.\n\nJego zalety to niskie wymagania obliczeniowe i skutecznoÅ›Ä‡ w analizie wielowymiarowych danych.\nMetody detekcji anomalii sklearn\nPrzykÅ‚ad: Wykrywanie oszustw bankowych\nBank analizuje transakcje kartÄ… kredytowÄ… i wykrywa te, ktÃ³re mogÄ… byÄ‡ nieautoryzowane.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# PrzykÅ‚adowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duÅ¼a kwota, rzadkoÅ›Ä‡)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nWynik: Transakcja 5000 zÅ‚ zostanie wykryta jako anomalia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "WykÅ‚ad 1",
    "section": "",
    "text": "â³ Czas trwania: 1,5h\nğŸ¯ Cel wykÅ‚adu\nZapoznanie studentÃ³w z podstawami real-time analytics, rÃ³Å¼nicami miÄ™dzy trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez koniecznoÅ›ci przechowywania i oczekiwania na pÃ³Åºniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wnioskÃ³w i reakcji na zmieniajÄ…ce siÄ™ warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) â€“ dane sÄ… analizowane w ciÄ…gu milisekund lub sekund od ich wygenerowania.\nStreaming vs.Â Batch Processing â€“ analiza danych moÅ¼e odbywaÄ‡ siÄ™ w sposÃ³b ciÄ…gÅ‚y (streaming) lub w z gÃ³ry okreÅ›lonych interwaÅ‚ach (batch).\nIntegracja z IoT, AI i ML â€“ real-time analytics czÄ™sto wspÃ³Å‚pracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym â€“ np. natychmiastowa detekcja oszustw w transakcjach bankowych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "WykÅ‚ad 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowoÅ›Ä‡\n\nWykrywanie oszustw â€“ analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujÄ…cych na oszustwa.\nAutomatyczny trading â€“ systemy HFT (High-Frequency Trading) analizujÄ… miliony danych w uÅ‚amkach sekundy.\nDynamiczne oceny kredytowe â€“ natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym â€“ dynamiczne rekomendacje produktÃ³w na podstawie aktualnego zachowania uÅ¼ytkownika.\nDynamiczne ceny â€“ np. Uber, Amazon i hotele stosujÄ… dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediÃ³w spoÅ‚ecznoÅ›ciowych â€“ analiza nastrojÃ³w klientÃ³w i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej â€“ analiza logÃ³w w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystÄ…pieniem.\nSmart Cities â€“ analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji Å›wietlnej.\nAnalityka IoT â€“ urzÄ…dzenia IoT generujÄ… strumienie danych, ktÃ³re moÅ¼na analizowaÄ‡ w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentÃ³w â€“ analiza sygnaÅ‚Ã³w z urzÄ…dzeÅ„ medycznych w celu natychmiastowego wykrycia zagroÅ¼enia Å¼ycia.\nAnalityka epidemiologiczna â€“ Å›ledzenie rozprzestrzeniania siÄ™ chorÃ³b na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemÃ³w informatycznych, ktÃ³ry umoÅ¼liwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branÅ¼ach â€“ od finansÃ³w, przez e-commerce, aÅ¼ po ochronÄ™ zdrowia i IoT.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "lectures/wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "WykÅ‚ad 1",
    "section": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstniejÄ… trzy gÅ‚Ã³wne podejÅ›cia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKaÅ¼de z nich rÃ³Å¼ni siÄ™ szybkoÅ›ciÄ… przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing â€“ Przetwarzanie wsadowe\nğŸ“Œ Definicja:\nBatch Processing polega na zbieraniu duÅ¼ych iloÅ›ci danych i ich przetwarzaniu w okreÅ›lonych odstÄ™pach czasu (np. co godzinÄ™, codziennie, co tydzieÅ„).\nğŸ“Œ Cechy:\n\nâœ… Wysoka wydajnoÅ›Ä‡ dla duÅ¼ych zbiorÃ³w danych\nâœ… Przetwarzanie danych po ich zgromadzeniu\nâœ… Nie wymaga natychmiastowej analizy\nâœ… Zwykle taÅ„sze niÅ¼ przetwarzanie w czasie rzeczywistym\nâŒ OpÃ³Åºnienia â€“ wyniki sÄ… dostÄ™pne dopiero po zakoÅ„czeniu przetwarzania\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nGenerowanie raportÃ³w finansowych na koniec dnia/miesiÄ…ca\nAnaliza trendÃ³w sprzedaÅ¼y na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiÄ…ca\n\n# Agregacja danych - miesiÄ™czne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wynikÃ³w do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyÅ› chciaÅ‚ utworzyÄ‡ dane do przykÅ‚adu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics â€“ Analiza niemal w czasie rzeczywistym\nğŸ“Œ Definicja:\nNear Real-Time Analytics to analiza danych, ktÃ³ra odbywa siÄ™ z minimalnym opÃ³Åºnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie peÅ‚na analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duÅ¼e opÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na biznes.\nğŸ“Œ Cechy:\n\nâœ… Przetwarzanie danych w krÃ³tkich odstÄ™pach czasu (kilka sekund â€“ minut)\nâœ… UmoÅ¼liwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\nâœ… Optymalny balans miÄ™dzy kosztami a szybkoÅ›ciÄ…\nâŒ Nie nadaje siÄ™ do systemÃ³w wymagajÄ…cych natychmiastowej reakcji\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciÄ…gu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w\nAnaliza logÃ³w serwerÃ³w i sieci w celu wykrycia anomalii\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logÃ³w IT)\nAmazon Kinesis\n\nPrzykÅ‚ad producenta danych realizujÄ…cego tranzakcje wysyÅ‚ane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generujÄ…ca przykÅ‚adowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota miÄ™dzy 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# ZakoÅ„czenie dziaÅ‚ania producenta\nproducer.flush()\nproducer.close()\nPrzykÅ‚ad consumenta - programu sparawdzajÄ…cego zbyt duÅ¼e transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"ğŸš¨ Wykryto duÅ¼Ä… transakcjÄ™: {transaction}\")\nPrzykÅ‚adowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics â€“ Analiza w czasie rzeczywistym\nğŸ“Œ Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w uÅ‚amku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagajÄ…cych reakcji w czasie rzeczywistym, np. w transakcjach gieÅ‚dowych, systemach IoT czy cyberbezpieczeÅ„stwie.\nğŸ“Œ Cechy:\n\nâœ… Bardzo niskie opÃ³Åºnienie (milliseconds-seconds)\nâœ… UmoÅ¼liwia natychmiastowÄ… reakcjÄ™ systemu\nâœ… Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\nâŒ DroÅ¼sze i bardziej zÅ‚oÅ¼one technologicznie niÅ¼ batch processing\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nHigh-Frequency Trading (HFT) â€“ analiza i podejmowanie decyzji w transakcjach gieÅ‚dowych w milisekundach\nAutonomiczne samochody â€“ analiza strumieni danych z kamer i sensorÃ³w w czasie rzeczywistym\nCyberbezpieczeÅ„stwo â€“ detekcja atakÃ³w w sieciach komputerowych w uÅ‚amku sekundy\nAnalityka IoT â€“ np. natychmiastowa detekcja anomalii w danych z czujnikÃ³w przemysÅ‚owych\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\nğŸ” PorÃ³wnanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpÃ³Åºnienie\nMinuty â€“ godziny â€“ dni\nSekundy â€“ minuty\nMilisekundy â€“ sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w peÅ‚ni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\nğŸ“‰ Niski\nğŸ“ˆ Åšredni\nğŸ“ˆğŸ“ˆ Wysoki\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ implementacji\nğŸ“‰ Prosta\nğŸ“ˆ Åšrednia\nğŸ“ˆğŸ“ˆ Trudna\n\n\nPrzykÅ‚ady zastosowaÅ„\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\nğŸ“Œ Kiedy stosowaÄ‡ Batch Processing?\n\nâœ… Gdy nie wymagasz natychmiastowej analizy\nâœ… Gdy masz duÅ¼e iloÅ›ci danych, ale przetwarzane sÄ… one okresowo\nâœ… Gdy chcesz obniÅ¼yÄ‡ koszty\n\nğŸ“Œ Kiedy stosowaÄ‡ Near Real-Time Analytics?\n\nâœ… Gdy wymagasz analizy w krÃ³tkim czasie (sekundy â€“ minuty)\nâœ… Gdy potrzebujesz bardziej aktualnych danych, ale nie w peÅ‚nym real-time\nâœ… Gdy szukasz kompromisu miÄ™dzy wydajnoÅ›ciÄ… a kosztami\n\nğŸ“Œ Kiedy stosowaÄ‡ Real-Time Analytics?\n\nâœ… Gdy kaÅ¼da milisekunda ma znaczenie (np. gieÅ‚da, autonomiczne pojazdy)\nâœ… Gdy chcesz wykrywaÄ‡ oszustwa, anomalie lub incydenty natychmiast\nâœ… Gdy system musi natychmiast reagowaÄ‡ na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne â€“ w wielu przypadkach near real-time jest wystarczajÄ…ce i bardziej opÅ‚acalne. Kluczowe jest zrozumienie wymagaÅ„ biznesowych przed wyborem odpowiedniego rozwiÄ…zania.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "href": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "title": "WykÅ‚ad 1",
    "section": "Dlaczego Real-Time Analytics jest waÅ¼ne?",
    "text": "Dlaczego Real-Time Analytics jest waÅ¼ne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje siÄ™ coraz bardziej istotna w wielu branÅ¼ach, poniewaÅ¼ umoÅ¼liwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodÃ³w, dla ktÃ³rych real-time analytics jest waÅ¼ne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagowaÄ‡ na zmiany i wydarzenia w czasie rzeczywistym. DziÄ™ki temu moÅ¼na podejmowaÄ‡ decyzje szybciej, co jest kluczowe w dynamicznych Å›rodowiskach, takich jak:\n\nMarketing: Reklamy mogÄ… byÄ‡ dostosowane do zachowaÅ„ uÅ¼ytkownikÃ³w w czasie rzeczywistym (np. personalizacja treÅ›ci reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie kaÅ¼da minuta moÅ¼e oznaczaÄ‡ rÃ³Å¼nicÄ™ w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogÄ… monitorowaÄ‡ kluczowe wskaÅºniki operacyjne na bieÅ¼Ä…co. PrzykÅ‚ady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urzÄ…dzeÅ„ w fabrykach, aby natychmiast wykrywaÄ‡ awarie i zapobiegaÄ‡ przestojom.\nHealthtech: Åšledzenie parametrÃ³w Å¼yciowych pacjentÃ³w i wykrywanie anomalii, co moÅ¼e ratowaÄ‡ Å¼ycie.\n\n\n\nZwiÄ™kszenie efektywnoÅ›ci operacyjnej\nReal-time analytics umoÅ¼liwia natychmiastowe wykrywanie i eliminowanie problemÃ³w operacyjnych, zanim stanÄ… siÄ™ powaÅ¼niejsze. PrzykÅ‚ady:\n\nLogistyka: Åšledzenie przesyÅ‚ek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywnoÅ›Ä‡ i zmniejsza opÃ³Åºnienia.\nRetail: Monitorowanie poziomu zapasÃ³w na bieÅ¼Ä…co i dostosowywanie zamÃ³wieÅ„ do aktualnych potrzeb.\n\n\n\nKonkurencyjnoÅ›Ä‡\nOrganizacje, ktÃ³re wykorzystujÄ… analitykÄ™ w czasie rzeczywistym, majÄ… przewagÄ™ nad konkurencjÄ…, poniewaÅ¼ mogÄ… szybciej reagowaÄ‡ na zmiany na rynku, nowe potrzeby klientÃ³w i sytuacje kryzysowe. DziÄ™ki natychmiastowym informacjom:\n\nMoÅ¼na podejmowaÄ‡ decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywaÄ‡ lepsze relacje z klientami, reagujÄ…c na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doÅ›wiadczenia uÅ¼ytkownikÃ³w (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z uÅ¼ytkownikami w trakcie ich trwania. PrzykÅ‚ady:\n\nE-commerce: Analiza koszyka zakupowego uÅ¼ytkownika w czasie rzeczywistym, aby np. zaoferowaÄ‡ rabat lub przypomnieÄ‡ o porzuconych produktach.\nStreaming: Optymalizacja jakoÅ›ci usÅ‚ugi wideo/streamingowej w zaleÅ¼noÅ›ci od dostÄ™pnej przepustowoÅ›ci Å‚Ä…cza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym Å›wiecie peÅ‚nym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeÅ„stwa. PrzykÅ‚ady:\n\nCyberbezpieczeÅ„stwo: Real-time analytics umoÅ¼liwia wykrywanie podejrzanych dziaÅ‚aÅ„ w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztÃ³w\nDziÄ™ki analizie w czasie rzeczywistym moÅ¼na optymalizowaÄ‡ zasoby i zmniejszaÄ‡ koszty. Na przykÅ‚ad:\n\nZarzÄ…dzanie energiÄ…: Analiza zuÅ¼ycia energii w czasie rzeczywistym, umoÅ¼liwiajÄ…ca optymalizacjÄ™ wydatkÃ³w na energiÄ™ w firmach.\nOptymalizacja Å‚aÅ„cucha dostaw: DziÄ™ki bieÅ¼Ä…cemu Å›ledzeniu zapasÃ³w i dostaw moÅ¼na lepiej zarzÄ…dzaÄ‡ kosztami magazynowania i transportu.\n\n\n\nZdolnoÅ›Ä‡ do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, ktÃ³re mogÄ… przewidywaÄ‡ przyszÅ‚e zachowania lub problemy, a takÅ¼e je eliminowaÄ‡ zanim siÄ™ pojawiÄ…. Na przykÅ‚ad:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w poÅ‚Ä…czeniu z modelami predykcyjnymi pozwala przewidywaÄ‡ awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym moÅ¼na dostosowywaÄ‡ produkcjÄ™ lub zapasy na podstawie bieÅ¼Ä…cych trendÃ³w.\n\nReal-time analytics to nie tylko analiza danych â€“ to kluczowy element strategii firm w Å›wiecie, ktÃ³ry wymaga szybkich reakcji, elastycznoÅ›ci i dostosowywania siÄ™ do zmieniajÄ…cego siÄ™ otoczenia. Firmy, ktÃ³re wdraÅ¼ajÄ… te technologie, mogÄ… znaczÄ…co poprawiÄ‡ swoje wyniki finansowe, obsÅ‚ugÄ™ klienta, wydajnoÅ›Ä‡ operacyjnÄ…, a takÅ¼e przewagÄ™ konkurencyjnÄ….",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z wieloma wyzwaniami i trudnoÅ›ciami, ktÃ³re trzeba rozwiÄ…zaÄ‡, aby systemy real-time dziaÅ‚aÅ‚y efektywnie i niezawodnie. Pomimo ogromnego potencjaÅ‚u, jaki daje moÅ¼liwoÅ›Ä‡ natychmiastowego przetwarzania danych, realizacja tych procesÃ³w w praktyce wiÄ…Å¼e siÄ™ z licznymi problemami technologicznymi, organizacyjnymi i dotyczÄ…cymi zarzÄ…dzania danymi.\nPoniÅ¼ej przedstawiamy najwaÅ¼niejsze wyzwania oraz moÅ¼liwe rozwiÄ…zania, ktÃ³re naleÅ¼y uwzglÄ™dniÄ‡ podczas implementacji systemÃ³w analizy danych w czasie rzeczywistym.\n\nSkalowalnoÅ›Ä‡ systemÃ³w\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadaÅ„. W miarÄ™ jak iloÅ›Ä‡ generowanych danych roÅ›nie, systemy muszÄ… byÄ‡ w stanie obsÅ‚ugiwaÄ‡ wiÄ™ksze obciÄ…Å¼enie bez opÃ³Åºnienia w przetwarzaniu.\nZwiÄ™kszona iloÅ›Ä‡ danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, iloÅ›Ä‡ generowanych danych moÅ¼e byÄ‡ olbrzymia. Potrzebna jest elastycznoÅ›Ä‡: System musi automatycznie dostosowywaÄ‡ zasoby w zaleÅ¼noÅ›ci od obciÄ…Å¼enia.\n\n\nRozwiÄ…zanie:\nWykorzystanie skalowalnych systemÃ³w chmurowych, ktÃ³re pozwalajÄ… na dynamiczne zwiÄ™kszanie zasobÃ³w obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarzÄ…dzania kontenerami i automatycznego skalowania mikroserwisÃ³w. Technologie strumieniowe (Apache Kafka, Apache Flink) umoÅ¼liwiajÄ…ce przetwarzanie danych w sposÃ³b wydajny i rozproszony.\n\n\n\nOpÃ³Åºnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, kaÅ¼de opÃ³Åºnienie w przetwarzaniu danych moÅ¼e mieÄ‡ powaÅ¼ne konsekwencje. Dotyczy to zwÅ‚aszcza obszarÃ³w takich jak:\nWykrywanie oszustw: W przypadku systemÃ³w pÅ‚atnoÅ›ci online, opÃ³Åºnienie w analizie transakcji moÅ¼e oznaczaÄ‡ przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentÃ³w: OpÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na skutecznoÅ›Ä‡ reakcji w sytuacjach kryzysowych.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie algorytmÃ³w optymalizujÄ…cych czas przetwarzania, np. stream processing z wykorzystaniem systemÃ³w takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliÅ¼ej ÅºrÃ³dÅ‚a (np. urzÄ…dzenia IoT), aby zmniejszyÄ‡ opÃ³Åºnienia w transmisji danych do chmury.\n\n\n\nJakoÅ›Ä‡ danych i zarzÄ…dzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizowaÄ‡ dane w czasie rzeczywistym, ale takÅ¼e zapewniÄ‡ ich wysokÄ… jakoÅ›Ä‡. W przeciwnym razie analizy mogÄ… prowadziÄ‡ do bÅ‚Ä™dnych wnioskÃ³w lub opÃ³ÅºnieÅ„ w reagowaniu na nieprawidÅ‚owe dane.\nZanieczyszczone dane: W systemach real-time dane czÄ™sto sÄ… niepeÅ‚ne, brudne, bÅ‚Ä™dne lub nieuporzÄ…dkowane. Zmiana charakterystyki danych: Dane mogÄ… zmieniaÄ‡ siÄ™ w czasie, co moÅ¼e utrudniaÄ‡ ich przetwarzanie i analizÄ™. #### RozwiÄ…zanie:\nData cleansing i data validation na wstÄ™pnym etapie procesu. Automatyczne systemy monitorowania jakoÅ›ci danych w celu wykrywania bÅ‚Ä™dÃ³w w czasie rzeczywistym. ZarzÄ…dzanie danymi w strumieniu: NarzÄ™dzia takie jak Apache Kafka pozwalajÄ… na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ integracji systemÃ³w\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym czÄ™sto muszÄ… wspÃ³Å‚pracowaÄ‡ z istniejÄ…cymi systemami IT i ÅºrÃ³dÅ‚ami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemÃ³w, zwÅ‚aszcza w rozproszonej architekturze, moÅ¼e byÄ‡ skomplikowana.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie API do Å‚atwiejszej integracji z zewnÄ™trznymi systemami. Mikroserwisy i konteneryzacja z pomocÄ… narzÄ™dzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, ktÃ³re umoÅ¼liwia Å‚atwÄ… integracjÄ™ rÃ³Å¼nych ÅºrÃ³deÅ‚ danych oraz zapewnia elastycznoÅ›Ä‡ w dostosowywaniu systemÃ³w do rosnÄ…cych potrzeb.\n\n\n\nBezpieczeÅ„stwo i prywatnoÅ›Ä‡\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z ogromnÄ… iloÅ›ciÄ… wraÅ¼liwych informacji, szczegÃ³lnie w branÅ¼ach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, Å¼e dane sÄ… odpowiednio chronione przed nieautoryzowanym dostÄ™pem, jest kluczowe.\nOchrona danych w czasie transmisji: MuszÄ… byÄ‡ szyfrowane zarÃ³wno podczas przesyÅ‚ania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym moÅ¼e byÄ‡ celem atakÃ³w, takich jak DDoS czy SQL injection.\n\n\nRozwiÄ…zanie:\nSzyfrowanie danych zarÃ³wno w spoczynku, jak i podczas przesyÅ‚ania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeÅ„stwa. ZgodnoÅ›Ä‡ z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarzÄ…dzanie bÅ‚Ä™dami i awariami\n\nWyzwanie:\nBÅ‚Ä™dy i awarie w systemach real-time mogÄ… prowadziÄ‡ do powaÅ¼nych konsekwencji, w tym utraty danych, opÃ³ÅºnieÅ„ w analizach czy nawet usuniÄ™cia usÅ‚ug. W systemach rozproszonych trudno jest osiÄ…gnÄ…Ä‡ peÅ‚nÄ… niezawodnoÅ›Ä‡.\n\n\nRozwiÄ…zanie:\nRedundancja: Tworzenie kopii zapasowych systemÃ³w i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), ktÃ³re pozwalajÄ… na szybkie wykrycie i naprawienie problemÃ³w. ZarzÄ…dzanie stanem: DziÄ™ki uÅ¼yciu narzÄ™dzi jak Apache Kafka, moÅ¼na ponownie przetwarzaÄ‡ dane, jeÅ›li wystÄ…piÅ‚ bÅ‚Ä…d w transmisji.\n\n\n\nKoszty zwiÄ…zane z infrastrukturÄ…\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, ktÃ³ra zapewni odpowiedniÄ… moc obliczeniowÄ… i pamiÄ™Ä‡. To moÅ¼e wiÄ…zaÄ‡ siÄ™ z duÅ¼ymi kosztami, szczegÃ³lnie gdy dane muszÄ… byÄ‡ przechowywane i przetwarzane w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\n\n\nRozwiÄ…zanie:\nChmura obliczeniowa: MoÅ¼liwoÅ›Ä‡ elastycznego skalowania zasobÃ³w w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalajÄ… na uruchamianie procesÃ³w bez potrzeby utrzymywania staÅ‚ej infrastruktury.\nChociaÅ¼ analiza danych w czasie rzeczywistym oferuje ogromne korzyÅ›ci, wiÄ…Å¼e siÄ™ takÅ¼e z wieloma wyzwaniami. WÅ‚aÅ›ciwa architektura, narzÄ™dzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogÄ… pomÃ³c w przezwyciÄ™Å¼eniu wielu z tych trudnoÅ›ci. Warto rÃ³wnieÅ¼ pamiÄ™taÄ‡ o koniecznoÅ›ci zapewnienia wysokiej jakoÅ›ci danych, ich bezpieczeÅ„stwa, a takÅ¼e elastycznoÅ›ci i skalowalnoÅ›ci systemÃ³w, ktÃ³re bÄ™dÄ… w stanie sprostaÄ‡ rosnÄ…cym wymaganiom.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html",
    "href": "lectures/wyklad2.html",
    "title": "WykÅ‚ad 2",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, jak dane ewoluowaÅ‚y w rÃ³Å¼nych branÅ¼ach i jakie narzÄ™dzia sÄ… dziÅ› wykorzystywane do ich analizy.\nNa tym wykÅ‚adzie przedstawimy ewolucjÄ™ analizy danych, pokazujÄ…c, jak zmieniaÅ‚y siÄ™ technologie i podejÅ›cia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aÅ¼ po nowoczesne podejÅ›cie do strumieniowego przetwarzania danych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "href": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "title": "WykÅ‚ad 2",
    "section": "Dane tabelaryczne (tabele SQL)",
    "text": "Dane tabelaryczne (tabele SQL)\nPoczÄ…tkowo dane byÅ‚y przechowywane w postaci tabel, gdzie kaÅ¼da tabela zawieraÅ‚a zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).\nModele takie doskonale nadawaÅ‚y siÄ™ do danych ustrukturyzowanych.\n\nğŸ“Œ Cechy:\nâœ… Dane podzielone na kolumny o staÅ‚ej strukturze.\nâœ… MoÅ¼liwoÅ›Ä‡ stosowania operacji CRUD (Create, Read, Update, Delete).\nâœ… ÅšcisÅ‚e reguÅ‚y spÃ³jnoÅ›ci i normalizacji.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Systemy bankowe, e-commerce, ERP, systemy CRM.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-grafowe",
    "href": "lectures/wyklad2.html#dane-grafowe",
    "title": "WykÅ‚ad 2",
    "section": "Dane grafowe",
    "text": "Dane grafowe\nWraz z rozwojem potrzeb biznesowych pojawiÅ‚y siÄ™ dane grafowe, w ktÃ³rych relacje miÄ™dzy obiektami sÄ… reprezentowane jako wierzchoÅ‚ki i krawÄ™dzie.\n\nğŸ“Œ Cechy:\nâœ… Dane opisujÄ…ce relacje i powiÄ…zania.\nâœ… Elastyczna struktura (grafy zamiast tabel).\nâœ… MoÅ¼liwoÅ›Ä‡ analizy poÅ‚Ä…czeÅ„ (np. algorytmy PageRank, centralnoÅ›Ä‡).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Sieci spoÅ‚ecznoÅ›ciowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Graf Karate - NetworkX):\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "href": "lectures/wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "title": "WykÅ‚ad 2",
    "section": "Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)",
    "text": "Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)\nDane te nie sÄ… w peÅ‚ni ustrukturyzowane jak w bazach SQL, ale majÄ… pewien schemat.\n\nğŸ“Œ Cechy:\nâœ… Hierarchiczna struktura (np. klucz-wartoÅ›Ä‡, obiekty zagnieÅ¼dÅ¼one).\nâœ… Brak Å›cisÅ‚ego schematu (moÅ¼liwoÅ›Ä‡ dodawania nowych pÃ³l).\nâœ… PopularnoÅ›Ä‡ w systemach NoSQL i API.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logÃ³w.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "href": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "title": "WykÅ‚ad 2",
    "section": "Dane tekstowe (NLP)",
    "text": "Dane tekstowe (NLP)\nTekst staÅ‚ siÄ™ kluczowym ÅºrÃ³dÅ‚em informacji, szczegÃ³lnie w analizie opinii, chatbotach czy wyszukiwarkach.\n\nğŸ“Œ Cechy:\nâœ… Nieustrukturyzowane dane wymagajÄ…ce przeksztaÅ‚cenia.\nâœ… Stosowanie embeddingÃ³w (np. Word2Vec, BERT, GPT).\nâœ… DuÅ¼e zastosowanie w analizie sentymentu i chatbotach.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Media spoÅ‚ecznoÅ›ciowe, e-maile, chatboty, tÅ‚umaczenie maszynowe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie:\n\n\nCode\nimport ollama\n\n# PrzykÅ‚adowe zdanie\nsentence = \"Sztuczna inteligencja zmienia Å›wiat.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-1.6779385805130005, 3.0364203453063965, -6.6012187004089355, -1.7487436532974243]",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "href": "lectures/wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "title": "WykÅ‚ad 2",
    "section": "Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)",
    "text": "Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)\nNowoczesne systemy analizy danych wykorzystujÄ… rÃ³wnieÅ¼ obrazy i dÅºwiÄ™k.\n\nğŸ“Œ Cechy:\nâœ… WymagajÄ… duÅ¼ej mocy obliczeniowej (sztuczna inteligencja, deep learning).\nâœ… Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dÅºwiÄ™k).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treÅ›ci wideo.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Obraz - OpenCV):\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-strumieniowe",
    "href": "lectures/wyklad2.html#dane-strumieniowe",
    "title": "WykÅ‚ad 2",
    "section": "Dane strumieniowe",
    "text": "Dane strumieniowe\nObecnie najbardziej dynamicznie rozwija siÄ™ analiza danych strumieniowych, gdzie dane sÄ… analizowane na bieÅ¼Ä…co, w miarÄ™ ich napÅ‚ywania.\n\nğŸ“Œ Cechy:\nâœ… Przetwarzanie w czasie rzeczywistym.\nâœ… Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Strumieniowe transakcje bankowe):\n\n\nCode\nimport time\ntransactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]\nfor transaction in transactions:\n    print(f\"Processing transaction: {transaction}\")\n    time.sleep(1)\n\n\nProcessing transaction: {'id': 1, 'amount': 100}\nProcessing transaction: {'id': 2, 'amount': 200}",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "href": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "title": "WykÅ‚ad 2",
    "section": "Dane sensoryczne i IoT",
    "text": "Dane sensoryczne i IoT\nDane z czujnikÃ³w i urzÄ…dzeÅ„ IoT sÄ… kolejnym krokiem w ewolucji.\n\nğŸ“Œ Cechy:\nâœ… CzÄ™sto pochodzÄ… z miliardÃ³w urzÄ…dzeÅ„ (big data).\nâœ… WymagajÄ… analizy brzegowej (edge computing).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Smart home, wearables, samochody autonomiczne, systemy przemysÅ‚owe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Sensor - temperatura):\n\n\nCode\nimport random\ndef get_temperature():\n    return round(random.uniform(20.0, 25.0), 2)\nprint(f\"Current temperature: {get_temperature()}Â°C\")\n\n\nCurrent temperature: 23.4Â°C",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "href": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "title": "WykÅ‚ad 2",
    "section": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data",
    "text": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data\nKiedy mÃ³wimy o skalowalnym przetwarzaniu danych, pierwszym skojarzeniem moÅ¼e byÄ‡ Google.\nAle co tak naprawdÄ™ sprawia, Å¼e moÅ¼emy wyszukiwaÄ‡ informacje w uÅ‚amku sekundy, przetwarzajÄ…c petabajty danych?\nğŸ‘‰ Czy wiesz, Å¼e nazwa â€œGoogleâ€ pochodzi od sÅ‚owa â€œGoogolâ€, czyli liczby rÃ³wnej 10Â¹â°â°?\nTo wiÄ™cej niÅ¼ liczba atomÃ³w w znanym WszechÅ›wiecie! ğŸŒŒ\n\nğŸ”¥ Wyzwanie: Czy uda Ci siÄ™ zapisaÄ‡ liczbÄ™ Googol do koÅ„ca zajÄ™Ä‡?",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "href": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?",
    "text": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?\nTradycyjne bazy danych SQL czy jednowÄ…tkowe algorytmy zawodzÄ…, gdy skala danych przekracza pojedynczy komputer.\nW tym miejscu pojawia siÄ™ MapReduce â€“ rewolucyjny model obliczeniowy stworzony przez Google.\n\nğŸ› ï¸ RozwiÄ…zania Google dla Big Data:\nâœ… Google File System (GFS) â€“ rozproszony system plikÃ³w.\nâœ… Bigtable â€“ system do przechowywania ogromnych iloÅ›ci ustrukturyzowanych danych.\nâœ… MapReduce â€“ algorytm podziaÅ‚u pracy na wiele maszyn.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "href": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "Graficzne przedstawienie MapReduce",
    "text": "Graficzne przedstawienie MapReduce\n\nMapowanie rozdziela zadania (Map)\nKaÅ¼de wejÅ›cie dzielone jest na mniejsze czÄ™Å›ci i przetwarzane rÃ³wnolegle.\nğŸŒ WyobraÅº sobie, Å¼e masz ksiÄ…Å¼kÄ™ telefonicznÄ… i chcesz znaleÅºÄ‡ wszystkie osoby o nazwisku â€œNowakâ€.\nâ¡ï¸ Podziel ksiÄ…Å¼kÄ™ na fragmenty i daj kaÅ¼demu do przeanalizowania jeden fragment.\n\n\nRedukcja zbiera wyniki (Reduce)\nWszystkie czÄ™Å›ciowe wyniki sÄ… Å‚Ä…czone w jednÄ…, koÅ„cowÄ… odpowiedÅº.\nğŸ”„ Wszyscy uczniowie zgÅ‚aszajÄ… swoje wyniki, a jeden student zbiera i podsumowuje odpowiedÅº.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "href": "lectures/wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie",
    "text": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie\nZaÅ‚Ã³Å¼my, Å¼e mamy miliony ksiÄ…Å¼ek i chcemy policzyÄ‡, ile razy wystÄ™puje kaÅ¼de sÅ‚owo.\n\nğŸ–¥ï¸ Kod MapReduce w Pythonie (z uÅ¼yciem multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Funkcja Map (podziaÅ‚ tekstu na sÅ‚owa)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Funkcja Reduce (sumowanie wynikÃ³w)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\nğŸ”¹ Co tu siÄ™ dzieje?\nâœ… KaÅ¼dy fragment tekstu jest przetwarzany niezaleÅ¼nie (map).\nâœ… Wyniki sÄ… zbierane i sumowane (reduce).\nâœ… Efekt: MoÅ¼emy przetwarzaÄ‡ terabajty tekstu rÃ³wnolegle!",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "href": "lectures/wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce",
    "text": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce\nğŸ“Š Stare podejÅ›cie â€“ Jeden komputer wykonuje wszystko sekwencyjnie.\nğŸ“Š Nowe podejÅ›cie (MapReduce) â€“ KaÅ¼da maszyna liczy fragment i wyniki sÄ… agregowane.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "href": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸš€ Wyzwanie dla Ciebie!",
    "text": "ğŸš€ Wyzwanie dla Ciebie!\nğŸ”¹ ZnajdÅº i uruchom swÃ³j wÅ‚asny algorytm MapReduce w dowolnym jÄ™zyku!\nğŸ”¹ Czy potrafisz zaimplementowaÄ‡ wÅ‚asny MapReduce do innego zadania? (np. analiza logÃ³w, zliczanie klikniÄ™Ä‡ na stronie)",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#big-data",
    "href": "lectures/wyklad2.html#big-data",
    "title": "WykÅ‚ad 2",
    "section": "Big Data",
    "text": "Big Data\nSystemy Big data mogÄ… byÄ‡ czÄ™Å›ciÄ… (ÅºrÃ³dÅ‚em) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie sÄ… systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsÅ‚uÅ¼y do rÃ³Å¼norodnych celÃ³w opartych na danych (analityka, data science â€¦)\nponiÅ¼ej 100% accuracy\n\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.â€™â€™ â€” Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, â€¦ four V\n\nVolume (ObjÄ™toÅ›Ä‡) - rozmiar danych produkowanych na caÅ‚ym Å›wiecie przyrasta w tempie wykÅ‚adniczym.\nVelocity (SzybkoÅ›Ä‡) - tempo produkowania danych, szybkoÅ›ci ich przesyÅ‚ania i przetwarzania.\nVariety (ZrÃ³Å¼nicowanie) - tradycyjne dane kojarzÄ… siÄ™ nam z postaciÄ… alfanumerycznÄ… zÅ‚oÅ¼onÄ… z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dÅºwiÄ™ki, pliki wideo, strumienie danych z IoT\nVeracity (WiarygodnoÅ›Ä‡) - Czy dane sÄ… kompletne i poprawne? Czy obiektywnie odzwierciedlajÄ… rzeczywistoÅ›Ä‡? Czy sÄ… podstawÄ… do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, itâ€™s all about cost and benefits.\n\n\nCelem obliczeÅ„ nie sÄ… liczby, lecz ich zrozumienie R.W. Hamming 1962.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "href": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "title": "WykÅ‚ad 2",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane sÄ… praktycznie od zawsze. W ciÄ…gu ostatnich dziesiÄ™cioleci iloÅ›Ä‡ przetwarzanych danych systematycznie roÅ›nie co wpÅ‚ywa na proces przygotowania i przetwarzania danych.\n\nTrochÄ™ historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : PoczÄ…tek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPÃ³Åºniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiÄ™kszoÅ›Ä‡ danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostÄ™p do danych sprowadza siÄ™ najczÄ™Å›ciej do realizacji zapytaÅ„ poprzez aplikacjÄ™.\nSposÃ³b wykorzystania i realizacji procesu dostÄ™pu do bazy danych nazywamy modelem przetwarzania. NajczÄ™Å›ciej uÅ¼ywane sÄ… dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Åšwietnie sprawdza siÄ™ w przypadku obsÅ‚ugi bieÅ¼Ä…cej np. obsÅ‚uga klienta, rejestr zamÃ³wieÅ„, obsÅ‚uga sprzedaÅ¼y itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiÄ…zaÅ„ m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostÄ™pu do danych,\nzarzÄ…dzania wspÃ³Å‚bieÅ¼noÅ›ciÄ…,\nprzetwarzania zdarzeÅ„ -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemÃ³w (np. dla wielu sklepÃ³w),\nraportowanie i podsumowania danych,\noptymalizacja zÅ‚oÅ¼onych zapytaÅ„,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziÅ‚y do sformuÅ‚owania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesÃ³w analizy i dostarczanie narzÄ™dzi umoÅ¼liwiajÄ…cych analizÄ™ wielowymiarowÄ… (czas, miejsce, produkt).\nProces zrzucania danych z rÃ³Å¼nych systemÃ³w do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatÃ³w (podsumowaÅ„) dotyczÄ…cych wymiarÃ³w hurtowni. Proces ten jest caÅ‚kowicie sterowany przez uÅ¼ytkownika.\nPrzykÅ‚ad\nZaÅ‚Ã³Å¼my, Å¼e mamy dostÄ™p do hurtowni danych gdzie przechowywane sÄ… informacje dotyczÄ…ce sprzedaÅ¼y produktÃ³w w supermarkecie. Jak przeanalizowaÄ‡ zapytania:\n\nJaka jest Å‚Ä…czna sprzedaÅ¼ produktÃ³w w kolejnych kwartaÅ‚ach, miesiÄ…cach, tygodniach ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na rodzaje produktÃ³w ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na oddziaÅ‚y supermarketu ?\n\nOdpowiedzi na te pytania pozwalajÄ… okreÅ›liÄ‡ wÄ…skie gardÅ‚a sprzedaÅ¼y produktÃ³w przynoszÄ…cych deficyt, zaplanowaÄ‡ zapasy w magazynach czy porÃ³wnaÄ‡ sprzedaÅ¼ rÃ³Å¼nych grup w rÃ³Å¼nych oddziaÅ‚ach supermarketu.\nW ramach Hurtowni Danych najczÄ™Å›ciej wykonuje siÄ™ dwa rodzaje zapytaÅ„(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczajÄ…ce biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagajÄ…ce krytyczne decyzje biznesowe.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli VI bud G\n\n18-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 1\n25-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 2\n04-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 3 online\n11-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 4\n18-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 5\n\nWykÅ‚ad 5 koÅ„czy siÄ™ TESTEM: 20 pytaÅ„ - 30 minut. Test przeprowadzany jest za poÅ›rednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡).\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJÄ™zyk prowadzenia: polski\nPoziom przedmiotu: Å›rednio-zaawansowany\nProwadzÄ…cy: Sebastian ZajÄ…c, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspÃ³Å‚czesny biznes opiera siÄ™ na podejmowaniu decyzji opartych na danych. Coraz wiÄ™ksza iloÅ›Ä‡ informacji, rosnÄ…ce wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiajÄ…, Å¼e analiza danych w czasie rzeczywistym staje siÄ™ kluczowym elementem nowoczesnych procesÃ³w biznesowych.\nNa zajÄ™ciach studenci zapoznajÄ… siÄ™ z metodami i technologiami umoÅ¼liwiajÄ…cymi przetwarzanie danych w czasie rzeczywistym. SzczegÃ³lnÄ… uwagÄ™ poÅ›wiÄ™cimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz gÅ‚Ä™bokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretowaÄ‡ zjawiska biznesowe, ale takÅ¼e podejmowaÄ‡ szybkie i trafne decyzje.\nW ramach kursu omÃ³wimy zarÃ³wno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dÅºwiÄ™k, strumieniowanie wideo). Studenci poznajÄ… architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a takÅ¼e wyzwania zwiÄ…zane z modelowaniem danych w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\nKurs obejmuje czÄ™Å›Ä‡ teoretycznÄ… oraz praktyczne laboratoria, podczas ktÃ³rych studenci bÄ™dÄ… pracowaÄ‡ z rzeczywistymi danymi w Å›rodowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. DziÄ™ki temu studenci nie tylko zdobÄ™dÄ… wiedzÄ™ na temat metod analitycznych, ale takÅ¼e nauczÄ… siÄ™ korzystaÄ‡ z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plikoÌw pÅ‚askich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (WykÅ‚ad)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametroÌw modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykÅ‚adzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego sÌrodowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego sÌrodowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 1.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 2.\nPrzygotowanie sÌrodowiska Microsoft Azure. Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzeÌ¨dzia IT do szybkiej analizy logoÌw.\nNarzeÌ¨dzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-ksztaÅ‚cenia",
    "href": "sylabus.html#efekty-ksztaÅ‚cenia",
    "title": "Sylabus",
    "section": "Efekty ksztaÅ‚cenia",
    "text": "Efekty ksztaÅ‚cenia\n\nWiedza:\n\n\nZna historieÌ¨ i filozofieÌ¨ modeli przetwarzania danych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna mozÌ‡liwosÌci i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nUmie wybracÌ struktureÌ¨ IT dla danego problemu biznesowego\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo kroÌtkim czasie\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejÄ™tnoÅ›ci:\n\n\nRozroÌzÌ‡nia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiaÌ¨zania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ, przetwarzacÌ oraz zachowywacÌ dane generowane w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikajaÌ¨ce z czasu przetwarzania przez urzaÌ¨dzenia oraz systemy informatyczne\n\nPowiaÌ¨zania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosowacÌ i skonstruowacÌ system do przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormuÅ‚uje problem analityczny wraz z jego informatycznym rozwiaÌ¨zaniem\n\nPowiaÌ¨zania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejeÌ¨tnosÌcÌ samodzielnego uzupeÅ‚niania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiaÌ¨zania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1ï¸âƒ£ ZajÄ…c S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzÄ™dzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2ï¸âƒ£ FrÄ…tczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3ï¸âƒ£ Bellemare A., MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™, Oâ€™Reilly 2021.\n4ï¸âƒ£ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, Oâ€™Reilly 2022.\n5ï¸âƒ£ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. RozwiÄ…zania typowych problemÃ³w dotyczÄ…cych przygotowania danych, konstruowania modeli i MLOps, Oâ€™Reilly 2021.\n6ï¸âƒ£ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, Oâ€™Reilly 2022.\n7ï¸âƒ£ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, Oâ€™Reilly 2018.\n8ï¸âƒ£ SebastiÃ¡n RamÃ­rez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostÄ™pna online).\n9ï¸âƒ£ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\nğŸ”Ÿ Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, Oâ€™Reilly 2019."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentÃ³w z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacjÄ… w Dockerze. Nauczysz siÄ™: - Tworzenia prostego REST API, - ObsÅ‚ugi zapytaÅ„ HTTP i obsÅ‚ugi bÅ‚Ä™dÃ³w w API, - Testowania API z wykorzystaniem pytest, - Przenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#cel",
    "href": "labs/lab1.html#cel",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentÃ³w z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacjÄ… w Dockerze. Nauczysz siÄ™: - Tworzenia prostego REST API, - ObsÅ‚ugi zapytaÅ„ HTTP i obsÅ‚ugi bÅ‚Ä™dÃ³w w API, - Testowania API z wykorzystaniem pytest, - Przenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "href": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "1. Tworzenie aplikacji REST API",
    "text": "1. Tworzenie aplikacji REST API\nNaszym zadaniem jest wystawienie aplikacji w Pythonie, ktÃ³ra na Å¼Ä…danie klienta udzieli odpowiedzi na podstawie predykcji wygenerowanej przez model.\nAplikacjÄ™ napiszemy w Pythonie z wykorzystaniem Flask 3.0.3.\n\nKod minimalnej aplikacji Flask\nNaszÄ… aplikacjÄ™ chcemy uruchomiÄ‡ lokalnie, a nastÄ™pnie w prosty sposÃ³b przenieÅ›Ä‡ i uruchomiÄ‡ na dowolnym komputerze. Dlatego naturalnym rozwiÄ…zaniem jest zapisanie kodu w pliku z rozszerzeniem .py.\nAby automatycznie zapisaÄ‡ kod aplikacji do pliku app.py, wykorzystamy magicznÄ… komendÄ™ %%file plik.py.\n\n%%file app.py\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\n\nif __name__ == '__main__':\n    app.run()\n\nWriting app.py\n\n\n\nUwaga! W dokumentacji Flask w kodzie podstawowej aplikacji nie wystÄ™pujÄ… dwie ostatnie linie odpowiedzialne za uruchomienie serwera.\n\nif __name__ == '__main__':\n    app.run()\nWyjaÅ›nijmy co zawiera przykÅ‚adowy kod.\n\nfrom flask import Flask ZaÅ‚adowanie biblioteki\napp = Flask(__name__) utworzenie interfejsu serwera API\nkod podstrony z wykorzystaniem dekoratora\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\nDekoratory w Pythonie pozwalajÄ… modyfikowaÄ‡ zachowanie funkcji bez zmiany jej kodu. Flask wykorzystuje dekoratory do tworzenia tras (@app.route), ale moÅ¼na je takÅ¼e stosowaÄ‡ w analizie danych â€“ np. do logowania czasu wykonania funkcji lub obsÅ‚ugi bÅ‚Ä™dÃ³w.\n\nPrzykÅ‚ad: Normalizacja wartoÅ›ci w danych\nZaÅ‚Ã³Å¼my, Å¼e mamy funkcjÄ™, ktÃ³ra pobiera dane z pliku CSV i zwraca listÄ™ wartoÅ›ci. Dodamy dekorator, ktÃ³ry automatycznie przeskaluje dane do zakresu 0-1, co czÄ™sto jest wymagane przed analizÄ… statystycznÄ… lub trenowaniem modeli ML.\n\nimport numpy as np\n\n# Dekorator do normalizacji danych\ndef normalize_data(func):\n    def wrapper(*args, **kwargs):\n        data = func(*args, **kwargs)  # Pobranie oryginalnych danych\n        min_val, max_val = min(data), max(data)\n        normalized = [(x - min_val) / (max_val - min_val) for x in data]\n        print(\"Dane po normalizacji:\", normalized)\n        return normalized\n    return wrapper\n\n\n@normalize_data\ndef get_data():\n    return [10, 15, 20, 30, 50]\n\nget_data()\n\nDane po normalizacji: [0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n[0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n\nÄ†wiczenie: â€Napisz dekorator, ktÃ³ry zaokrÄ…gla wartoÅ›ci do 2 miejsc po przecinku.â€\n\n\n\n\nObsÅ‚uga bÅ‚Ä™dÃ³w w API\nDodajmy obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w, np. kiedy klient poda niepoprawne dane:\n@app.errorhandler(404)\ndef not_found(error):\n    return jsonify({\"error\": \"Not Found\"}), 404\n\n@app.errorhandler(400)\ndef bad_request(error):\n    return jsonify({\"error\": \"Bad Request\"}), 400",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "href": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "Uruchomienie serwera lokalnie",
    "text": "Uruchomienie serwera lokalnie\nUruchomienie serwera moze odbyÄ‡ siÄ™ na przynajmniej na dwa sposoby.\n\nUruchomienie serwera przez terminal\nOtwÃ³rz termianal w lokalizacji gdzie znajduje siÄ™ plik aplikacji\npython app.py\nlub (jeÅ›li nie ma fragmentu app.run())\nflask run\nPowinna pojawiÄ‡ siÄ™ informacja podobna do ponizszej:\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\nW celu weryfikacji dziaÅ‚ania moÅ¼esz otworzyÄ‡ nowe okno terminalu wpisujÄ…c:\ncurl localhost:5000\n\\{\"message\":\"Hello, World!\"\\}\n\n\nUruchomienie serwera w notatniku\nBezpoÅ›renie uruchomienia kodu w notatniku spowoduje uruchomienie serwera i zatrzymanie jakiejkolwiek mozliwoÅ›ci realizacji kodu. Aby tego uniknÄ…Ä‡ mozesz wykorzystaÄ‡ bibliotekÄ™ subprocess.\n\nimport subprocess\np = subprocess.Popen([\"python\", \"app.py\"])\n\nJeÅ›li potrzebujemy zamknÄ…Ä‡ subprocess wykonaj:\n\np.kill()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#testowanie-api",
    "href": "labs/lab1.html#testowanie-api",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "2. Testowanie API",
    "text": "2. Testowanie API\nDo testowania API wykorzystamy pytest oraz bibliotekÄ™ requests. ### Instalacja pytest:\n\n!pip install pytest requests -q\n\n\n%%file test_app.py\nimport pytest\nimport requests\n\ndef test_home():\n    response = requests.get(\"http://127.0.0.1:5000/\")\n    assert response.status_code == 200\n    assert response.json()[\"message\"] == \"Hello, World!\"\n\nWriting test_app.py\n\n\n\n!pytest test_app.py\n\n============================= test session starts ==============================\nplatform linux -- Python 3.11.6, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/jovyan/notebooks\nplugins: anyio-4.0.0\ncollected 1 item                                                               \n\ntest_app.py .                                                            [100%]\n\n============================== 1 passed in 0.03s ===============================\n\n\n\n# wersja bez testu\nimport requests\nresponse = requests.get(\"http://127.0.0.1:5000/\")\nprint(response.json())\n\n{'message': 'Hello, World!'}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#Å›rodowisko-python",
    "href": "labs/lab1.html#Å›rodowisko-python",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "Åšrodowisko Python",
    "text": "Åšrodowisko Python\nAby uruchomiÄ‡ kod aplikacji app.py, potrzebujemy interpretera jÄ™zyka Python zainstalowanego na naszym komputerze. Jednak samo posiadanie interpretera nie jest wystarczajÄ…ce â€“ aby aplikacja dziaÅ‚aÅ‚a poprawnie, naleÅ¼y utworzyÄ‡ Å›rodowisko (najlepiej wirtualne), w ktÃ³rym bÄ™dÄ… dostÄ™pne wszystkie wymagane biblioteki, takie jak Flask.\n\nuwaga: wszystkie polecenia terminala dotyczyÄ‡ bÄ™dÄ… wersji linux/mac os\n\nW pierwszej kolejnoÅ›ci sprawdÅº czy dostÄ™pne sÄ… polecenia pozwalajÄ…ce realizowaÄ‡ kod pythonowy.\nwhich python\nwhich python3\nwhich pip \nwhich pip3\nWszystkie te polecenia powinny wskazyaÄ‡ na folder z domyÅ›lnym Å›rodowiskiem Pythona.\nWygeneruj i uruchom Å›rodowisko wirtualne lokalnie wpisujÄ…c w terminalu:\npython3 -m venv .venv\nsource .venv/bin/activate\n\nDobra praktyka: Å›rodowisko python to nic innego jak katalog. W naszej wersji to katalog ukryty o nazwie .venv. JeÅ›li skopiujesz ten katalog gdzie indziej przestanie peÅ‚niÄ‡ on swojÄ… funkcjÄ™ Å›rodowiska python. Dlatego jego odtworzenie nie polega na jego kopiowaniu. JeÅ›li TwÃ³j projekt jest powiÄ…zany ze Å›rodowiskiem kontroli wersji GIT zadbaj aby katalog Å›rodowiska nie byÅ‚ dodawany do repozytorium. Mozesz wykonaÄ‡ to dziaÅ‚anie dodajÄ…c odpowiedni wpis do pliki .gitignore\n\nPosiadajÄ…c utworzone nowe Å›rodowisko sprawdÅº jakie biblioteki siÄ™ w nim znajdujÄ….\npip list \n\nPackage    Version\n---------- -------\npip        23.2.1\nsetuptools 65.5.0\nMozemy ponownie sprawdziÄ‡ polecenia python i pip:\nwhich python\nwhich pip \nDomyÅ›lnie powinny pojawiÄ‡ siÄ™ biblioteki pip oraz setuptools.\nDoinstaluj bibliotekÄ™ flask.\npip install flask==3.0.3\npip list \nPackage      Version\n------------ -------\nblinker      1.7.0\nclick        8.1.7\nFlask        3.0.3\nitsdangerous 2.1.2\nJinja2       3.1.3\nMarkupSafe   2.1.5\npip          23.2.1\nsetuptools   65.5.0\nWerkzeug     3.0.2\nJak widaÄ‡ instalacja biblioteki flask wymusiÅ‚a doinstalowanie rÃ³wniez innych pakietÃ³w.\nJedynÄ… mozliwoÅ›ciÄ… przeniesienia Å›rodowiska python jest jego ponowna instalacja na nowej maszynie i instalacja wszystkich pakietÃ³w. Aby jednak nie instalowaÄ‡ kazdego pakietu osobno mozemy wykorzystaÄ‡ plik konfiguracyjny requirements.txt zawierajÄ…cy listÄ™ pakietÃ³w.\n\nPamiÄ™taj - kazdy pakiet powinien zawieraÄ‡ nr wersji pakietu. W innym przypadku moze okazaÄ‡ siÄ™, ze nowe werjse pakietÃ³w spowodujÄ… brak obsÅ‚ugi twojego kodu.\n\nAby utworzyÄ‡ plik konfiguracyjny uzyj polecenia w terminalu:\npip freeze &gt;&gt; requirements.txt\nTak wygenerowany plik mozesz uzywaÄ‡ na dowolnej maszynie do instalacji i odtworzenia potrzebnego Å›rodowiska wykonawczego python.\n\nDygresja. W momencie przygotowywania materiaÅ‚Ã³w Flask byÅ‚ w wersji 3.0.1 - dziÅ› juz realizowany jest w wersji 3.0.3. Zmiany nastÄ™pujÄ… szybciej niz siÄ™ wydaje. Instalacja pakietÃ³w z pliku odbywa siÄ™ z wykorzystaniem polecenia:\n\npip install -r requierements.txt\nMamy teraz dwa pliki: app.py, i requirements.txt. PrzenoszÄ…c je do dowolnego projektu na serwerach github jesteÅ›my w stanie uruchomiÄ‡ naszÄ… aplikacjÄ™ wszÄ™dzie tam gdzie dostÄ™pny bÄ™dzie interpreter python na ktÃ³rym mozemy utworzyÄ‡ nowe wirtualne Å›rodowisko i zainstalowaÄ‡ biblioteki z pliku requirements.txt.\nDo peÅ‚nej automatyzacji przydaÅ‚aby siÄ™ jeszcze mozliwoÅ›Ä‡ uruchomienia Å›rodowiska python na dowolnej maszynie.\nW tym celu utwÃ³rz plik Dockerfile:\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 5000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\"]\n\nPowyzszy plik pozwala w docker desktop uruchomiÄ‡ obraz wykorzystujÄ…cy podstawowy system operacyjny (tutaj linux) wraz z podstawowym Å›rodowiskiem python3.11.\nPonadto plik ten kopiuje potrzebne pliki (app.py, requirements.txt) na obraz dockera.\nPolecenie RUN pozwala uruchomiÄ‡ dowolne polecenie bash wewnÄ…trz obrazu dockera.\nPolecenie CMD pozwala uruchomiÄ‡ polecenie uruchamiajÄ…ce serwer w trybie tak by nie zamknÄ…Ä‡ tego polecenia.\nOstatniÄ… informacjÄ… jest ustalenie portu na 8000.\nutworzenie kontenera na podstawie pliku Dockerfile\ndocker build -t modelML .\nuruchomienie kontenera\ndocker run -p 8000:8000 modelML",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "",
    "text": "W tym Ä‡wiczeniu nauczysz siÄ™, jak stworzyÄ‡ proste API w Flasku, uruchomiÄ‡ je, wysyÅ‚aÄ‡ do niego zapytania oraz wykorzystaÄ‡ model decyzyjny w oparciu o podstawowÄ… reguÅ‚Ä™ logicznÄ….",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-podstawowego-api",
    "href": "labs/lab2.html#tworzenie-podstawowego-api",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "1ï¸âƒ£ Tworzenie podstawowego API",
    "text": "1ï¸âƒ£ Tworzenie podstawowego API\nNajpierw utworzymy podstawowÄ… aplikacjÄ™ Flask.\n\nZapisanie kodu API do pliku\nW Jupyter Notebooku uÅ¼yj magicznej komendy %%file, aby zapisaÄ‡ kod podstawowej aplikacji flask do pliku app.py: Kod znajdziesz na cw1 Jako tekst do wyÅ›wietlenie strony gÅ‚Ã³wnej uÅ¼yj Witaj w moim API!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nWriting app.py\n\n\nTeraz uruchom API w terminalu, wpisujÄ…c:\npython app.py\nFlask uruchomi serwer lokalnie pod adresem http://127.0.0.1:5000/.\n\n\nSprawdzenie dziaÅ‚ania API\nW Jupyter Notebooku wykonaj zapytanie GET do strony gÅ‚Ã³wnej. Na podstawie pola status_code napisz wyraÅ¼enie warunkowe ktÃ³re dla status_code 200 wyÅ›wietli zawartoÅ›Ä‡ odpowiedzi (z pola content).\n\nimport requests\nresponse = pass # TWOJ KOD\n\nJeÅ›li wszystko dziaÅ‚a poprawnie, zobaczysz komunikat Witaj w moim API!.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#dodanie-nowej-podstrony",
    "href": "labs/lab2.html#dodanie-nowej-podstrony",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "2ï¸âƒ£ Dodanie nowej podstrony",
    "text": "2ï¸âƒ£ Dodanie nowej podstrony\nDodajmy nowÄ… podstronÄ™ mojastrona, ktÃ³ra zwrÃ³ci komunikat To jest moja strona!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nPonownie uruchom API i wykonaj zapytanie do strony \"http://127.0.0.1:5000/mojastrona\":\n\nresponse = pass # TWOJ KOD\n\nPowinieneÅ› zobaczyÄ‡: To jest moja strona!",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "href": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "3ï¸âƒ£ Automatyczne uruchamianie serwera z Jupyter Notebook",
    "text": "3ï¸âƒ£ Automatyczne uruchamianie serwera z Jupyter Notebook\nZamknij wczeÅ›niej uruchomiony serwer (Ctrl+C w terminalu) i uruchom go ponownie bezpoÅ›rednio z Jupyter Notebook, korzystajÄ…c z subprocess.Popen:\n\nimport subprocess\n# TWOJ KOD \nserver = pass\n\nPo testach zamknij serwer wykorzystujÄ…c metodÄ™ kill:\n\n# TWOJ KOD",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#obsÅ‚uga-parametrÃ³w-w-adresie-url",
    "href": "labs/lab2.html#obsÅ‚uga-parametrÃ³w-w-adresie-url",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "4ï¸âƒ£ ObsÅ‚uga parametrÃ³w w adresie URL",
    "text": "4ï¸âƒ£ ObsÅ‚uga parametrÃ³w w adresie URL\nDodajemy nowÄ… podstronÄ™ /hello, ktÃ³ra bÄ™dzie przyjmowaÄ‡ parametr name.\nEdytuj app.py, dodajÄ…c odpowiedni kod\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nUruchom serwer i sprawdÅº dziaÅ‚anie API:\nres1 = requests.get(\"http://127.0.0.1:5000/hello\")\nprint(res1.content)  # Powinno zwrÃ³ciÄ‡ \"Hello!\"\n\nres2 = requests.get(\"http://127.0.0.1:5000/hello?name=Sebastian\")\nprint(res2.content)  # Powinno zwrÃ³ciÄ‡ \"Hello Sebastian!\"",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "href": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "5ï¸âƒ£ Tworzenie API z prostym modelem ML",
    "text": "5ï¸âƒ£ Tworzenie API z prostym modelem ML\nStworzymy nowÄ… podstronÄ™ /api/v1.0/predict, ktÃ³ra przyjmuje dwie liczby i zwraca wynik reguÅ‚y decyzyjnej: - JeÅ›li suma dwÃ³ch liczb jest wiÄ™ksza niÅ¼ 5.8, zwraca 1. - W przeciwnym razie zwraca 0.\nSprawdÅº dziaÅ‚anie API:\nres = requests.get(\"http://127.0.0.1:5000/api/v1.0/predict?num1=3&num2=4\")\nprint(res.json())  # Powinno zwrÃ³ciÄ‡ {\"prediction\": 1, \"features\": {\"num1\": 3.0, \"num2\": 4.0}}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#podsumowanie",
    "href": "labs/lab2.html#podsumowanie",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "Podsumowanie",
    "text": "Podsumowanie\nPo wykonaniu tego Ä‡wiczenia studenci bÄ™dÄ… umieli:\n\nâœ… TworzyÄ‡ podstawowe API w Flasku.\n\nâœ… DodawaÄ‡ podstrony i obsÅ‚ugiwaÄ‡ parametry URL.\n\nâœ… WysyÅ‚aÄ‡ zapytania GET i analizowaÄ‡ odpowiedzi.\n\nâœ… Automatycznie uruchamiaÄ‡ serwer z Jupyter Notebook.\n\nâœ… ImplementowaÄ‡ prosty model decyzyjny w API.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#ksiÄ…Å¼ki",
    "href": "ksiazki.html#ksiÄ…Å¼ki",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatnikÃ³w\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI donâ€™t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "href": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z serwisu GitHub",
    "text": "Zacznij korzystaÄ‡ z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystaÄ‡ z serwisu github\nPracujÄ…c nad projektem np. praca magisterska, (samodzielnie lub w zespole) czÄ™sto potrzebujesz sprawdziÄ‡ jakie zmiany, kiedy i przez kogo zostaÅ‚y wprowadzone do projektu. W zadaniu tym Å›wietnie sprawdza siÄ™ system kontroli wersji czyli GIT.\nGit moÅ¼esz pobraÄ‡ i zainstalowaÄ‡ jak zwykÅ‚y program na dowolnym komputerze. Jednak najczÄ™Å›ciej (maÅ‚e projekty) korzysta siÄ™ z serwisÃ³w z jakimÅ› systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dziÄ™ki ktÃ³remu moÅ¼esz korzystaÄ‡ z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki moÅ¼esz przechowywaÄ‡ w publicznych (dostÄ™p majÄ… wszyscy) repozytoriach.\nSkupimy siÄ™ wyÅ‚Ä…cznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyÅ¼szym poziomie znajdujÄ… siÄ™ konta indywidualne np http://github.com/sebkaz, bÄ…dÅº zakÅ‚adane przez organizacje. UÅ¼ytkownicy indywidualni mogÄ… tworzyÄ‡ repozytoria publiczne (public ) bÄ…dÅº prywatne (private).\nJeden plik nie powinien przekraczaÄ‡ 100 MB.\nRepo (skrÃ³t do repozytorium) tworzymy za pomocÄ… Create a new repository. KaÅ¼de repo powinno mieÄ‡ swojÄ… indywidualnÄ… nazwÄ™.\n\n\nBranche\nGÅ‚Ã³wna (tworzona domyÅ›lnie) gaÅ‚Ä…Åº rapozytorium ma nazwÄ™ master.\n\n\nNajwaÅ¼niejsze polecnia do zapamiÄ™tania\n\nÅ›ciÄ…ganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba moÅ¼esz pobraÄ‡ repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejÅ›cie do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawiÄ‡ siÄ™ ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPoÅ‚Ä…cz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsÅ‚uga w 3 krokach\n\n# sprawdÅº zmiany jakie zostaÅ‚y dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzÄ…cy stan wraz z informacjÄ… co zrobiÅ‚eÅ›\ngit commit -m \" opis \"\n# 3. potem juÅ¼ zostaje tylko\ngit push origin master\nWarto obejrzeÄ‡ Youtube course.\nCiekawe i proste wprowadzenie mozna znaleÅºÄ‡ tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "href": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z Dockera",
    "text": "Zacznij korzystaÄ‡ z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swÃ³j system przejdÅº do strony.\nJeÅ¼li wszystko zainstalowaÅ‚o siÄ™ prawidÅ‚owo wykonaj nastÄ™pujÄ…ce polecenia:\n\nSprawdÅº zainstalowanÄ… wersjÄ™\n\ndocker --version\n\nÅšciÄ…gnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzeglÄ…d Å›ciÄ…gnietych obrazÃ³w:\n\ndocker image ls\n\ndocker images\n\nPrzeglÄ…d uruchomionych kontenerÃ³w:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsuniÄ™cie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam rÃ³wnieÅ¼ krÃ³tkie intro"
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli III bud G\n\n22-02-2025 (sobota) 08:00-09:30 - WykÅ‚ad 1\n08-03-2025 (sobota) 08:00-09:30 - WykÅ‚ad 2\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡) 20 pytaÅ„.\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "lectures/wyklad3.html",
    "href": "lectures/wyklad3.html",
    "title": "WykÅ‚ad 3",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, podstawowych sposobÃ³w przetwarzania i analizowania danych strumieniowych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#definicje",
    "href": "lectures/wyklad3.html#definicje",
    "title": "WykÅ‚ad 3",
    "section": "Definicje",
    "text": "Definicje\n\nZapoznaj siÄ™ z tematem danych strumieniowych\n\nDefinicja 1 â€“ Zdarzenie to wszystko, co moÅ¼na zaobserwowaÄ‡ w danym momencie czasu. Jest generowane jako bezpoÅ›redni skutek dziaÅ‚ania.\nDefinicja 2 â€“ W kontekÅ›cie danych zdarzenie to niezmienialny rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.\nDefinicja 3 â€“ CiÄ…gÅ‚y strumieÅ„ zdarzeÅ„ to nieskoÅ„czony zbiÃ³r pojedynczych zdarzeÅ„ uporzÄ…dkowanych w czasie, np. logi z urzÄ…dzeÅ„.\nDefinicja 4 â€“ StrumieÅ„ danych to dane tworzone przyrostowo w czasie, generowane ze ÅºrÃ³deÅ‚ statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).\nPrzedsiÄ™biorstwo to organizacja, ktÃ³ra generuje i odpowiada na ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„.\n\n\n\n\nAnalityka strumieniowa\nAnalityka strumieniowa (ang. stream analytics) nazywana jest rÃ³wnieÅ¼ przetwarzaniem strumieniowym zdarzeÅ„ (ang. event stream processing) â€“ czyli przetwarzaniem duÅ¼ych iloÅ›ci danych juÅ¼ na etapie ich generowania.\nNiezaleÅ¼nie od zastosowanej technologii, wszystkie dane powstajÄ… jako ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„ â€“ obejmuje to m.in.:\n\ndziaÅ‚ania uÅ¼ytkownikÃ³w na stronach internetowych,\n\nlogi systemowe,\n\npomiary z sensorÃ³w.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 3",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma Å¼adnego zwiÄ…zku z momentem wystÄ…pienia analizowanych zdarzeÅ„.\nNatomiast w przetwarzaniu strumieniowym wyrÃ³Å¼niamy dwie koncepcje czasu:\n\nCzas zdarzenia (event time) â€“ moment, w ktÃ³rym zdarzenie faktycznie miaÅ‚o miejsce.\nCzas przetwarzania (processing time) â€“ moment, w ktÃ³rym system przetwarza zdarzenie.\n\nIdealne przetwarzanie danych\nW idealnej sytuacji przetwarzanie nastÄ™puje natychmiast po wystÄ…pieniu zdarzenia:\n\nRzeczywiste przetwarzanie danych\nW praktyce przetwarzanie danych zawsze odbywa siÄ™ z pewnym opÃ³Åºnieniem, co jest widoczne jako punkty poniÅ¼ej linii idealnego przetwarzania (poniÅ¼ej przekÄ…tnej na wykresie).\n\nW aplikacjach przetwarzania strumieniowego istotna jest rÃ³Å¼nica miÄ™dzy czasem powstania zdarzenia a czasem jego przetwarzania. Do najczÄ™stszych przyczyn opÃ³ÅºnieÅ„ naleÅ¼Ä…:\n\nprzesyÅ‚anie danych przez sieÄ‡,\nbrak komunikacji miÄ™dzy urzÄ…dzeniem a sieciÄ….\n\nPrzykÅ‚adem jest Å›ledzenie poÅ‚oÅ¼enia samochodu przez aplikacjÄ™ GPS â€“ przejazd przez tunel moÅ¼e spowodowaÄ‡ chwilowÄ… utratÄ™ danych.\nObsÅ‚uga opÃ³ÅºnieÅ„ w przetwarzaniu strumieniowym\nOpÃ³Åºnienia w przetwarzaniu zdarzeÅ„ moÅ¼na obsÅ‚uÅ¼yÄ‡ na dwa sposoby:\n\nMonitorowanie liczby pominiÄ™tych zdarzeÅ„ i wyzwalanie alarmu w przypadku zbyt duÅ¼ej liczby odrzuceÅ„.\nZastosowanie korekty za pomocÄ… watermarkingu, czyli dodatkowego mechanizmu uwzglÄ™dniajÄ…cego opÃ³Åºnione zdarzenia.\n\nProces przetwarzania zdarzeÅ„ w czasie rzeczywistym moÅ¼na przedstawiÄ‡ jako funkcjÄ™ schodkowÄ…:\n\nNie wszystkie zdarzenia wnoszÄ… wkÅ‚ad do analizy â€“ niektÃ³re mogÄ… zostaÄ‡ odrzucone ze wzglÄ™du na zbyt duÅ¼e opÃ³Åºnienie.\nWykorzystanie watermarkingu pozwala na uwzglÄ™dnienie dodatkowego czasu na pojawienie siÄ™ opÃ³Åºnionych zdarzeÅ„. Proces ten obejmuje wszystkie zdarzenia powyÅ¼ej przerywanej linii. Mimo to nadal mogÄ… zdarzyÄ‡ siÄ™ przypadki, w ktÃ³rych niektÃ³re punkty zostanÄ… pominiÄ™te.\n\nPrzedstawione na wykresach sytuacje jawnie wskazujÄ… dlaczego pojÄ™cie czasu jest istotnym czynnikiem i wymaga Å›cisÅ‚ego okreÅ›lenia juÅ¼ na poziomie definiowania potrzeb biznesowych. Przypisywanie znacznikÃ³w czasu do danych (zdarzeÅ„) to trudne zadanie.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "href": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "title": "WykÅ‚ad 3",
    "section": "Okna czasowe w analizie strumieniowej",
    "text": "Okna czasowe w analizie strumieniowej\nW przetwarzaniu strumieniowym okna czasowe pozwalajÄ… na grupowanie danych w ograniczone czasowo segmenty, co umoÅ¼liwia analizÄ™ zdarzeÅ„ w okreÅ›lonych przedziaÅ‚ach czasowych. W zaleÅ¼noÅ›ci od zastosowania stosuje siÄ™ rÃ³Å¼ne typy okien, dostosowane do charakterystyki danych i wymagaÅ„ analitycznych.\n\n\nOkno rozÅ‚Ä…czne (Tumbling Window)\nJest to okno o staÅ‚ej dÅ‚ugoÅ›ci, ktÃ³re nie nakÅ‚ada siÄ™ na siebie â€“ kaÅ¼de zdarzenie naleÅ¼y tylko do jednego okna.\nâœ… Charakterystyka:\n\nStaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n\nBrak nakÅ‚adania siÄ™ na siebie\n\nIdealne do podziaÅ‚u danych na rÃ³wne segmenty czasowe\n\nğŸ“Œ PrzykÅ‚ad: Analiza liczby zamÃ³wieÅ„ w sklepie internetowym co 5 minut.\n\n\n\n\nOkno przesuwne (Sliding Window)\nObejmuje wszystkie zdarzenia nastÄ™pujÄ…ce w okreÅ›lonym przedziale czasu, gdzie okno przesuwa siÄ™ w sposÃ³b ciÄ…gÅ‚y.\nâœ… Charakterystyka:\n\nKaÅ¼de zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien\n\nOkno przesuwa siÄ™ o zadany interwaÅ‚\n\nPrzydatne do wykrywania trendÃ³w i anomalii\n\nğŸ“Œ PrzykÅ‚ad: Åšledzenie Å›redniej temperatury w ciÄ…gu ostatnich 10 minut, aktualizowane co 2 minuty.\n\n\n\n\nOkno skokowe (Hopping Window)\nJest podobne do okna rozÅ‚Ä…cznego, ale pozwala na nakÅ‚adanie siÄ™ okien na siebie, dziÄ™ki czemu jedno zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien. Jest stosowane do wygÅ‚adzania danych.\nâœ… Charakterystyka:\n\nStaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n\nMoÅ¼liwoÅ›Ä‡ nakÅ‚adania siÄ™ na siebie\n\nPrzydatne do redukcji szumÃ³w w danych\n\nğŸ“Œ PrzykÅ‚ad: Analiza liczby odwiedzajÄ…cych stronÄ™ co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywaÄ‡ trendy.\n\n\n\n\nOkno sesyjne (Session Window)\nOkno sesyjne grupuje zdarzenia na podstawie okresÃ³w aktywnoÅ›ci i zamyka siÄ™ po okreÅ›lonym czasie braku aktywnoÅ›ci.\nâœ… Charakterystyka:\n\nDynamiczna dÅ‚ugoÅ›Ä‡ okna\n\nDefiniowane przez aktywnoÅ›Ä‡ uÅ¼ytkownika\n\nStosowane w analizie sesji uÅ¼ytkownikÃ³w\n\nğŸ“Œ PrzykÅ‚ad: Analiza sesji uÅ¼ytkownikÃ³w na stronie internetowej â€“ sesja trwa tak dÅ‚ugo, jak dÅ‚ugo uÅ¼ytkownik wykonuje akcje, ale koÅ„czy siÄ™ po 15 minutach braku aktywnoÅ›ci.\n\n\n\nPodsumowanie\nRÃ³Å¼ne rodzaje okien czasowych sÄ… stosowane w zaleÅ¼noÅ›ci od specyfiki danych i celÃ³w analizy. WybÃ³r odpowiedniego okna wpÅ‚ywa na dokÅ‚adnoÅ›Ä‡ wynikÃ³w i efektywnoÅ›Ä‡ systemu analitycznego.\n\n\n\n\n\n\n\n\nTyp okna\nCharakterystyka\nZastosowanie\n\n\n\n\nRozÅ‚Ä…czne (Tumbling)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, brak nakÅ‚adania\nRaporty okresowe\n\n\nPrzesuwne (Sliding)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, nakÅ‚adajÄ…ce siÄ™ okna\nTrendy, wykrywanie anomalii\n\n\nSkokowe (Hopping)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, czÄ™Å›ciowe nakÅ‚adanie\nWygÅ‚adzanie danych\n\n\nSesyjne (Session)\nDynamiczna dÅ‚ugoÅ›Ä‡, zaleÅ¼na od aktywnoÅ›ci\nAnaliza sesji uÅ¼ytkownikÃ³w\n\n\n\nKaÅ¼dy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. WybÃ³r wÅ‚aÅ›ciwej metody zaleÅ¼y od potrzeb biznesowych i charakterystyki analizowanych danych.\nW analizie danych strumieniowych interpretacja czasu jest zÅ‚oÅ¼onym zagadnieniem, poniewaÅ¼:\n\nRÃ³Å¼ne systemy majÄ… rÃ³Å¼ne zegary, co moÅ¼e prowadziÄ‡ do niespÃ³jnoÅ›ci,\nDane mogÄ… docieraÄ‡ z opÃ³Åºnieniem, co wymaga technik watermarkingu i okien czasowych,\nRÃ³Å¼ne podejÅ›cia do analizy czasu zdarzenia i czasu przetwarzania wpÅ‚ywajÄ… na dokÅ‚adnoÅ›Ä‡ wynikÃ³w.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html",
    "href": "lectures/wyklad4.html",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#historia-podejÅ›cia-do-architektury",
    "href": "lectures/wyklad4.html#historia-podejÅ›cia-do-architektury",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "href": "lectures/wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "title": "WykÅ‚ad 4",
    "section": "WpÅ‚yw technologii na systemy informatyczne",
    "text": "WpÅ‚yw technologii na systemy informatyczne\nKomunikacja sieciowa, relacyjne bazy danych, rozwiÄ…zania chmurowe oraz Big Data znaczÄ…co zmieniÅ‚y sposÃ³b budowania systemÃ³w informatycznych i wykonywania w nich pracy.\nPodobnie, narzÄ™dzia do przekazu informacji â€“ takie jak gazeta, radio, telewizja, internet, komunikatory i media spoÅ‚ecznoÅ›ciowe â€“ wpÅ‚ynÄ™Å‚y na interakcje miÄ™dzyludzkie oraz struktury spoÅ‚eczne.\nKaÅ¼de nowe medium technologiczne ksztaÅ‚tuje sposÃ³b, w jaki ludzie korzystajÄ… z informatyki i postrzegajÄ… jej rolÄ™ w codziennym Å¼yciu.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "href": "lectures/wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "title": "WykÅ‚ad 4",
    "section": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT",
    "text": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT\nJednym z najpopularniejszych podejÅ›Ä‡ do budowy systemÃ³w informatycznych jest koncepcja mikrousÅ‚ug (microservices).\nJest ona szeroko stosowana zarÃ³wno w tworzeniu oprogramowania, jak i w prowadzeniu firm opartych na analizie danych (Data-Driven).\n\nGÅ‚Ã³wne zalety mikroserwisÃ³w:\n\nWydajnoÅ›Ä‡ â€“ kaÅ¼da usÅ‚uga realizuje jedno, dobrze okreÅ›lone zadanie (â€œrÃ³b jednÄ… rzecz, ale dobrzeâ€).\nElastycznoÅ›Ä‡ â€“ umoÅ¼liwiajÄ… Å‚atwe modyfikacje i skalowanie systemu.\nPrzejrzystoÅ›Ä‡ architektury â€“ system skÅ‚ada siÄ™ z niewielkich, niezaleÅ¼nych moduÅ‚Ã³w.\n\nMikroserwisy moÅ¼na porÃ³wnaÄ‡ do czystych funkcji w programowaniu funkcyjnym â€“ kaÅ¼da usÅ‚uga dziaÅ‚a niezaleÅ¼nie i posiada jasno okreÅ›lone wejÅ›cia oraz wyjÅ›cia.\nAby umoÅ¼liwiÄ‡ komunikacjÄ™ miÄ™dzy mikroserwisami, czÄ™sto wykorzystuje siÄ™ Application Programming Interfaces (API), ktÃ³re pozwalajÄ… na wymianÄ™ danych i integracjÄ™ rÃ³Å¼nych usÅ‚ug.\n\n\nPrzykÅ‚ad API w mikroserwisach â€“ Python & FastAPI\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy mikroserwis REST API w Pythonie z uÅ¼yciem FastAPI, ktÃ³ry zwraca informacje o uÅ¼ytkownikach:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# PrzykÅ‚adowe dane uÅ¼ytkownikÃ³w\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane uÅ¼ytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nJak to dziaÅ‚a?\n\nUruchamiamy serwer FastAPI.\nMoÅ¼emy uzyskaÄ‡ dane uÅ¼ytkownika, wysyÅ‚ajÄ…c Å¼Ä…danie GET do http://127.0.0.1:8000/users/1.\nAPI zwrÃ³ci dane w formacie JSON, np.:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousÅ‚ug jest wykorzystanie interfejsÃ³w API (Application Programming Interface).\nAPI umoÅ¼liwia komunikacjÄ™ i integracjÄ™ miÄ™dzy rÃ³Å¼nymi mikroserwisami, podobnie jak strona internetowa komunikuje siÄ™ z przeglÄ…darkÄ….\nGdy odwiedzasz stronÄ™ internetowÄ…, serwer wysyÅ‚a kod, ktÃ³ry Twoja przeglÄ…darka interpretuje i wyÅ›wietla jako stronÄ™ internetowÄ….\nPodobnie dziaÅ‚a API â€“ wysyÅ‚a odpowiedzi na zapytania klienta.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "href": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "title": "WykÅ‚ad 4",
    "section": "Przypadek biznesowy: Model ML jako usÅ‚uga",
    "text": "Przypadek biznesowy: Model ML jako usÅ‚uga\nZaÅ‚Ã³Å¼my, Å¼e pracujesz w firmie zajmujÄ…cej siÄ™ sprzedaÅ¼Ä… nieruchomoÅ›ci w Bostonie.\nChcesz zwiÄ™kszyÄ‡ sprzedaÅ¼ i poprawiÄ‡ jakoÅ›Ä‡ obsÅ‚ugi klientÃ³w poprzez nowÄ… aplikacjÄ™ mobilnÄ…, z ktÃ³rej moÅ¼e korzystaÄ‡ nawet 1 000 000 uÅ¼ytkownikÃ³w jednoczeÅ›nie.\nJednym z rozwiÄ…zaÅ„ jest udostÄ™pnienie prognozy wartoÅ›ci domu w czasie rzeczywistym.\nGdy uÅ¼ytkownik prosi o wycenÄ™ nieruchomoÅ›ci, aplikacja wysyÅ‚a zapytanie do serwera, ktÃ³ry przetwarza je za pomocÄ… modelu Machine Learning (ML) i zwraca oszacowanÄ… wartoÅ›Ä‡.\n\nCzym jest serwowanie modelu ML?\nTrening dobrego modelu ML to dopiero pierwszy krok caÅ‚ego procesu.\nAby model byÅ‚ uÅ¼yteczny, musisz go udostÄ™pniÄ‡ uÅ¼ytkownikom koÅ„cowym, np. w formie API.\n\n\nJak to zrobiÄ‡?\n\nPotrzebujesz:\n\n\nwytrenowanego modelu ML,\ninterpreter modelu (np. TensorFlow, Scikit-Learn, PyTorch),\ndanych wejÅ›ciowych dla modelu.\n\n\nKluczowe metryki jakoÅ›ci serwowania modelu:\nCzas odpowiedzi (latency),\nKoszt uruchomienia modelu (infrastruktura serwerowa),\nLiczba zapytaÅ„ na sekundÄ™ (QPS â€“ Queries Per Second).\n\n\nUdostÄ™pnianie danych miÄ™dzy systemami zawsze byÅ‚o kluczowym wyzwaniem w tworzeniu oprogramowania.\nW obszarze tradycyjnego DevOps koncentrujemy siÄ™ na infrastrukturze, a w MLOps â€“ na wdraÅ¼aniu i utrzymaniu modeli uczenia maszynowego.\n\nZbudowanie systemu przygotowanego do Å›rodowiska produkcyjnego jest bardziej skomplikowane niÅ¼ wytrenowanie samego modelu:\n\nczyszczenie i zaÅ‚adowanie odpowiednich i zwalidowanych danych\nObliczenie zmiennych i ich serwowanie na wÅ‚aÅ›ciwym Å›rodowisku\nSerwowanie modelu w sposÃ³b najbardziej efektywny ze wzglÄ™du na koszty\nWersjonowanie, Å›ledzenie i udostÄ™pnianie danych i modeli oraz innych artefaktÃ³w\nMonitorowanie infrastruktury i modelu\nWdroÅ¼enie modelu na skalowalnej infrastrukturze\nAutomatyzacja procesu wdraÅ¼ania i treningu",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#jak-dziaÅ‚a-api",
    "href": "lectures/wyklad4.html#jak-dziaÅ‚a-api",
    "title": "WykÅ‚ad 4",
    "section": "Jak dziaÅ‚a API?",
    "text": "Jak dziaÅ‚a API?\nKiedy wywoÅ‚asz interfejs API, serwer otrzymuje Twoje Å¼Ä…danie, przetwarza je i generuje odpowiedÅº.\nJeÅ›li wszystko dziaÅ‚a poprawnie, otrzymasz wynik w formacie JSON lub XML.\nJeÅ›li wystÄ…pi bÅ‚Ä…d, API zwrÃ³ci kod bÅ‚Ä™du HTTP (np. 400 â€“ nieprawidÅ‚owe Å¼Ä…danie, 500 â€“ bÅ‚Ä…d serwera).\n\nKluczowe zasady REST API:\n\nKlient-Serwer â†’ Klient (np. aplikacja mobilna) wysyÅ‚a Å¼Ä…danie HTTP do API hostowanego na serwerze, ktÃ³ry zwraca odpowiedÅº.\nDziaÅ‚a to identycznie jak przeglÄ…darka internetowa, ktÃ³ra wysyÅ‚a Å¼Ä…danie do serwera WWW i otrzymuje stronÄ™ HTML.\nBezstanowoÅ›Ä‡ â†’ KaÅ¼de Å¼Ä…danie klienta musi zawieraÄ‡ wszystkie niezbÄ™dne informacje do przetworzenia odpowiedzi,\nAPI nie powinno przechowywaÄ‡ informacji o wczeÅ›niejszych Å¼Ä…daniach uÅ¼ytkownika.\n\n\n\nPrzykÅ‚ad: API serwujÄ…ce model ML\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy serwis API, ktÃ³ry udostÄ™pnia model ML do prognozowania ceny nieruchomoÅ›ci,\nz wykorzystaniem FastAPI oraz Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wczeÅ›niej wytrenowany model ML (np. regresjÄ™ liniowÄ…)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Prognoza ceny nieruchomoÅ›ci na podstawie cech:\n    - area (powierzchnia w mÂ²),\n    - bedrooms (liczba sypialni),\n    - age (wiek budynku w latach).\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n\n\ndomenÄ™,\nport,\ndodatkowe Å›cieÅ¼ki,\nzapytanie\n\n\nMetody HTTP:\n\n\nGET,\nPOST\n\n\nNagÅ‚Ã³wki HTTP zawierajÄ…:\n\n\ninformacje o autoryzacji,\ncookies metadata\n\nCaÅ‚a informacja zawarta jest w Content-Type: application/json, text â€¦ Accept: application/json, Authorization: Basic abase64string, Tokens 4. CiaÅ‚o zapytania\nNajczÄ™Å›ciej wybieranym formatem dla wymiany informacji miÄ™dzy serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt sÅ‚ownika - â€œkluczâ€: â€œwartoÅ›Ä‡â€.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedÅº - Response\n\nTreÅ›Ä‡ odpowiedzi przekazywana jest razem z nagÅ‚Ã³wkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: â€œContent-Typeâ€ =&gt; â€application/json; charset=utf-8â€, â€Serverâ€ =&gt; â€Genie/Julia/1.8.5â€\nTreÅ›Ä‡ (ciaÅ‚o) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidÅ‚owe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#publikujsubskrybuj",
    "href": "lectures/wyklad4.html#publikujsubskrybuj",
    "title": "WykÅ‚ad 4",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyÅ‚ania wiadomoÅ›ci â€Publikuj/Subskrybujâ€ ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujÄ…cy siÄ™ tym, Å¼e nadawca (publikujÄ…cy) fragmentu danych (wiadomoÅ›ci) nie kieruje go wprost do odbiorcy. pub/sub to systemy, ktÃ³re czÄ™sto posiadajÄ… brokera czyli centralny punkt, w ktÃ³rym znajdujÄ… siÄ™ wiadomoÅ›ci.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#apache-kafka",
    "href": "lectures/wyklad4.html#apache-kafka",
    "title": "WykÅ‚ad 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicjÄ™:\n\nRozproszona platforma streamingowa\nCo to jest â€platforma rozproszonego przesyÅ‚ania strumieniowegoâ€?\nNajpierw chcÄ™ przypomnieÄ‡, czym jest â€strumieÅ„â€. Strumienie to po prostu nieograniczone dane, dane, ktÃ³re nigdy siÄ™ nie koÅ„czÄ…. CiÄ…gle ich przybywa i moÅ¼esz przetwarzaÄ‡ je w czasie rzeczywistym.\nA â€rozproszoneâ€? Rozproszony oznacza, Å¼e â€‹â€‹Kafka dziaÅ‚a w klastrze, a kaÅ¼dy wÄ™zeÅ‚ w grupie nazywa siÄ™ Brokerem. Ci brokerzy to po prostu serwery wykonujÄ…ce kopiÄ™ Apache Kafka.\nTak wiÄ™c Kafka to zestaw wspÃ³Å‚pracujÄ…cych ze sobÄ… maszyn, aby mÃ³c obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ nieograniczone dane w czasie rzeczywistym.\nBrokerzy sprawiajÄ…, Å¼e jest niezawodny, skalowalny i odporny na bÅ‚Ä™dy. Ale dlaczego panuje bÅ‚Ä™dne przekonanie, Å¼e Kafka to kolejny â€kolejkowy system przesyÅ‚ania wiadomoÅ›ciâ€?\nAby odpowiedzieÄ‡ na tÄ™ odpowiedÅº, musimy najpierw wyjaÅ›niÄ‡, jak dziaÅ‚a kolejkowe przesyÅ‚anie wiadomoÅ›ci.\n\n\nKolejkowy system przesyÅ‚ania wiadomoÅ›ci\nPrzesyÅ‚anie wiadomoÅ›ci, to po prostu czynnoÅ›Ä‡ wysyÅ‚ania wiadomoÅ›ci z jednego miejsca do drugiego. Ma trzech gÅ‚Ã³wnych â€œaktorÃ³wâ€:\n\nProducent: KtÃ³ry tworzy i wysyÅ‚a komunikaty do jednej lub wiÄ™cej kolejek;\nKolejka: struktura danych bufora, ktÃ³ra odbiera (od producentÃ³w) i dostarcza komunikaty (do konsumentÃ³w) w sposÃ³b FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jednÄ… lub wiÄ™cej kolejek i otrzymuje ich wiadomoÅ›ci po opublikowaniu.\n\nI to jest to; tak dziaÅ‚a przesyÅ‚anie wiadomoÅ›ci. Jak widaÄ‡, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Apache Kafka\nWiÄ™cej informacji na temat Kafki znajdziesz w tym linku.\nTeraz, gdy rozumiemy podstawy przesyÅ‚ania wiadomoÅ›ci, zagÅ‚Ä™bmy siÄ™ w Å›wiat Apache Kafka.\nW Kafce mamy dwa kluczowe pojÄ™cia: ProducentÃ³w (Producers) i KonsumentÃ³w (Consumers),\nktÃ³rzy dziaÅ‚ajÄ… w podobny sposÃ³b jak w klasycznych systemach kolejkowych, produkujÄ…c i konsumujÄ…c wiadomoÅ›ci.\n\n\nJak widaÄ‡, Kafka przypomina klasyczny system przesyÅ‚ania wiadomoÅ›ci, jednak w odrÃ³Å¼nieniu od tradycyjnych kolejek,\nzamiast pojÄ™cia kolejki (queue) mamy Tematy (Topics).\n\n\nTematy (Topics) i Partycje (Partitions)\nTemat (Topic) to podstawowy kanaÅ‚ przesyÅ‚ania danych w Kafce.\nMoÅ¼na go porÃ³wnaÄ‡ do folderu, w ktÃ³rym przechowywane sÄ… wiadomoÅ›ci.\nKaÅ¼dy temat posiada jednÄ… lub wiÄ™cej partycji (Partitions) â€“ podziaÅ‚ ten wpÅ‚ywa na skalowalnoÅ›Ä‡ i rÃ³wnowaÅ¼enie obciÄ…Å¼enia.\nPodczas tworzenia tematu okreÅ›lamy liczbÄ™ partycji.\n\nKluczowe cechy tematÃ³w i partycji:\n\nTemat to logiczna jednostka, do ktÃ³rej producenci wysyÅ‚ajÄ… wiadomoÅ›ci, a konsumenci je odczytujÄ….\nPartycja to fizyczny podziaÅ‚ tematu. MoÅ¼na jÄ… porÃ³wnaÄ‡ do plikÃ³w w folderze.\nOffset â€“ kaÅ¼da wiadomoÅ›Ä‡ w partycji otrzymuje unikalny identyfikator (offset),\nktÃ³ry pozwala konsumentom Å›ledziÄ‡, ktÃ³re wiadomoÅ›ci zostaÅ‚y juÅ¼ przetworzone.\nKafka przechowuje wiadomoÅ›ci na dysku, dziÄ™ki czemu moÅ¼e je ponownie odczytaÄ‡ (w przeciwieÅ„stwie do klasycznych kolejek, gdzie wiadomoÅ›Ä‡ jest usuwana po przetworzeniu).\nKonsumenci odczytujÄ… wiadomoÅ›ci sekwencyjnie, od najstarszej do najnowszej.\nW przypadku awarii konsument moÅ¼e wznowiÄ‡ przetwarzanie od ostatniego zapisanego offsetu.\n\n\n\n\n\n\nBrokerzy (Brokers) i Klaster Kafka\nKafka dziaÅ‚a w sposÃ³b rozproszony â€“ oznacza to, Å¼e moÅ¼e skÅ‚adaÄ‡ siÄ™ z wielu brokerÃ³w (Brokers),\nktÃ³re wspÃ³Å‚pracujÄ… jako jeden klaster.\n\n\n\nKluczowe informacje o brokerach\n\nBroker to pojedynczy serwer w klastrze Kafki, odpowiedzialny za przechowywanie partycji tematÃ³w.\nKaÅ¼dy broker w klastrze ma unikalny identyfikator.\nAby zwiÄ™kszyÄ‡ dostÄ™pnoÅ›Ä‡ i niezawodnoÅ›Ä‡, Kafka wykorzystuje replikacjÄ™ danych.\nWspÃ³Å‚czynnik replikacji okreÅ›la, ile kopii danej partycji ma byÄ‡ przechowywane na rÃ³Å¼nych brokerach.\nJeÅ›li temat ma trzy partycje i wspÃ³Å‚czynnik replikacji rÃ³wny trzy,\noznacza to, Å¼e kaÅ¼da partycja zostanie powielona na trzech rÃ³Å¼nych brokerach.\n\nLiczba partycji powinna byÄ‡ dobrana w taki sposÃ³b, aby kaÅ¼dy broker miaÅ‚ co najmniej jednÄ… partycjÄ™ do obsÅ‚ugi.\n\n\n\nProducenci (Producers)\nW Kafka producenci to aplikacje lub usÅ‚ugi, ktÃ³re tworzÄ… i wysyÅ‚ajÄ… wiadomoÅ›ci do tematÃ³w.\nDziaÅ‚a to podobnie do systemÃ³w kolejkowych, z tÄ… rÃ³Å¼nicÄ…, Å¼e Kafka zapisuje wiadomoÅ›ci w partycjach.\n\nJak Kafka przypisuje wiadomoÅ›ci do partycji?\n\nWiadomoÅ›ci sÄ… rozsyÅ‚ane okrÄ™Å¼nie (round-robin) do dostÄ™pnych partycji.\nMoÅ¼emy okreÅ›liÄ‡ klucz wiadomoÅ›ci, a Kafka wyliczy jego hash,\naby okreÅ›liÄ‡, do ktÃ³rej partycji trafi wiadomoÅ›Ä‡.\nKlucz wiadomoÅ›ci determinuje przypisanie do partycji â€“ jeÅ›li temat zostaÅ‚ juÅ¼ utworzony,\nliczba partycji nie moÅ¼e byÄ‡ zmieniona bez zakÅ‚Ã³cenia tego mechanizmu.\n\nPrzykÅ‚ad przypisania wiadomoÅ›ci do partycji:\n\nWiadomoÅ›Ä‡ 01 trafia do partycji 0 tematu Topic_1.\nWiadomoÅ›Ä‡ 02 trafia do partycji 1 tego samego tematu.\nKolejna wiadomoÅ›Ä‡ moÅ¼e ponownie trafiÄ‡ do partycji 0, jeÅ›li stosujemy przypisanie round-robin.\n\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# WysyÅ‚amy wiadomoÅ›Ä‡ do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"Nowe mieszkanie na sprzedaÅ¼\"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"WiadomoÅ›Ä‡ wysÅ‚ana do tematu '{topic}'\")\n\n\n\nKonsumenci (Consumers)\nKonsumenci w Kafce odczytujÄ… i przetwarzajÄ… wiadomoÅ›ci z tematÃ³w. KaÅ¼dy konsument moÅ¼e naleÅ¼eÄ‡ do grupy konsumentÃ³w (Consumer Group), co pozwala na rÃ³wnolegÅ‚e przetwarzanie wiadomoÅ›ci. - JeÅ›li wielu konsumentÃ³w naleÅ¼y do tej samej grupy, Kafka rÃ³wnowaÅ¼y obciÄ…Å¼enie miÄ™dzy nimi. - JeÅ›li jeden konsument przestanie dziaÅ‚aÄ‡, Kafka automatycznie przypisze jego partycje do innego aktywnego konsumenta.\nPrzykÅ‚ad konsumenta w Pythonie:\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, ktÃ³ry nasÅ‚uchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Oczekiwanie na wiadomoÅ›ci...\")\n\nfor message in consumer:\n    print(f\"Otrzymano wiadomoÅ›Ä‡: {message.value.decode()}\")\nInnym waÅ¼nym pojÄ™ciem Kafki sÄ… â€Grupy konsumentÃ³wâ€. Jest to bardzo waÅ¼ne, gdy musimy skalowaÄ‡ odczytywanie wiadomoÅ›ci. Staje siÄ™ to bardzo kosztowne, gdy pojedynczy konsument musi czytaÄ‡ z wielu partycji, wiÄ™c musimy zrÃ³wnowaÅ¼yÄ‡ obciÄ…Å¼enie miÄ™dzy naszymi konsumentami, wtedy wchodzÄ… grupy konsumentÃ³w.\nDane z jednego tematu bÄ™dÄ… rÃ³wnowaÅ¼one obciÄ…Å¼eniem miÄ™dzy konsumentami, dziÄ™ki czemu moÅ¼emy zagwarantowaÄ‡, Å¼e nasi konsumenci bÄ™dÄ… w stanie obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ dane. IdeaÅ‚em jest posiadanie takiej samej liczby konsumentÃ³w w grupie, jakÄ… mamy jako partycje w temacie, w ten sposÃ³b kaÅ¼dy konsument czyta tylko z jednego. Podczas dodawania konsumentÃ³w do grupy naleÅ¼y uwaÅ¼aÄ‡, jeÅ›li liczba konsumentÃ³w jest wiÄ™ksza niÅ¼ liczba partycji, niektÃ³rzy konsumenci nie bÄ™dÄ… czytaÄ‡ z Å¼adnego tematu i pozostanÄ… bezczynni.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html",
    "href": "kafka_codes/kafka2.html",
    "title": "Producent w Å›rodowisku python",
    "section": "",
    "text": "TÄ™ wersjÄ™ Ä‡wiczeÅ„ moÅ¼na przejÅ›Ä‡, posiadajÄ…c nowy obraz Dockerowy i uruchomiony Docker Desktop na wÅ‚asnym komputerze. Jak rÃ³wnieÅ¼ na Å›rodowisku SGH.\n\n1ï¸âƒ£ Uruchomienie Å›rodowiska\nPrzejdÅº do przeglÄ…darki i otwÃ³rz stronÄ™ ze Å›rodowiskiem (w przypadku Dockera otwÃ³rz localhost:8888).\nUruchom Jupyter Lab, a nastÄ™pnie otwÃ³rz nowy terminal (za pomocÄ… ikony terminala).\n\n\n2ï¸âƒ£ Sprawdzenie katalogÃ³w i dostÄ™pnoÅ›ci Kafki\nPrzejdÅº do katalogu gÅ‚Ã³wnego i wypisz listÄ™ wszystkich elementÃ³w. SprawdÅº, czy na liÅ›cie znajduje siÄ™ katalog kafka.\ncd ~\nls -la\n\n\n3ï¸âƒ£ Sprawdzenie listy topicÃ³w\nUruchom polecenie sprawdzajÄ…ce listÄ™ topicÃ³w serwera Kafki:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\n\n4ï¸âƒ£ Dodanie nowego topicu\nDodaj topic o nazwie streaming:\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nSprawdÅº ponownie listÄ™ topicÃ³w, upewniajÄ…c siÄ™, Å¼e streaming zostaÅ‚ dodany:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep streaming\n\n\n5ï¸âƒ£ Uruchomienie producenta w Pythonie\nW nowym terminalu utwÃ³rz plik stream.py i wklej poniÅ¼szy kod:\n\n%%file stream.py\nimport json\nimport random\nimport sys\nfrom datetime import datetime\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nSERVER = \"broker:9092\"\nTOPIC = \"streaming\"\n\nif __name__ == \"__main__\":\n    \n    \n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\")\n    )\n    \n    try:\n        while True:\n            \n            message = {\n                \"time\": str(datetime.now()),\n                \"id\": random.choice(['a','b','c','d']),\n                \"value\": random.randint(0,100)\n            }\n            producer.send(TOPIC, value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()\n\nWriting stream.py\n\n\n\n\n6ï¸âƒ£ Uruchomienie konsumenta w konsoli\nAby sprawdziÄ‡, czy wysyÅ‚anie wiadomoÅ›ci dziaÅ‚a, otwÃ³rz kolejne okno terminala i uruchom konsumenta:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\nTeraz wszystkie wiadomoÅ›ci wysÅ‚ane przez producenta powinny pojawiÄ‡ siÄ™ w konsoli konsumenta.\n\n\n7ï¸âƒ£ ZakoÅ„czenie pracy\nPamiÄ™taj, aby uruchamiaÄ‡ komendy z odpowiedniego katalogu. Po zakoÅ„czeniu Ä‡wiczeÅ„ uÅ¼yj Ctrl+C, aby zatrzymaÄ‡ zarÃ³wno producenta, jak i konsumenta.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  }
]