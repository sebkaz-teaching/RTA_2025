[
  {
    "objectID": "kafka_codes/kafka1.html",
    "href": "kafka_codes/kafka1.html",
    "title": "Wprowadzenie",
    "section": "",
    "text": "Apache Kafka to system przetwarzania strumieniowego (event streaming), który działa jako rozproszony broker wiadomości. Pozwala na przesyłanie i przetwarzanie danych w czasie rzeczywistym.\nDomyślnym adresem naszego brokera jest broker:9092.\nW Apache Kafka dane są przechowywane w strukturach zwanych topicami, które pełnią funkcję kolejek komunikacyjnych.\nZarządzanie Kafką odbywa się za pomocą skryptów. W naszym przypadku będą to skrypty .sh.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#sprawdź-listę-topiców",
    "href": "kafka_codes/kafka1.html#sprawdź-listę-topiców",
    "title": "Wprowadzenie",
    "section": "1️⃣ Sprawdź listę topiców",
    "text": "1️⃣ Sprawdź listę topiców\nPamiętaj, aby przejść do katalogu domowego:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#utwórz-nowy-topic-o-nazwie-mytopic",
    "href": "kafka_codes/kafka1.html#utwórz-nowy-topic-o-nazwie-mytopic",
    "title": "Wprowadzenie",
    "section": "2️⃣ Utwórz nowy topic o nazwie mytopic",
    "text": "2️⃣ Utwórz nowy topic o nazwie mytopic\nkafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#utwórz-producenta-w-terminalu",
    "href": "kafka_codes/kafka1.html#utwórz-producenta-w-terminalu",
    "title": "Wprowadzenie",
    "section": "3️⃣ Utwórz producenta w terminalu",
    "text": "3️⃣ Utwórz producenta w terminalu\nTen skrypt pozwoli Ci wprowadzać eventy ręcznie przez terminal. Opcje --property są dodatkowe i służą do analizy w tym przykładzie.\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#consumer-w-sparku",
    "href": "kafka_codes/kafka1.html#consumer-w-sparku",
    "title": "Wprowadzenie",
    "section": "4️⃣ Consumer w Sparku",
    "text": "4️⃣ Consumer w Sparku\nOtwórz nowy terminal w miejscu, gdzie znajduje się plik test_key_value.py, i uruchom program Consumera w Sparku.\n\n%%file test_key_value.py\nfrom pyspark.sql import SparkSession\n\nKAFKA_BROKER = 'broker:9092'\nKAFKA_TOPIC = 'mytopic'\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\ndf = (spark.readStream.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n      .option(\"subscribe\", KAFKA_TOPIC)\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n     )\n\n# Konwersja danych binarnych na stringi\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n .writeStream \\\n .format(\"console\") \\\n .outputMode(\"append\") \\\n .start() \\\n .awaitTermination()\n\nPamiętaj, że Apache Spark nie posiada domyślnego konektora do Kafki, dlatego uruchom proces za pomocą spark-submit i pobierz odpowiedni pakiet w Scali:\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#przetestuj-przesyłanie-danych",
    "href": "kafka_codes/kafka1.html#przetestuj-przesyłanie-danych",
    "title": "Wprowadzenie",
    "section": "5️⃣ Przetestuj przesyłanie danych",
    "text": "5️⃣ Przetestuj przesyłanie danych\nW terminalu z uruchomionym producentem wpisz tekst w postaci:\njan:45\nalicja:20\nSprawdź, co pojawia się w oknie aplikacji Consumera.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#zakończenie-procesu",
    "href": "kafka_codes/kafka1.html#zakończenie-procesu",
    "title": "Wprowadzenie",
    "section": "6️⃣ Zakończenie procesu",
    "text": "6️⃣ Zakończenie procesu\nPo zakończeniu pokazu użyj Ctrl+C, aby zamknąć zarówno okno producenta, jak i aplikację Spark.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html",
    "href": "kafka_codes/kafka2.html",
    "title": "Producent w środowisku python",
    "section": "",
    "text": "Tę wersję ćwiczeń można przejść, posiadając nowy obraz Dockerowy i uruchomiony Docker Desktop na własnym komputerze. Jak również na środowisku SGH.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-środowiska",
    "href": "kafka_codes/kafka2.html#uruchomienie-środowiska",
    "title": "Producent w środowisku python",
    "section": "1️⃣ Uruchomienie środowiska",
    "text": "1️⃣ Uruchomienie środowiska\nPrzejdź do przeglądarki i otwórz stronę ze środowiskiem (w przypadku Dockera otwórz localhost:8888).\nUruchom Jupyter Lab, a następnie otwórz nowy terminal (za pomocą ikony terminala).",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#sprawdzenie-katalogów-i-dostępności-kafki",
    "href": "kafka_codes/kafka2.html#sprawdzenie-katalogów-i-dostępności-kafki",
    "title": "Producent w środowisku python",
    "section": "2️⃣ Sprawdzenie katalogów i dostępności Kafki",
    "text": "2️⃣ Sprawdzenie katalogów i dostępności Kafki\nPrzejdź do katalogu głównego i wypisz listę wszystkich elementów. Sprawdź, czy na liście znajduje się katalog kafka.\ncd ~\nls -la",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#sprawdzenie-listy-topiców",
    "href": "kafka_codes/kafka2.html#sprawdzenie-listy-topiców",
    "title": "Producent w środowisku python",
    "section": "3️⃣ Sprawdzenie listy topiców",
    "text": "3️⃣ Sprawdzenie listy topiców\nUruchom polecenie sprawdzające listę topiców serwera Kafki:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#dodanie-nowego-topicu",
    "href": "kafka_codes/kafka2.html#dodanie-nowego-topicu",
    "title": "Producent w środowisku python",
    "section": "4️⃣ Dodanie nowego topicu",
    "text": "4️⃣ Dodanie nowego topicu\nDodaj topic o nazwie streaming:\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nSprawdź ponownie listę topiców, upewniając się, że streaming został dodany:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep streaming",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-producenta-w-pythonie",
    "href": "kafka_codes/kafka2.html#uruchomienie-producenta-w-pythonie",
    "title": "Producent w środowisku python",
    "section": "5️⃣ Uruchomienie producenta w Pythonie",
    "text": "5️⃣ Uruchomienie producenta w Pythonie\nW nowym terminalu utwórz plik stream.py i wklej poniższy kod:\n\n%%file stream.py\nimport json\nimport random\nimport sys\nfrom datetime import datetime\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nSERVER = \"broker:9092\"\nTOPIC = \"streaming\"\n\nif __name__ == \"__main__\":\n    \n    \n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\")\n    )\n    \n    try:\n        while True:\n            \n            message = {\n                \"time\": str(datetime.now()),\n                \"id\": random.choice(['a','b','c','d']),\n                \"value\": random.randint(0,100)\n            }\n            producer.send(TOPIC, value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()\n\nWriting stream.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-konsumenta-w-konsoli",
    "href": "kafka_codes/kafka2.html#uruchomienie-konsumenta-w-konsoli",
    "title": "Producent w środowisku python",
    "section": "6️⃣ Uruchomienie konsumenta w konsoli",
    "text": "6️⃣ Uruchomienie konsumenta w konsoli\nAby sprawdzić, czy wysyłanie wiadomości działa, otwórz kolejne okno terminala i uruchom konsumenta:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\nTeraz wszystkie wiadomości wysłane przez producenta powinny pojawić się w konsoli konsumenta.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#zakończenie-pracy",
    "href": "kafka_codes/kafka2.html#zakończenie-pracy",
    "title": "Producent w środowisku python",
    "section": "7️⃣ Zakończenie pracy",
    "text": "7️⃣ Zakończenie pracy\nPamiętaj, aby uruchamiać komendy z odpowiedniego katalogu. Po zakończeniu ćwiczeń użyj Ctrl+C, aby zatrzymać zarówno producenta, jak i konsumenta.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w środowisku python"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html",
    "href": "lectures/wyklad4.html",
    "title": "Wykład 4",
    "section": "",
    "text": "Rozwój technologii, szczególnie przejście od monolitów do mikroserwisów, miał ogromny wpływ na współczesne systemy informatyczne. Monolityczne aplikacje, które były dominującym podejściem w przeszłości, stanowiły jedną, dużą jednostkę kodu. Takie podejście miało swoje zalety, takie jak prostota na początkowych etapach rozwoju systemu, ale także istotne wady, w tym trudności w skalowaniu, ograniczoną elastyczność i skomplikowaną konserwację.\nW miarę jak technologia ewoluowała, pojawiły się mikroserwisy – podejście, które polega na dzieleniu aplikacji na mniejsze, niezależne usługi, z których każda odpowiada za określoną funkcjonalność. Przejście na mikroserwisy umożliwiło większą elastyczność, łatwiejsze skalowanie systemów oraz szybkie wdrażanie nowych funkcji. Ponadto każda usługa może być rozwijana, testowana i wdrażana niezależnie, co upraszcza zarządzanie kodem i zmniejsza ryzyko błędów.\nDzięki mikroserwisom organizacje mogą lepiej dostosować się do zmieniających się potrzeb biznesowych, poprawić dostępność systemów (poprzez izolowanie awarii do pojedynczych usług) oraz szybciej wprowadzać innowacje. Dodatkowo, mikroserwisy sprzyjają stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiązania, co dodatkowo ułatwia zarządzanie infrastrukturą i pozwala na lepsze wykorzystanie zasobów.\nJednakże, mimo wielu korzyści, przejście do mikroserwisów wiąże się również z wyzwaniami, takimi jak:\n\nzłożoność zarządzania komunikacją między usługami,\nkonieczność monitorowania i utrzymania większej liczby komponentów\nzarządzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzędzi i podejść do zarządzania oraz wdrożenia kultury DevOps.\nWraz z rozwojem mikroserwisów, pojawiły się także nowe technologie, takie jak serverless i konteneryzacja, które stanowią naturalne rozszerzenie elastyczności systemów. Te technologie jeszcze bardziej zwiększają efektywność zarządzania i skalowania nowoczesnych aplikacji, stając się kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w którym deweloperzy nie muszą zarządzać serwerami ani infrastrukturą. Zamiast tego, dostawcy chmurowi zajmują się całą infrastrukturą, a programiści koncentrują się jedynie na kodzie aplikacji. Kluczowym atutem tego podejścia jest jego skalowalność – aplikacje automatycznie skalują się w zależności od zapotrzebowania na zasoby. Systemy serverless pozwalają na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztów (płacisz tylko za faktyczne wykorzystanie zasobów). To podejście ułatwia zarządzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest także doskonałym uzupełnieniem dla mikroserwisów, pozwalając na uruchamianie niezależnych funkcji w odpowiedzi na różne zdarzenia, co daje jeszcze większą elastyczność. Może być wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsługa API czy automatyzacja zadań.\n\n\n\nKonteneryzacja (np. przy użyciu Docker) to kolejny krok w kierunku zwiększenia elastyczności. Dzięki kontenerom, aplikacje oraz ich zależności są zapakowane w izolowane jednostki, które można uruchamiać w różnych środowiskach w sposób spójny i przewidywalny. Kontenery są lekkie, szybkie do uruchomienia i oferują łatwość w przenoszeniu aplikacji między różnymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczególnie w połączeniu z narzędziami do zarządzania kontenerami, takimi jak Kubernetes, które automatycznie skalują aplikacje, monitorują ich stan, zapewniają wysoką dostępność oraz zarządzają ich cyklem życia. To podejście idealnie wspiera zarówno mikroserwisy, jak i serverless, umożliwiając łatwe wdrażanie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarówno serverless, jak i konteneryzacja, stanowią dalszy krok w kierunku elastyczności, oferując możliwość szybkiej reakcji na zmieniające się warunki i zapotrzebowanie. Wspólnie z mikroserwisami tworzą nowoczesne podejście do architektury aplikacji, które pozwala na rozdzielenie odpowiedzialności, łatwiejsze skalowanie, dynamiczne dostosowywanie zasobów i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umożliwia firmom szybkie wdrażanie nowych funkcji, reagowanie na zmieniające się potrzeby użytkowników oraz minimalizowanie kosztów poprzez optymalne wykorzystanie zasobów, co jest szczególnie istotne w dzisiejszym, dynamicznie zmieniającym się środowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#historia-podejścia-do-architektury",
    "href": "lectures/wyklad4.html#historia-podejścia-do-architektury",
    "title": "Wykład 4",
    "section": "",
    "text": "Rozwój technologii, szczególnie przejście od monolitów do mikroserwisów, miał ogromny wpływ na współczesne systemy informatyczne. Monolityczne aplikacje, które były dominującym podejściem w przeszłości, stanowiły jedną, dużą jednostkę kodu. Takie podejście miało swoje zalety, takie jak prostota na początkowych etapach rozwoju systemu, ale także istotne wady, w tym trudności w skalowaniu, ograniczoną elastyczność i skomplikowaną konserwację.\nW miarę jak technologia ewoluowała, pojawiły się mikroserwisy – podejście, które polega na dzieleniu aplikacji na mniejsze, niezależne usługi, z których każda odpowiada za określoną funkcjonalność. Przejście na mikroserwisy umożliwiło większą elastyczność, łatwiejsze skalowanie systemów oraz szybkie wdrażanie nowych funkcji. Ponadto każda usługa może być rozwijana, testowana i wdrażana niezależnie, co upraszcza zarządzanie kodem i zmniejsza ryzyko błędów.\nDzięki mikroserwisom organizacje mogą lepiej dostosować się do zmieniających się potrzeb biznesowych, poprawić dostępność systemów (poprzez izolowanie awarii do pojedynczych usług) oraz szybciej wprowadzać innowacje. Dodatkowo, mikroserwisy sprzyjają stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiązania, co dodatkowo ułatwia zarządzanie infrastrukturą i pozwala na lepsze wykorzystanie zasobów.\nJednakże, mimo wielu korzyści, przejście do mikroserwisów wiąże się również z wyzwaniami, takimi jak:\n\nzłożoność zarządzania komunikacją między usługami,\nkonieczność monitorowania i utrzymania większej liczby komponentów\nzarządzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzędzi i podejść do zarządzania oraz wdrożenia kultury DevOps.\nWraz z rozwojem mikroserwisów, pojawiły się także nowe technologie, takie jak serverless i konteneryzacja, które stanowią naturalne rozszerzenie elastyczności systemów. Te technologie jeszcze bardziej zwiększają efektywność zarządzania i skalowania nowoczesnych aplikacji, stając się kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w którym deweloperzy nie muszą zarządzać serwerami ani infrastrukturą. Zamiast tego, dostawcy chmurowi zajmują się całą infrastrukturą, a programiści koncentrują się jedynie na kodzie aplikacji. Kluczowym atutem tego podejścia jest jego skalowalność – aplikacje automatycznie skalują się w zależności od zapotrzebowania na zasoby. Systemy serverless pozwalają na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztów (płacisz tylko za faktyczne wykorzystanie zasobów). To podejście ułatwia zarządzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest także doskonałym uzupełnieniem dla mikroserwisów, pozwalając na uruchamianie niezależnych funkcji w odpowiedzi na różne zdarzenia, co daje jeszcze większą elastyczność. Może być wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsługa API czy automatyzacja zadań.\n\n\n\nKonteneryzacja (np. przy użyciu Docker) to kolejny krok w kierunku zwiększenia elastyczności. Dzięki kontenerom, aplikacje oraz ich zależności są zapakowane w izolowane jednostki, które można uruchamiać w różnych środowiskach w sposób spójny i przewidywalny. Kontenery są lekkie, szybkie do uruchomienia i oferują łatwość w przenoszeniu aplikacji między różnymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczególnie w połączeniu z narzędziami do zarządzania kontenerami, takimi jak Kubernetes, które automatycznie skalują aplikacje, monitorują ich stan, zapewniają wysoką dostępność oraz zarządzają ich cyklem życia. To podejście idealnie wspiera zarówno mikroserwisy, jak i serverless, umożliwiając łatwe wdrażanie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarówno serverless, jak i konteneryzacja, stanowią dalszy krok w kierunku elastyczności, oferując możliwość szybkiej reakcji na zmieniające się warunki i zapotrzebowanie. Wspólnie z mikroserwisami tworzą nowoczesne podejście do architektury aplikacji, które pozwala na rozdzielenie odpowiedzialności, łatwiejsze skalowanie, dynamiczne dostosowywanie zasobów i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umożliwia firmom szybkie wdrażanie nowych funkcji, reagowanie na zmieniające się potrzeby użytkowników oraz minimalizowanie kosztów poprzez optymalne wykorzystanie zasobów, co jest szczególnie istotne w dzisiejszym, dynamicznie zmieniającym się środowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#wpływ-technologii-na-systemy-informatyczne",
    "href": "lectures/wyklad4.html#wpływ-technologii-na-systemy-informatyczne",
    "title": "Wykład 4",
    "section": "Wpływ technologii na systemy informatyczne",
    "text": "Wpływ technologii na systemy informatyczne\nKomunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe oraz Big Data znacząco zmieniły sposób budowania systemów informatycznych i wykonywania w nich pracy.\nPodobnie, narzędzia do przekazu informacji – takie jak gazeta, radio, telewizja, internet, komunikatory i media społecznościowe – wpłynęły na interakcje międzyludzkie oraz struktury społeczne.\nKażde nowe medium technologiczne kształtuje sposób, w jaki ludzie korzystają z informatyki i postrzegają jej rolę w codziennym życiu.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#mikrousługi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "href": "lectures/wyklad4.html#mikrousługi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "title": "Wykład 4",
    "section": "Mikrousługi (Mikroserwisy) w nowoczesnej architekturze IT",
    "text": "Mikrousługi (Mikroserwisy) w nowoczesnej architekturze IT\nJednym z najpopularniejszych podejść do budowy systemów informatycznych jest koncepcja mikrousług (microservices).\nJest ona szeroko stosowana zarówno w tworzeniu oprogramowania, jak i w prowadzeniu firm opartych na analizie danych (Data-Driven).\n\nGłówne zalety mikroserwisów:\n\nWydajność – każda usługa realizuje jedno, dobrze określone zadanie (“rób jedną rzecz, ale dobrze”).\nElastyczność – umożliwiają łatwe modyfikacje i skalowanie systemu.\nPrzejrzystość architektury – system składa się z niewielkich, niezależnych modułów.\n\nMikroserwisy można porównać do czystych funkcji w programowaniu funkcyjnym – każda usługa działa niezależnie i posiada jasno określone wejścia oraz wyjścia.\nAby umożliwić komunikację między mikroserwisami, często wykorzystuje się Application Programming Interfaces (API), które pozwalają na wymianę danych i integrację różnych usług.\n\n\nPrzykład API w mikroserwisach – Python & FastAPI\nPoniżej znajduje się przykładowy mikroserwis REST API w Pythonie z użyciem FastAPI, który zwraca informacje o użytkownikach:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# Przykładowe dane użytkowników\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane użytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nJak to działa?\n\nUruchamiamy serwer FastAPI.\nMożemy uzyskać dane użytkownika, wysyłając żądanie GET do http://127.0.0.1:8000/users/1.\nAPI zwróci dane w formacie JSON, np.:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API (Application Programming Interface).\nAPI umożliwia komunikację i integrację między różnymi mikroserwisami, podobnie jak strona internetowa komunikuje się z przeglądarką.\nGdy odwiedzasz stronę internetową, serwer wysyła kod, który Twoja przeglądarka interpretuje i wyświetla jako stronę internetową.\nPodobnie działa API – wysyła odpowiedzi na zapytania klienta.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usługa",
    "href": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usługa",
    "title": "Wykład 4",
    "section": "Przypadek biznesowy: Model ML jako usługa",
    "text": "Przypadek biznesowy: Model ML jako usługa\nZałóżmy, że pracujesz w firmie zajmującej się sprzedażą nieruchomości w Bostonie.\nChcesz zwiększyć sprzedaż i poprawić jakość obsługi klientów poprzez nową aplikację mobilną, z której może korzystać nawet 1 000 000 użytkowników jednocześnie.\nJednym z rozwiązań jest udostępnienie prognozy wartości domu w czasie rzeczywistym.\nGdy użytkownik prosi o wycenę nieruchomości, aplikacja wysyła zapytanie do serwera, który przetwarza je za pomocą modelu Machine Learning (ML) i zwraca oszacowaną wartość.\n\nCzym jest serwowanie modelu ML?\nTrening dobrego modelu ML to dopiero pierwszy krok całego procesu.\nAby model był użyteczny, musisz go udostępnić użytkownikom końcowym, np. w formie API.\n\n\nJak to zrobić?\n\nPotrzebujesz:\n\n\nwytrenowanego modelu ML,\ninterpreter modelu (np. TensorFlow, Scikit-Learn, PyTorch),\ndanych wejściowych dla modelu.\n\n\nKluczowe metryki jakości serwowania modelu:\nCzas odpowiedzi (latency),\nKoszt uruchomienia modelu (infrastruktura serwerowa),\nLiczba zapytań na sekundę (QPS – Queries Per Second).\n\n\nUdostępnianie danych między systemami zawsze było kluczowym wyzwaniem w tworzeniu oprogramowania.\nW obszarze tradycyjnego DevOps koncentrujemy się na infrastrukturze, a w MLOps – na wdrażaniu i utrzymaniu modeli uczenia maszynowego.\n\nZbudowanie systemu przygotowanego do środowiska produkcyjnego jest bardziej skomplikowane niż wytrenowanie samego modelu:\n\nczyszczenie i załadowanie odpowiednich i zwalidowanych danych\nObliczenie zmiennych i ich serwowanie na właściwym środowisku\nSerwowanie modelu w sposób najbardziej efektywny ze względu na koszty\nWersjonowanie, śledzenie i udostępnianie danych i modeli oraz innych artefaktów\nMonitorowanie infrastruktury i modelu\nWdrożenie modelu na skalowalnej infrastrukturze\nAutomatyzacja procesu wdrażania i treningu",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#jak-działa-api",
    "href": "lectures/wyklad4.html#jak-działa-api",
    "title": "Wykład 4",
    "section": "Jak działa API?",
    "text": "Jak działa API?\nKiedy wywołasz interfejs API, serwer otrzymuje Twoje żądanie, przetwarza je i generuje odpowiedź.\nJeśli wszystko działa poprawnie, otrzymasz wynik w formacie JSON lub XML.\nJeśli wystąpi błąd, API zwróci kod błędu HTTP (np. 400 – nieprawidłowe żądanie, 500 – błąd serwera).\n\nKluczowe zasady REST API:\n\nKlient-Serwer → Klient (np. aplikacja mobilna) wysyła żądanie HTTP do API hostowanego na serwerze, który zwraca odpowiedź.\nDziała to identycznie jak przeglądarka internetowa, która wysyła żądanie do serwera WWW i otrzymuje stronę HTML.\nBezstanowość → Każde żądanie klienta musi zawierać wszystkie niezbędne informacje do przetworzenia odpowiedzi,\nAPI nie powinno przechowywać informacji o wcześniejszych żądaniach użytkownika.\n\n\n\nPrzykład: API serwujące model ML\nPoniżej znajduje się przykładowy serwis API, który udostępnia model ML do prognozowania ceny nieruchomości,\nz wykorzystaniem FastAPI oraz Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wcześniej wytrenowany model ML (np. regresję liniową)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Prognoza ceny nieruchomości na podstawie cech:\n    - area (powierzchnia w m²),\n    - bedrooms (liczba sypialni),\n    - age (wiek budynku w latach).\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n\n\ndomenę,\nport,\ndodatkowe ścieżki,\nzapytanie\n\n\nMetody HTTP:\n\n\nGET,\nPOST\n\n\nNagłówki HTTP zawierają:\n\n\ninformacje o autoryzacji,\ncookies metadata\n\nCała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens 4. Ciało zapytania\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidłowe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#publikujsubskrybuj",
    "href": "lectures/wyklad4.html#publikujsubskrybuj",
    "title": "Wykład 4",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyłania wiadomości „Publikuj/Subskrybuj” ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujący się tym, że nadawca (publikujący) fragmentu danych (wiadomości) nie kieruje go wprost do odbiorcy. pub/sub to systemy, które często posiadają brokera czyli centralny punkt, w którym znajdują się wiadomości.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#apache-kafka",
    "href": "lectures/wyklad4.html#apache-kafka",
    "title": "Wykład 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicję:\n\nRozproszona platforma streamingowa\nCo to jest „platforma rozproszonego przesyłania strumieniowego”?\nNajpierw chcę przypomnieć, czym jest „strumień”. Strumienie to po prostu nieograniczone dane, dane, które nigdy się nie kończą. Ciągle ich przybywa i możesz przetwarzać je w czasie rzeczywistym.\nA „rozproszone”? Rozproszony oznacza, że ​​Kafka działa w klastrze, a każdy węzeł w grupie nazywa się Brokerem. Ci brokerzy to po prostu serwery wykonujące kopię Apache Kafka.\nTak więc Kafka to zestaw współpracujących ze sobą maszyn, aby móc obsługiwać i przetwarzać nieograniczone dane w czasie rzeczywistym.\nBrokerzy sprawiają, że jest niezawodny, skalowalny i odporny na błędy. Ale dlaczego panuje błędne przekonanie, że Kafka to kolejny „kolejkowy system przesyłania wiadomości”?\nAby odpowiedzieć na tę odpowiedź, musimy najpierw wyjaśnić, jak działa kolejkowe przesyłanie wiadomości.\n\n\nKolejkowy system przesyłania wiadomości\nPrzesyłanie wiadomości, to po prostu czynność wysyłania wiadomości z jednego miejsca do drugiego. Ma trzech głównych “aktorów”:\n\nProducent: Który tworzy i wysyła komunikaty do jednej lub więcej kolejek;\nKolejka: struktura danych bufora, która odbiera (od producentów) i dostarcza komunikaty (do konsumentów) w sposób FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jedną lub więcej kolejek i otrzymuje ich wiadomości po opublikowaniu.\n\nI to jest to; tak działa przesyłanie wiadomości. Jak widać, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Apache Kafka\nWięcej informacji na temat Kafki znajdziesz w tym linku.\nTeraz, gdy rozumiemy podstawy przesyłania wiadomości, zagłębmy się w świat Apache Kafka.\nW Kafce mamy dwa kluczowe pojęcia: Producentów (Producers) i Konsumentów (Consumers),\nktórzy działają w podobny sposób jak w klasycznych systemach kolejkowych, produkując i konsumując wiadomości.\n\n\nJak widać, Kafka przypomina klasyczny system przesyłania wiadomości, jednak w odróżnieniu od tradycyjnych kolejek,\nzamiast pojęcia kolejki (queue) mamy Tematy (Topics).\n\n\nTematy (Topics) i Partycje (Partitions)\nTemat (Topic) to podstawowy kanał przesyłania danych w Kafce.\nMożna go porównać do folderu, w którym przechowywane są wiadomości.\nKażdy temat posiada jedną lub więcej partycji (Partitions) – podział ten wpływa na skalowalność i równoważenie obciążenia.\nPodczas tworzenia tematu określamy liczbę partycji.\n\nKluczowe cechy tematów i partycji:\n\nTemat to logiczna jednostka, do której producenci wysyłają wiadomości, a konsumenci je odczytują.\nPartycja to fizyczny podział tematu. Można ją porównać do plików w folderze.\nOffset – każda wiadomość w partycji otrzymuje unikalny identyfikator (offset),\nktóry pozwala konsumentom śledzić, które wiadomości zostały już przetworzone.\nKafka przechowuje wiadomości na dysku, dzięki czemu może je ponownie odczytać (w przeciwieństwie do klasycznych kolejek, gdzie wiadomość jest usuwana po przetworzeniu).\nKonsumenci odczytują wiadomości sekwencyjnie, od najstarszej do najnowszej.\nW przypadku awarii konsument może wznowić przetwarzanie od ostatniego zapisanego offsetu.\n\n\n\n\n\n\nBrokerzy (Brokers) i Klaster Kafka\nKafka działa w sposób rozproszony – oznacza to, że może składać się z wielu brokerów (Brokers),\nktóre współpracują jako jeden klaster.\n\n\n\nKluczowe informacje o brokerach\n\nBroker to pojedynczy serwer w klastrze Kafki, odpowiedzialny za przechowywanie partycji tematów.\nKażdy broker w klastrze ma unikalny identyfikator.\nAby zwiększyć dostępność i niezawodność, Kafka wykorzystuje replikację danych.\nWspółczynnik replikacji określa, ile kopii danej partycji ma być przechowywane na różnych brokerach.\nJeśli temat ma trzy partycje i współczynnik replikacji równy trzy,\noznacza to, że każda partycja zostanie powielona na trzech różnych brokerach.\n\nLiczba partycji powinna być dobrana w taki sposób, aby każdy broker miał co najmniej jedną partycję do obsługi.\n\n\n\nProducenci (Producers)\nW Kafka producenci to aplikacje lub usługi, które tworzą i wysyłają wiadomości do tematów.\nDziała to podobnie do systemów kolejkowych, z tą różnicą, że Kafka zapisuje wiadomości w partycjach.\n\nJak Kafka przypisuje wiadomości do partycji?\n\nWiadomości są rozsyłane okrężnie (round-robin) do dostępnych partycji.\nMożemy określić klucz wiadomości, a Kafka wyliczy jego hash,\naby określić, do której partycji trafi wiadomość.\nKlucz wiadomości determinuje przypisanie do partycji – jeśli temat został już utworzony,\nliczba partycji nie może być zmieniona bez zakłócenia tego mechanizmu.\n\nPrzykład przypisania wiadomości do partycji:\n\nWiadomość 01 trafia do partycji 0 tematu Topic_1.\nWiadomość 02 trafia do partycji 1 tego samego tematu.\nKolejna wiadomość może ponownie trafić do partycji 0, jeśli stosujemy przypisanie round-robin.\n\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# Wysyłamy wiadomość do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"Nowe mieszkanie na sprzedaż\"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"Wiadomość wysłana do tematu '{topic}'\")\n\n\n\nKonsumenci (Consumers)\nKonsumenci w Kafce odczytują i przetwarzają wiadomości z tematów. Każdy konsument może należeć do grupy konsumentów (Consumer Group), co pozwala na równoległe przetwarzanie wiadomości. - Jeśli wielu konsumentów należy do tej samej grupy, Kafka równoważy obciążenie między nimi. - Jeśli jeden konsument przestanie działać, Kafka automatycznie przypisze jego partycje do innego aktywnego konsumenta.\nPrzykład konsumenta w Pythonie:\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, który nasłuchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Oczekiwanie na wiadomości...\")\n\nfor message in consumer:\n    print(f\"Otrzymano wiadomość: {message.value.decode()}\")\nInnym ważnym pojęciem Kafki są „Grupy konsumentów”. Jest to bardzo ważne, gdy musimy skalować odczytywanie wiadomości. Staje się to bardzo kosztowne, gdy pojedynczy konsument musi czytać z wielu partycji, więc musimy zrównoważyć obciążenie między naszymi konsumentami, wtedy wchodzą grupy konsumentów.\nDane z jednego tematu będą równoważone obciążeniem między konsumentami, dzięki czemu możemy zagwarantować, że nasi konsumenci będą w stanie obsługiwać i przetwarzać dane. Ideałem jest posiadanie takiej samej liczby konsumentów w grupie, jaką mamy jako partycje w temacie, w ten sposób każdy konsument czyta tylko z jednego. Podczas dodawania konsumentów do grupy należy uważać, jeśli liczba konsumentów jest większa niż liczba partycji, niektórzy konsumenci nie będą czytać z żadnego tematu i pozostaną bezczynni.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 4"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html",
    "href": "lectures/wyklad3.html",
    "title": "Wykład 3",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, podstawowych sposobów przetwarzania i analizowania danych strumieniowych.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#definicje",
    "href": "lectures/wyklad3.html#definicje",
    "title": "Wykład 3",
    "section": "Definicje",
    "text": "Definicje\n\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 – Zdarzenie to wszystko, co można zaobserwować w danym momencie czasu. Jest generowane jako bezpośredni skutek działania.\nDefinicja 2 – W kontekście danych zdarzenie to niezmienialny rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.\nDefinicja 3 – Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie, np. logi z urządzeń.\nDefinicja 4 – Strumień danych to dane tworzone przyrostowo w czasie, generowane ze źródeł statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\n\n\n\nAnalityka strumieniowa\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) – czyli przetwarzaniem dużych ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologii, wszystkie dane powstają jako ciągły strumień zdarzeń – obejmuje to m.in.:\n\ndziałania użytkowników na stronach internetowych,\n\nlogi systemowe,\n\npomiary z sensorów.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Wykład 3",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma żadnego związku z momentem wystąpienia analizowanych zdarzeń.\nNatomiast w przetwarzaniu strumieniowym wyróżniamy dwie koncepcje czasu:\n\nCzas zdarzenia (event time) – moment, w którym zdarzenie faktycznie miało miejsce.\nCzas przetwarzania (processing time) – moment, w którym system przetwarza zdarzenie.\n\nIdealne przetwarzanie danych\nW idealnej sytuacji przetwarzanie następuje natychmiast po wystąpieniu zdarzenia:\n\nRzeczywiste przetwarzanie danych\nW praktyce przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co jest widoczne jako punkty poniżej linii idealnego przetwarzania (poniżej przekątnej na wykresie).\n\nW aplikacjach przetwarzania strumieniowego istotna jest różnica między czasem powstania zdarzenia a czasem jego przetwarzania. Do najczęstszych przyczyn opóźnień należą:\n\nprzesyłanie danych przez sieć,\nbrak komunikacji między urządzeniem a siecią.\n\nPrzykładem jest śledzenie położenia samochodu przez aplikację GPS – przejazd przez tunel może spowodować chwilową utratę danych.\nObsługa opóźnień w przetwarzaniu strumieniowym\nOpóźnienia w przetwarzaniu zdarzeń można obsłużyć na dwa sposoby:\n\nMonitorowanie liczby pominiętych zdarzeń i wyzwalanie alarmu w przypadku zbyt dużej liczby odrzuceń.\nZastosowanie korekty za pomocą watermarkingu, czyli dodatkowego mechanizmu uwzględniającego opóźnione zdarzenia.\n\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić jako funkcję schodkową:\n\nNie wszystkie zdarzenia wnoszą wkład do analizy – niektóre mogą zostać odrzucone ze względu na zbyt duże opóźnienie.\nWykorzystanie watermarkingu pozwala na uwzględnienie dodatkowego czasu na pojawienie się opóźnionych zdarzeń. Proces ten obejmuje wszystkie zdarzenia powyżej przerywanej linii. Mimo to nadal mogą zdarzyć się przypadki, w których niektóre punkty zostaną pominięte.\n\nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "href": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "title": "Wykład 3",
    "section": "Okna czasowe w analizie strumieniowej",
    "text": "Okna czasowe w analizie strumieniowej\nW przetwarzaniu strumieniowym okna czasowe pozwalają na grupowanie danych w ograniczone czasowo segmenty, co umożliwia analizę zdarzeń w określonych przedziałach czasowych. W zależności od zastosowania stosuje się różne typy okien, dostosowane do charakterystyki danych i wymagań analitycznych.\n\n\nOkno rozłączne (Tumbling Window)\nJest to okno o stałej długości, które nie nakłada się na siebie – każde zdarzenie należy tylko do jednego okna.\n✅ Charakterystyka:\n\nStała długość okna\n\nBrak nakładania się na siebie\n\nIdealne do podziału danych na równe segmenty czasowe\n\n📌 Przykład: Analiza liczby zamówień w sklepie internetowym co 5 minut.\n\n\n\n\nOkno przesuwne (Sliding Window)\nObejmuje wszystkie zdarzenia następujące w określonym przedziale czasu, gdzie okno przesuwa się w sposób ciągły.\n✅ Charakterystyka:\n\nKażde zdarzenie może należeć do kilku okien\n\nOkno przesuwa się o zadany interwał\n\nPrzydatne do wykrywania trendów i anomalii\n\n📌 Przykład: Śledzenie średniej temperatury w ciągu ostatnich 10 minut, aktualizowane co 2 minuty.\n\n\n\n\nOkno skokowe (Hopping Window)\nJest podobne do okna rozłącznego, ale pozwala na nakładanie się okien na siebie, dzięki czemu jedno zdarzenie może należeć do kilku okien. Jest stosowane do wygładzania danych.\n✅ Charakterystyka:\n\nStała długość okna\n\nMożliwość nakładania się na siebie\n\nPrzydatne do redukcji szumów w danych\n\n📌 Przykład: Analiza liczby odwiedzających stronę co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywać trendy.\n\n\n\n\nOkno sesyjne (Session Window)\nOkno sesyjne grupuje zdarzenia na podstawie okresów aktywności i zamyka się po określonym czasie braku aktywności.\n✅ Charakterystyka:\n\nDynamiczna długość okna\n\nDefiniowane przez aktywność użytkownika\n\nStosowane w analizie sesji użytkowników\n\n📌 Przykład: Analiza sesji użytkowników na stronie internetowej – sesja trwa tak długo, jak długo użytkownik wykonuje akcje, ale kończy się po 15 minutach braku aktywności.\n\n\n\nPodsumowanie\nRóżne rodzaje okien czasowych są stosowane w zależności od specyfiki danych i celów analizy. Wybór odpowiedniego okna wpływa na dokładność wyników i efektywność systemu analitycznego.\n\n\n\n\n\n\n\n\nTyp okna\nCharakterystyka\nZastosowanie\n\n\n\n\nRozłączne (Tumbling)\nStała długość, brak nakładania\nRaporty okresowe\n\n\nPrzesuwne (Sliding)\nStała długość, nakładające się okna\nTrendy, wykrywanie anomalii\n\n\nSkokowe (Hopping)\nStała długość, częściowe nakładanie\nWygładzanie danych\n\n\nSesyjne (Session)\nDynamiczna długość, zależna od aktywności\nAnaliza sesji użytkowników\n\n\n\nKażdy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. Wybór właściwej metody zależy od potrzeb biznesowych i charakterystyki analizowanych danych.\nW analizie danych strumieniowych interpretacja czasu jest złożonym zagadnieniem, ponieważ:\n\nRóżne systemy mają różne zegary, co może prowadzić do niespójności,\nDane mogą docierać z opóźnieniem, co wymaga technik watermarkingu i okien czasowych,\nRóżne podejścia do analizy czasu zdarzenia i czasu przetwarzania wpływają na dokładność wyników.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 3"
    ]
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli III bud G\n\n22-02-2025 (sobota) 08:00-09:30 - Wykład 1\n08-03-2025 (sobota) 08:00-09:30 - Wykład 2\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć) 20 pytań.\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentów z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacją w Dockerze.\nNauczysz się:\n\nTworzenia prostego REST API,\nObsługi zapytań HTTP i obsługi błędów w API,\nTestowania API z wykorzystaniem pytest,\nPrzenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#cel",
    "href": "labs/lab1.html#cel",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentów z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacją w Dockerze.\nNauczysz się:\n\nTworzenia prostego REST API,\nObsługi zapytań HTTP i obsługi błędów w API,\nTestowania API z wykorzystaniem pytest,\nPrzenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "href": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "1. Tworzenie aplikacji REST API",
    "text": "1. Tworzenie aplikacji REST API\nNaszym zadaniem jest wystawienie aplikacji w Pythonie, która na żądanie klienta udzieli odpowiedzi na podstawie predykcji wygenerowanej przez model.\nAplikację napiszemy w Pythonie z wykorzystaniem Flask 3.0.3.\n\nKod minimalnej aplikacji Flask\nNaszą aplikację chcemy uruchomić lokalnie, a następnie w prosty sposób przenieść i uruchomić na dowolnym komputerze. Dlatego naturalnym rozwiązaniem jest zapisanie kodu w pliku z rozszerzeniem .py.\nAby automatycznie zapisać kod aplikacji do pliku app.py, wykorzystamy magiczną komendę %%file plik.py.\n\n%%file app.py\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\n\nif __name__ == '__main__':\n    app.run()\n\nWriting app.py\n\n\n\nUwaga! W dokumentacji Flask w kodzie podstawowej aplikacji nie występują dwie ostatnie linie odpowiedzialne za uruchomienie serwera.\n\nif __name__ == '__main__':\n    app.run()\nWyjaśnijmy co zawiera przykładowy kod.\n\nfrom flask import Flask Załadowanie biblioteki\napp = Flask(__name__) utworzenie interfejsu serwera API\nkod podstrony z wykorzystaniem dekoratora\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\nDekoratory w Pythonie pozwalają modyfikować zachowanie funkcji bez zmiany jej kodu. Flask wykorzystuje dekoratory do tworzenia tras (@app.route), ale można je także stosować w analizie danych – np. do logowania czasu wykonania funkcji lub obsługi błędów.\n\nPrzykład: Normalizacja wartości w danych\nZałóżmy, że mamy funkcję, która pobiera dane z pliku CSV i zwraca listę wartości. Dodamy dekorator, który automatycznie przeskaluje dane do zakresu 0-1, co często jest wymagane przed analizą statystyczną lub trenowaniem modeli ML.\n\nimport numpy as np\n\n# Dekorator do normalizacji danych\ndef normalize_data(func):\n    def wrapper(*args, **kwargs):\n        data = func(*args, **kwargs)  # Pobranie oryginalnych danych\n        min_val, max_val = min(data), max(data)\n        normalized = [(x - min_val) / (max_val - min_val) for x in data]\n        print(\"Dane po normalizacji:\", normalized)\n        return normalized\n    return wrapper\n\n\n@normalize_data\ndef get_data():\n    return [10, 15, 20, 30, 50]\n\nget_data()\n\nDane po normalizacji: [0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n[0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n\nĆwiczenie: „Napisz dekorator, który zaokrągla wartości do 2 miejsc po przecinku.”\n\n\n\n\nObsługa błędów w API\nDodajmy obsługę błędów, np. kiedy klient poda niepoprawne dane:\n@app.errorhandler(404)\ndef not_found(error):\n    return jsonify({\"error\": \"Not Found\"}), 404\n\n@app.errorhandler(400)\ndef bad_request(error):\n    return jsonify({\"error\": \"Bad Request\"}), 400",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "href": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "Uruchomienie serwera lokalnie",
    "text": "Uruchomienie serwera lokalnie\nUruchomienie serwera moze odbyć się na przynajmniej na dwa sposoby.\n\nUruchomienie serwera przez terminal\nOtwórz termianal w lokalizacji gdzie znajduje się plik aplikacji\npython app.py\nlub (jeśli nie ma fragmentu app.run())\nflask run\nPowinna pojawić się informacja podobna do ponizszej:\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\nW celu weryfikacji działania możesz otworzyć nowe okno terminalu wpisując:\ncurl localhost:5000\n\\{\"message\":\"Hello, World!\"\\}\n\n\nUruchomienie serwera w notatniku\nBezpośrenie uruchomienia kodu w notatniku spowoduje uruchomienie serwera i zatrzymanie jakiejkolwiek mozliwości realizacji kodu. Aby tego uniknąć mozesz wykorzystać bibliotekę subprocess.\n\nimport subprocess\np = subprocess.Popen([\"python\", \"app.py\"])\n\nJeśli potrzebujemy zamknąć subprocess wykonaj:\n\np.kill()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#testowanie-api",
    "href": "labs/lab1.html#testowanie-api",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "2. Testowanie API",
    "text": "2. Testowanie API\nDo testowania API wykorzystamy pytest oraz bibliotekę requests. ### Instalacja pytest:\n\n!pip install pytest requests -q\n\n\n%%file test_app.py\nimport pytest\nimport requests\n\ndef test_home():\n    response = requests.get(\"http://127.0.0.1:5000/\")\n    assert response.status_code == 200\n    assert response.json()[\"message\"] == \"Hello, World!\"\n\nWriting test_app.py\n\n\n\n!pytest test_app.py\n\n============================= test session starts ==============================\nplatform linux -- Python 3.11.6, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/jovyan/notebooks\nplugins: anyio-4.0.0\ncollected 1 item                                                               \n\ntest_app.py .                                                            [100%]\n\n============================== 1 passed in 0.03s ===============================\n\n\n\n# wersja bez testu\nimport requests\nresponse = requests.get(\"http://127.0.0.1:5000/\")\nprint(response.json())\n\n{'message': 'Hello, World!'}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#środowisko-python",
    "href": "labs/lab1.html#środowisko-python",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "Środowisko Python",
    "text": "Środowisko Python\nAby uruchomić kod aplikacji app.py, potrzebujemy interpretera języka Python zainstalowanego na naszym komputerze. Jednak samo posiadanie interpretera nie jest wystarczające – aby aplikacja działała poprawnie, należy utworzyć środowisko (najlepiej wirtualne), w którym będą dostępne wszystkie wymagane biblioteki, takie jak Flask.\n\nuwaga: wszystkie polecenia terminala dotyczyć będą wersji linux/mac os\n\nW pierwszej kolejności sprawdź czy dostępne są polecenia pozwalające realizować kod pythonowy.\nwhich python\nwhich python3\nwhich pip \nwhich pip3\nWszystkie te polecenia powinny wskazyać na folder z domyślnym środowiskiem Pythona.\nWygeneruj i uruchom środowisko wirtualne lokalnie wpisując w terminalu:\npython3 -m venv .venv\nsource .venv/bin/activate\n\nDobra praktyka: środowisko python to nic innego jak katalog. W naszej wersji to katalog ukryty o nazwie .venv. Jeśli skopiujesz ten katalog gdzie indziej przestanie pełnić on swoją funkcję środowiska python. Dlatego jego odtworzenie nie polega na jego kopiowaniu. Jeśli Twój projekt jest powiązany ze środowiskiem kontroli wersji GIT zadbaj aby katalog środowiska nie był dodawany do repozytorium. Mozesz wykonać to działanie dodając odpowiedni wpis do pliki .gitignore\n\nPosiadając utworzone nowe środowisko sprawdź jakie biblioteki się w nim znajdują.\npip list \n\nPackage    Version\n---------- -------\npip        23.2.1\nsetuptools 65.5.0\nMozemy ponownie sprawdzić polecenia python i pip:\nwhich python\nwhich pip \nDomyślnie powinny pojawić się biblioteki pip oraz setuptools.\nDoinstaluj bibliotekę flask.\npip install flask==3.0.3\npip list \nPackage      Version\n------------ -------\nblinker      1.7.0\nclick        8.1.7\nFlask        3.0.3\nitsdangerous 2.1.2\nJinja2       3.1.3\nMarkupSafe   2.1.5\npip          23.2.1\nsetuptools   65.5.0\nWerkzeug     3.0.2\nJak widać instalacja biblioteki flask wymusiła doinstalowanie równiez innych pakietów.\nJedyną mozliwością przeniesienia środowiska python jest jego ponowna instalacja na nowej maszynie i instalacja wszystkich pakietów. Aby jednak nie instalować kazdego pakietu osobno mozemy wykorzystać plik konfiguracyjny requirements.txt zawierający listę pakietów.\n\nPamiętaj - kazdy pakiet powinien zawierać nr wersji pakietu. W innym przypadku moze okazać się, ze nowe werjse pakietów spowodują brak obsługi twojego kodu.\n\nAby utworzyć plik konfiguracyjny uzyj polecenia w terminalu:\npip freeze &gt;&gt; requirements.txt\nTak wygenerowany plik mozesz uzywać na dowolnej maszynie do instalacji i odtworzenia potrzebnego środowiska wykonawczego python.\n\nDygresja. W momencie przygotowywania materiałów Flask był w wersji 3.0.1 - dziś juz realizowany jest w wersji 3.0.3. Zmiany następują szybciej niz się wydaje. Instalacja pakietów z pliku odbywa się z wykorzystaniem polecenia:\n\npip install -r requierements.txt\nMamy teraz dwa pliki: app.py, i requirements.txt. Przenosząc je do dowolnego projektu na serwerach github jesteśmy w stanie uruchomić naszą aplikację wszędzie tam gdzie dostępny będzie interpreter python na którym mozemy utworzyć nowe wirtualne środowisko i zainstalować biblioteki z pliku requirements.txt.\nDo pełnej automatyzacji przydałaby się jeszcze mozliwość uruchomienia środowiska python na dowolnej maszynie.\nW tym celu utwórz plik Dockerfile:\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 5000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\"]\n\nPowyzszy plik pozwala w docker desktop uruchomić obraz wykorzystujący podstawowy system operacyjny (tutaj linux) wraz z podstawowym środowiskiem python3.11.\nPonadto plik ten kopiuje potrzebne pliki (app.py, requirements.txt) na obraz dockera.\nPolecenie RUN pozwala uruchomić dowolne polecenie bash wewnątrz obrazu dockera.\nPolecenie CMD pozwala uruchomić polecenie uruchamiające serwer w trybie tak by nie zamknąć tego polecenia.\nOstatnią informacją jest ustalenie portu na 8000.\nutworzenie kontenera na podstawie pliku Dockerfile\ndocker build -t modelML .\nuruchomienie kontenera\ndocker run -p 8000:8000 modelML",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "dane nieustruktyryzowane",
    "section": "",
    "text": "Dane nieustrukturyzowane to dane, które nie są w żaden sposób uporządkowane, takie jak:\nNiezależnie od typu, wszystko przetwarzamy w tensorach (macierzach wielowymiarowych). To może prowadzić do chęci wykorzystania modeli ML i sieci neuronowych do analizy danych nieustrukturyzowanych.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\n\n# 2-dim picture 28 x 28 pixel\npicture_2d = np.random.uniform(size=(28,28))\npicture_2d[0:5,0:5]\n\narray([[0.51209426, 0.14663302, 0.22121714, 0.05310908, 0.75023721],\n       [0.32680599, 0.1005447 , 0.55604465, 0.09762113, 0.94599531],\n       [0.03287616, 0.60521745, 0.19558811, 0.7921698 , 0.097081  ],\n       [0.98409001, 0.87207277, 0.67522807, 0.71541535, 0.75485835],\n       [0.02429088, 0.06016476, 0.89526054, 0.87395299, 0.8126181 ]])\nplt.imshow(picture_2d, interpolation='nearest')\nplt.show()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#pytorch---pretrenowane-modele-klasyfikujące",
    "href": "labs/lab4.html#pytorch---pretrenowane-modele-klasyfikujące",
    "title": "dane nieustruktyryzowane",
    "section": "PyTorch - pretrenowane modele klasyfikujące",
    "text": "PyTorch - pretrenowane modele klasyfikujące\n\nimport urllib.request\nurl = 'https://pytorch.tips/coffee'\nfpath = 'coffee.jpg'\n# pobierz na dysk\nurllib.request.urlretrieve(url, fpath)\n\n('coffee.jpg', &lt;http.client.HTTPMessage at 0x145c46750&gt;)\n\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image # pillow library\n\n\nimg = Image.open('coffee.jpg')\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimport torch\nfrom torchvision import transforms\n\nOdrobinę zmienimy własności obrazka\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize( \n    mean = [0.485, 0.456, 0.406],\n    std = [0.229, 0.224,0.225])\n])\n\n\nimg_tensor = transform(img)\n\nSprawdzmy rozmiary\n\ntype(img_tensor), img_tensor.shape\n\n(torch.Tensor, torch.Size([3, 224, 224]))\n\n\n\n# utworzenie batch size - dodatkowego wymiaru (na inne obrazki)\nbatch = img_tensor.unsqueeze(0)\nbatch.shape\n\ntorch.Size([1, 3, 224, 224])\n\n\n\nfrom torchvision import models\n\nmodels.list_models()[:5]\n\n['alexnet',\n 'convnext_base',\n 'convnext_large',\n 'convnext_small',\n 'convnext_tiny']",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#model-alexnet",
    "href": "labs/lab4.html#model-alexnet",
    "title": "dane nieustruktyryzowane",
    "section": "Model alexnet",
    "text": "Model alexnet\n\nalexnet = models.alexnet(pretrained=True)\n\n/Users/seba/Documents/GitHub/RTA_2025/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/seba/Documents/GitHub/RTA_2025/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/seba/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:03&lt;00:00, 67.9MB/s] \n\n\n\nalexnet\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\nalexnet.eval()\npredict = alexnet(batch)\n\n\n_, idx = torch.max(predict,1)\n\n\nprint(idx)\n\ntensor([967])\n\n\n\nurl = 'https://pytorch.tips/imagenet-labels'\nfpath = 'imagenet_class_labels.txt'\nurllib.request.urlretrieve(url, fpath)\n\n('imagenet_class_labels.txt', &lt;http.client.HTTPMessage at 0x30aab0490&gt;)\n\n\n\nwith open('imagenet_class_labels.txt') as f:\n    classes = [line.strip() for line in f.readlines()]\n\n\nclasses[0:5]\n\n[\"{0: 'tench, Tinca tinca',\",\n \"1: 'goldfish, Carassius auratus',\",\n \"2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\",\n \"3: 'tiger shark, Galeocerdo cuvieri',\",\n \"4: 'hammerhead, hammerhead shark',\"]\n\n\n\nprob = torch.nn.functional.softmax(predict, dim=1)[0] *100\nprob[:10]\n\ntensor([2.5403e-09, 1.5528e-07, 1.2023e-08, 1.0434e-09, 2.9924e-07, 3.6093e-08,\n        8.3350e-10, 1.4222e-11, 1.0724e-10, 1.2831e-10],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nclasses[idx.item()], prob[idx.item()].item()\n\n(\"967: 'espresso',\", 87.99551391601562)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#model-resnet",
    "href": "labs/lab4.html#model-resnet",
    "title": "dane nieustruktyryzowane",
    "section": "Model resnet",
    "text": "Model resnet\n\nresnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n\nDownloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /Users/seba/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n100%|██████████| 171M/171M [00:02&lt;00:00, 73.7MB/s] \n\n\n\n# resnet\n\n\nresnet.eval()\nout = resnet(batch)\n\n\n_, index = torch.max(out,1)\nprob = torch.nn.functional.softmax(out, dim=1)[0] *100\n\n\nclasses[index.item()], prob[index.item()].item()\n\n(\"967: 'espresso',\", 49.123924255371094)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#własny-model-dla-danych-graficznych",
    "href": "labs/lab4.html#własny-model-dla-danych-graficznych",
    "title": "dane nieustruktyryzowane",
    "section": "własny model dla danych graficznych",
    "text": "własny model dla danych graficznych\nZobaczmy jak sieci neuronowe działają na danych graficznych.\n\n# 60000 obrazow 28x28\n\n# Loading the Fashion-MNIST dataset\nfrom torchvision import datasets, transforms\n# transformacja i normalizacja danych \ntransform = transforms.Compose([transforms.ToTensor(),\n  transforms.Normalize((0.5,), (0.5,))\n])\n\n# Download and load the training data\ntrainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\ntestset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n\n100%|██████████| 26.4M/26.4M [00:00&lt;00:00, 48.0MB/s]\n100%|██████████| 29.5k/29.5k [00:00&lt;00:00, 928kB/s]\n100%|██████████| 4.42M/4.42M [00:00&lt;00:00, 16.5MB/s]\n100%|██████████| 5.15k/5.15k [00:00&lt;00:00, 12.1MB/s]\n\n\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n\nindexes = np.random.randint(0, images.shape[0], size=25)\nimages_rand = images[indexes]\nplt.figure(figsize=(5,5))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    image = images_rand[i]\n    plt.imshow(image[0])\n    plt.axis('off')\n\nplt.show()\nplt.close('all')\n\n\n\n\n\n\n\n\nPrzykładowy model sieci nueronowej (bez konwolucji) - czy sądzisz, że to dobre rozwiązanie?\n\n# Define the network architecture\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nmodel = nn.Sequential(nn.Linear(784, 128),\n                      nn.ReLU(),\n                      nn.Linear(128, 10),\n                      nn.LogSoftmax(dim = 1)\n                     )\n\n# Define the loss\ncriterion = nn.NLLLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr = 0.002)\n\n# Define the epochs\nepochs = 30\n\ntrain_losses, test_losses = [], []\n\nfor e in range(epochs):\n  running_loss = 0\n  for images, labels in trainloader:\n    # Flatten Fashion-MNIST images into a 784 long vector\n    images = images.view(images.shape[0], -1)\n    \n    # Training pass\n    optimizer.zero_grad()\n    \n    output = model.forward(images)\n    loss = criterion(output, labels)\n    loss.backward()\n    optimizer.step()\n    \n    running_loss += loss.item()\n  else:\n    test_loss = 0\n    accuracy = 0\n    \n    # Turn off gradients for validation, saves memory and computation\n    with torch.no_grad():\n      # Set the model to evaluation mode\n      model.eval()\n      \n      # Validation pass\n      for images, labels in testloader:\n        images = images.view(images.shape[0], -1)\n        log_ps = model(images)\n        test_loss += criterion(log_ps, labels)\n        \n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim = 1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy += torch.mean(equals.type(torch.FloatTensor))\n    \n    model.train()\n    train_losses.append(running_loss/len(trainloader))\n    test_losses.append(test_loss/len(testloader))\n    \n    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n\nEpoch: 1/30.. Training loss: 0.491.. Test loss: 0.417.. Test Accuracy: 0.849\nEpoch: 2/30.. Training loss: 0.381.. Test loss: 0.464.. Test Accuracy: 0.832\nEpoch: 3/30.. Training loss: 0.347.. Test loss: 0.400.. Test Accuracy: 0.854\nEpoch: 4/30.. Training loss: 0.321.. Test loss: 0.391.. Test Accuracy: 0.857\nEpoch: 5/30.. Training loss: 0.304.. Test loss: 0.409.. Test Accuracy: 0.856\nEpoch: 6/30.. Training loss: 0.293.. Test loss: 0.393.. Test Accuracy: 0.861\nEpoch: 7/30.. Training loss: 0.279.. Test loss: 0.386.. Test Accuracy: 0.865\nEpoch: 8/30.. Training loss: 0.270.. Test loss: 0.362.. Test Accuracy: 0.879\nEpoch: 9/30.. Training loss: 0.260.. Test loss: 0.388.. Test Accuracy: 0.868\nEpoch: 10/30.. Training loss: 0.253.. Test loss: 0.376.. Test Accuracy: 0.877\nEpoch: 11/30.. Training loss: 0.246.. Test loss: 0.367.. Test Accuracy: 0.879\nEpoch: 12/30.. Training loss: 0.237.. Test loss: 0.368.. Test Accuracy: 0.880\nEpoch: 13/30.. Training loss: 0.235.. Test loss: 0.374.. Test Accuracy: 0.877\nEpoch: 14/30.. Training loss: 0.224.. Test loss: 0.367.. Test Accuracy: 0.877\nEpoch: 15/30.. Training loss: 0.218.. Test loss: 0.420.. Test Accuracy: 0.865\nEpoch: 16/30.. Training loss: 0.215.. Test loss: 0.390.. Test Accuracy: 0.874\nEpoch: 17/30.. Training loss: 0.208.. Test loss: 0.395.. Test Accuracy: 0.876\nEpoch: 18/30.. Training loss: 0.204.. Test loss: 0.392.. Test Accuracy: 0.880\nEpoch: 19/30.. Training loss: 0.196.. Test loss: 0.417.. Test Accuracy: 0.878\nEpoch: 20/30.. Training loss: 0.196.. Test loss: 0.409.. Test Accuracy: 0.878\nEpoch: 21/30.. Training loss: 0.192.. Test loss: 0.390.. Test Accuracy: 0.880\nEpoch: 22/30.. Training loss: 0.188.. Test loss: 0.412.. Test Accuracy: 0.885\nEpoch: 23/30.. Training loss: 0.183.. Test loss: 0.406.. Test Accuracy: 0.883\nEpoch: 24/30.. Training loss: 0.178.. Test loss: 0.402.. Test Accuracy: 0.884\nEpoch: 25/30.. Training loss: 0.175.. Test loss: 0.427.. Test Accuracy: 0.878\nEpoch: 26/30.. Training loss: 0.173.. Test loss: 0.450.. Test Accuracy: 0.882\nEpoch: 27/30.. Training loss: 0.165.. Test loss: 0.444.. Test Accuracy: 0.882\nEpoch: 28/30.. Training loss: 0.165.. Test loss: 0.442.. Test Accuracy: 0.881\nEpoch: 29/30.. Training loss: 0.160.. Test loss: 0.459.. Test Accuracy: 0.876\nEpoch: 30/30.. Training loss: 0.163.. Test loss: 0.449.. Test Accuracy: 0.882\n\n\n\nplt.plot(train_losses, label = \"Training loss\")\nplt.plot(test_losses, label = \"Validation loss\")\nplt.legend(frameon = False)\n\n\n\n\n\n\n\n\n\nprint(\"My model: \\n\\n\", model, \"\\n\")\nprint(\"The state dict keys: \\n\\n\", model.state_dict().keys())\n\nMy model: \n\n Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=10, bias=True)\n  (3): LogSoftmax(dim=1)\n) \n\nThe state dict keys: \n\n odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n\n\n\ntorch.save(model.state_dict(), 'checkpoint.pth')\n\nA jakie inne sieci i warstwy możemy wykorzystać do analizy danych nieustrukturyzowanych?\n\nZnajdź odpowiedź na to pytanie w dokumentacji biblioteki PyTorch.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#dane-tekstowe-i-model-worka-słów",
    "href": "labs/lab4.html#dane-tekstowe-i-model-worka-słów",
    "title": "dane nieustruktyryzowane",
    "section": "Dane tekstowe i model Worka słów",
    "text": "Dane tekstowe i model Worka słów\n\nimport pandas as pd\ndf_train = pd.read_csv(\"train.csv\")\ndf_train = df_train.drop(\"index\", axis=1)\nprint(df_train.head())\nprint(np.bincount(df_train[\"label\"]))\n\n                                                text  label\n0  When we started watching this series on cable,...      1\n1  Steve Biko was a black activist who tried to r...      1\n2  My short comment for this flick is go pick it ...      1\n3  As a serious horror fan, I get that certain ma...      0\n4  Robert Cummings, Laraine Day and Jean Muir sta...      1\n[17452 17548]\n\n\n\n# BoW model  - wektoryzator z sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(lowercase=True, max_features=10_000, stop_words=\"english\")\n\ncv.fit(df_train[\"text\"])\n\nCountVectorizer(max_features=10000, stop_words='english')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(max_features=10000, stop_words='english') \n\n\n\n# słownik i nasze zmienne ..\nprint(list(cv.vocabulary_.keys())[:10])\nprint(list(cv.vocabulary_.values())[:10])\n\n['started', 'watching', 'series', 'cable', 'idea', 'hate', 'character', 'hold', 'beautifully', 'developed']\n[8515, 9725, 7957, 1320, 4488, 4191, 1544, 4339, 892, 2574]\n\n\n\nX_train = cv.transform(df_train[\"text\"])\n\n\n# to dense matrix\nfeat_vec = np.array(X_train[0].todense())[0]\nprint(feat_vec.shape)\nnp.bincount(feat_vec)\n\n(10000,)\n\n\narray([9926,   67,    5,    0,    1,    0,    1])",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#obiektowe-podejście-do-modelowania",
    "href": "labs/lab4.html#obiektowe-podejście-do-modelowania",
    "title": "dane nieustruktyryzowane",
    "section": "Obiektowe podejście do modelowania",
    "text": "Obiektowe podejście do modelowania\n\nimport pandas as pd\nimport numpy as np\n \n# przykład danych ustrukturyzowanych\ndf = pd.read_csv(\"students.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nsex\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\ntarget\n\n\n\n\n0\nfemale\ngroup B\nbachelor's degree\nstandard\nnone\n72\n72\n74\n0\n\n\n1\nfemale\ngroup C\nsome college\nstandard\ncompleted\n69\n90\n88\n1\n\n\n2\nfemale\ngroup B\nmaster's degree\nstandard\nnone\n90\n95\n93\n0\n\n\n3\nmale\ngroup A\nassociate's degree\nfree/reduced\nnone\n47\n57\n44\n1\n\n\n4\nmale\ngroup C\nsome college\nstandard\nnone\n76\n78\n75\n0\n\n\n\n\n\n\n\n\nlen(df), list(df.columns)\n\n(99,\n ['sex',\n  'race/ethnicity',\n  'parental level of education',\n  'lunch',\n  'test preparation course',\n  'math score',\n  'reading score',\n  'writing score',\n  'target'])\n\n\n\nX = df.drop(columns=['target'])\ny = df['target']\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# ZAMIAST OD RAZU PRZETWARZAC !!! najpierw przygotuj kroki - pipeline\n\nnumeric_features = ['math score','reading score','writing score']\ncategorical_features = ['sex','race/ethnicity','parental level of education','lunch','test preparation course']\n\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num_trans\", numeric_transformer, numeric_features),\n    (\"cat_trans\", categorical_transformer, categorical_features)\n])\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    (\"preproc\", preprocessor),\n    (\"model\", LogisticRegression())\n])\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\npipeline\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('model', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('model', LogisticRegression())]) preproc: ColumnTransformer?Documentation for preproc: ColumnTransformerColumnTransformer(transformers=[('num_trans',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['math score', 'reading score',\n                                  'writing score']),\n                                ('cat_trans',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex', 'race/ethnicity',\n                                  'parental level of education', 'lunch',\n                                  'test preparation course'])]) num_trans['math score', 'reading score', 'writing score'] SimpleImputer?Documentation for SimpleImputerSimpleImputer() StandardScaler?Documentation for StandardScalerStandardScaler() cat_trans['sex', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\nPAMIETAJ - obiekt pipeline to obiekt pythonowy i tak jak obiekt modelu można go zapisać do pickla.\n\n\nfrom sklearn.model_selection import train_test_split\nX_tr, X_test, y_tr, y_test = train_test_split(X,y,\ntest_size=0.2, random_state=42)\n\npipeline.fit(X_tr, y_tr)\n\nscore = pipeline.score(X_test, y_test)\nprint(score)\n\n0.45\n\n\n\nimport joblib\njoblib.dump(pipeline, 'your_pipeline.pkl')\n\n['your_pipeline.pkl']\n\n\nTU ZACZYNA SIĘ MAGIA OBIEKTOWEGO PYTHONA - nie pisz kodu i nie uruchamiaj kodów wiele razy dla różnych parametrów - niech Python zrobi to za Ciebie\n\nparam_grid = [\n              {\"preproc__num_trans__imputer__strategy\":\n              [\"mean\",\"median\"],\n               \"model__n_estimators\":[2,5,10,100,500],\n               \"model__min_samples_leaf\": [1, 0.1],\n               \"model\":[RandomForestClassifier()]},\n              {\"preproc__num_trans__imputer__strategy\":\n                [\"mean\",\"median\"],\n               \"model__C\":[0.1,1.0,10.0,100.0,1000],\n                \"model\":[LogisticRegression()]}\n]\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ngrid_search = GridSearchCV(pipeline, param_grid,\ncv=2, verbose=1, n_jobs=-1)\n\n\ngrid_search.fit(X_tr, y_tr)\n\ngrid_search.best_params_\n\nFitting 2 folds for each of 30 candidates, totalling 60 fits\n\n\n{'model': RandomForestClassifier(),\n 'model__min_samples_leaf': 0.1,\n 'model__n_estimators': 500,\n 'preproc__num_trans__imputer__strategy': 'median'}\n\n\n\ngrid_search.score(X_test, y_test), grid_search.score(X_tr, y_tr)\n\n(0.5, 0.7341772151898734)\n\n\nTeraz drobna modyfikacja - wiemy, że takiej zmiennej nie chcemy do modelu - ma tylko jedną wartość. Ale jak zweryfikować jakie to zmienne jeśli masz 3 mln kolumn?\n\ndf['bad_feature'] = 1\n\n\nX = df.drop(columns=['target'])\ny = df['target']\nX_tr, X_test, y_tr, y_test = train_test_split(X,y,\ntest_size=0.2, random_state=42)\n\n\nnumeric_features = ['math score','reading score','writing score', 'bad_feature']\n# znajdz sposób na automatyczny podział dla zmiennych numerycznych i nienumerycznych\n\n\ngrid_search = GridSearchCV(pipeline, param_grid,\ncv=2, verbose=1, n_jobs=-1)\n\ngrid_search.fit(X_tr, y_tr)\n\ngrid_search.best_params_\n\nFitting 2 folds for each of 30 candidates, totalling 60 fits\n\n\n{'model': RandomForestClassifier(),\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 2,\n 'preproc__num_trans__imputer__strategy': 'median'}\n\n\n\ngrid_search.score(X_tr, y_tr), grid_search.score(X_test, y_test)\n\n(0.8227848101265823, 0.7)\n\n\n\nNAPISZ WŁASNĄ KLASĘ KTÓRA ZREALIZUJE TRNSFORMACJE ZA CIEBIE\n\n# your own transformator class\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DelOneValueFeature(BaseEstimator, TransformerMixin):\n    \"\"\"Description\"\"\"\n    def __init__(self):\n        self.one_value_features = []\n        \n    def fit(self, X, y=None):\n        for feature in X.columns:\n            unique = X[feature].unique()\n            if len(unique)==1:\n                self.one_value_features.append(feature)\n        return self\n    def transform(self, X, y=None):\n        if not self.one_value_features:\n            return X\n        return X.drop(axis='columns', columns=self.one_value_features)\n\n\n# UTWÓRZ NOWY PIPELINE\npipeline2 = Pipeline([\n    (\"moja_transformacja\",DelOneValueFeature()),\n    (\"preprocesser\", preprocessor),\n    (\"classifier\", LogisticRegression())])\n    \npipeline2.fit(X_tr, y_tr)\nscore2 = pipeline2.score(X_test, y_test)\nprint(pipeline2)\n\nPipeline(steps=[('moja_transformacja', DelOneValueFeature()),\n                ('preprocesser',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('classifier', LogisticRegression())])\n\n\n🔍 Analiza zawartości laboratorium 4\n\nWprowadzenie do danych nieustrukturyzowanych\n\nLaboratorium rozpoczyna się od przedstawienia danych nieustrukturyzowanych, takich jak obrazy, teksty, dźwięki czy wideo. Podkreślono, że niezależnie od typu danych, wszystko przetwarzane jest w tensorach (macierzach wielowymiarowych), co umożliwia wykorzystanie modeli uczenia maszynowego i sieci neuronowych do ich analizy.\n\nPraca z obrazami\n\nĆwiczenia pokazują, jak za pomocą bibliotek NumPy i Matplotlib generować i wizualizować obrazy dwuwymiarowe. Następnie wprowadzono bibliotekę PyTorch i jej moduł torchvision do przetwarzania obrazów, w tym: • Pobieranie i wczytywanie obrazów z internetu • Transformacje obrazów (zmiana rozmiaru, przycinanie, normalizacja) • Konwersja obrazów do tensorów • Tworzenie batchy danych\n\nWykorzystanie pretrenowanych modeli\n\nLaboratorium demonstruje, jak załadować i wykorzystać pretrenowany model AlexNet z biblioteki torchvision.models do klasyfikacji obrazów. Pokazano również, jak przygotować dane wejściowe i uzyskać predykcje z modelu.\n\nPraca z danymi tekstowymi\n\nW dalszej części laboratorium wprowadzono analizę danych tekstowych za pomocą modelu worka słów (bag-of-words). Pokazano, jak przekształcić teksty na reprezentacje numeryczne, które mogą być wykorzystane w modelach uczenia maszynowego.\n\nObiektowe podejście do modelowania\n\nNa koniec laboratorium przedstawiono obiektowe podejście do tworzenia modeli w PyTorch, co jest istotne przy budowie bardziej złożonych architektur sieci neuronowych.\n⸻\n💡 Propozycje rozszerzeń\nAby jeszcze bardziej wzbogacić laboratorium 4, można rozważyć dodanie następujących elementów:\n\nWykorzystanie innych pretrenowanych modeli\n\nDodanie przykładów z wykorzystaniem innych pretrenowanych modeli, takich jak ResNet czy VGG, pozwoliłoby studentom porównać różne architektury i ich zastosowania.\n\nFinałowy projekt integrujący obrazy i tekst\n\nZaproponowanie projektu, w którym studenci łączą analizę obrazów i tekstów (np. klasyfikacja memów), umożliwiłoby praktyczne zastosowanie zdobytej wiedzy.\n\nWprowadzenie do transfer learningu\n\nPokazanie, jak dostosować pretrenowane modele do nowych zadań poprzez transfer learning, przygotowałoby studentów do pracy z ograniczonymi zbiorami danych.\n\nAnaliza danych dźwiękowych\n\nDodanie sekcji dotyczącej analizy danych dźwiękowych (np. rozpoznawanie mowy) rozszerzyłoby zakres omawianych danych nieustrukturyzowanych.\n\nWykorzystanie bibliotek NLP\n\nWprowadzenie bibliotek takich jak spaCy czy Hugging Face Transformers do analizy tekstu pozwoliłoby na bardziej zaawansowane przetwarzanie języka naturalnego.\n⸻\n✅ Podsumowanie\nLaboratorium 4 stanowi solidne wprowadzenie do analizy danych nieustrukturyzowanych z wykorzystaniem bibliotek NumPy i PyTorch. Dodanie powyższych rozszerzeń mogłoby jeszcze bardziej zwiększyć wartość edukacyjną zajęć, przygotowując studentów do realnych wyzwań w pracy z różnorodnymi danymi w kontekście Big Data.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "",
    "text": "W tym ćwiczeniu nauczysz się, jak stworzyć proste API w Flasku, uruchomić je, wysyłać do niego zapytania oraz wykorzystać model decyzyjny w oparciu o podstawową regułę logiczną.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-podstawowego-api",
    "href": "labs/lab2.html#tworzenie-podstawowego-api",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "1️⃣ Tworzenie podstawowego API",
    "text": "1️⃣ Tworzenie podstawowego API\nNajpierw utworzymy podstawową aplikację Flask.\n\nZapisanie kodu API do pliku\nW Jupyter Notebooku użyj magicznej komendy %%file, aby zapisać kod podstawowej aplikacji flask do pliku app.py: Kod znajdziesz na cw1 Jako tekst do wyświetlenie strony głównej użyj Witaj w moim API!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nWriting app.py\n\n\nTeraz uruchom API w terminalu, wpisując:\npython app.py\nFlask uruchomi serwer lokalnie pod adresem http://127.0.0.1:5000/.\n\n\nSprawdzenie działania API\nW Jupyter Notebooku wykonaj zapytanie GET do strony głównej. Na podstawie pola status_code napisz wyrażenie warunkowe które dla status_code 200 wyświetli zawartość odpowiedzi (z pola content).\n\nimport requests\nresponse = pass # TWOJ KOD\n\nJeśli wszystko działa poprawnie, zobaczysz komunikat Witaj w moim API!.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#dodanie-nowej-podstrony",
    "href": "labs/lab2.html#dodanie-nowej-podstrony",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "2️⃣ Dodanie nowej podstrony",
    "text": "2️⃣ Dodanie nowej podstrony\nDodajmy nową podstronę mojastrona, która zwróci komunikat To jest moja strona!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nPonownie uruchom API i wykonaj zapytanie do strony \"http://127.0.0.1:5000/mojastrona\":\n\nresponse = pass # TWOJ KOD\n\nPowinieneś zobaczyć: To jest moja strona!",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "href": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "3️⃣ Automatyczne uruchamianie serwera z Jupyter Notebook",
    "text": "3️⃣ Automatyczne uruchamianie serwera z Jupyter Notebook\nZamknij wcześniej uruchomiony serwer (Ctrl+C w terminalu) i uruchom go ponownie bezpośrednio z Jupyter Notebook, korzystając z subprocess.Popen:\n\nimport subprocess\n# TWOJ KOD \nserver = pass\n\nPo testach zamknij serwer wykorzystując metodę kill:\n\n# TWOJ KOD",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#obsługa-parametrów-w-adresie-url",
    "href": "labs/lab2.html#obsługa-parametrów-w-adresie-url",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "4️⃣ Obsługa parametrów w adresie URL",
    "text": "4️⃣ Obsługa parametrów w adresie URL\nDodajemy nową podstronę /hello, która będzie przyjmować parametr name.\nEdytuj app.py, dodając odpowiedni kod\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nUruchom serwer i sprawdź działanie API:\nres1 = requests.get(\"http://127.0.0.1:5000/hello\")\nprint(res1.content)  # Powinno zwrócić \"Hello!\"\n\nres2 = requests.get(\"http://127.0.0.1:5000/hello?name=Sebastian\")\nprint(res2.content)  # Powinno zwrócić \"Hello Sebastian!\"",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "href": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "5️⃣ Tworzenie API z prostym modelem ML",
    "text": "5️⃣ Tworzenie API z prostym modelem ML\nStworzymy nową podstronę /api/v1.0/predict, która przyjmuje dwie liczby i zwraca wynik reguły decyzyjnej: - Jeśli suma dwóch liczb jest większa niż 5.8, zwraca 1. - W przeciwnym razie zwraca 0.\nSprawdź działanie API:\nres = requests.get(\"http://127.0.0.1:5000/api/v1.0/predict?num1=3&num2=4\")\nprint(res.json())  # Powinno zwrócić {\"prediction\": 1, \"features\": {\"num1\": 3.0, \"num2\": 4.0}}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#podsumowanie",
    "href": "labs/lab2.html#podsumowanie",
    "title": "Tworzenie API we Flasku – Wprowadzenie",
    "section": "Podsumowanie",
    "text": "Podsumowanie\nPo wykonaniu tego ćwiczenia studenci będą umieli:\n\n✅ Tworzyć podstawowe API w Flasku.\n\n✅ Dodawać podstrony i obsługiwać parametry URL.\n\n✅ Wysyłać zapytania GET i analizować odpowiedzi.\n\n✅ Automatycznie uruchamiać serwer z Jupyter Notebook.\n\n✅ Implementować prosty model decyzyjny w API.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku – Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Dane ustrukturyzowane",
    "section": "",
    "text": "Dane klienta to pewne wartości, które możesz przypisać do zmiennych: np wiek: 42, wzrost: 178, pozyczka: 1000, zarobki: 5000, imię: Jan\nZdefiniuj zmienne customer_1_{cecha} i przypisz im wartości z przykładu powyżej\ncustomer_1_wiek = 42\ncustomer_1_wzrost = 178\ncustomer_1_pozyczka = 1000\ncustomer_1_zarobki = 5000\ncustomer_1_imie = 'Jan'\ncustomer_1 = [42,178,1000,5000,'Jan']\nWeźmy dwie listy numeryczne\na = [1,2,3]\nb = [4,5,6]\n# dodawanie list\nprint(f\"a+b: {a+b}\")\n# można też użyć metody format\nprint(\"a+b: {}\".format(a+b))\n\na+b: [1, 2, 3, 4, 5, 6]\na+b: [1, 2, 3, 4, 5, 6]\n# mnożenie list\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\nno-defined operation\nKażdy obiekt pythonowy można rozszerzyć o nowe metody i atrybuty.\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n[1 2 3] [4 5 6]\nprint(f\"aa+bb: {aa+bb}\")\n# dodawanie działa\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - czy to poprawne mnożenie?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - a czy otrzymany wynik też realizuje poprawne mnożenie?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# mnożenie również działa\n\naa+bb: [5 7 9]\n==================================================\n[ 4 10 18]\naa*bb - czy to poprawne mnożenie?\n32\nnp.dot - a czy otrzymany wynik też realizuje poprawne mnożenie?\nCo działa szybciej?\ndef iloczyn_skalarny_lista(x: list, y: list) -&gt; float: \n    iloczyn = 0.\n    for i in range(len(x)):\n        iloczyn += x[i] * y[i]\n    return iloczyn\n\na = list(range(1000))\nb = list(range(1000))\n\n%timeit iloczyn_skalarny_lista(a, b)\n\n27.9 μs ± 1.22 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nimport numpy as np\ndef iloczyn_skalarny_numpy(x, w):\n    return x.dot(w)\n    \na = np.arange(1000)\nb = np.arange(1000)\n\n%timeit iloczyn_skalarny_numpy(a, b)\n\n545 ns ± 3.29 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n# własności tablic\nx = np.array(range(4))\nprint(x)\nx.shape\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\n[0 1 2 3]\nscalar object dim: 0\nvector object dim: 1\nmatrix object dim: 2\nObliczenia wykonywane na danych mieszczących się w pamięci. &gt; czy można jeszcze przyśpieszyć obliczenia?\nKurs Numpy ze strony Sebastiana Raschki",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#pytorch",
    "href": "labs/lab3.html#pytorch",
    "title": "Dane ustrukturyzowane",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source Python-based deep learning library. PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n\nPyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\nPyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\nPyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n\n\nimport torch\n\nprint(torch.__version__)\n\nprint(torch.cuda.is_available())\nprint(torch.backends.mps.is_available())\n\ntensor0d = torch.tensor(1) \ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n2.6.0\nFalse\nTrue\n\n\n\n# typy\nprint(tensor1d.dtype)\n\nprint(torch.tensor([1.0, 2.0, 3.0]).dtype)\n\ntorch.int64\ntorch.float32\n\n\n\ntensor2d.shape\nprint(tensor2d.reshape(3, 2))\nprint(tensor2d.view(3, 2))\n\ntensor([[1, 2],\n        [2, 3],\n        [4, 5]])\ntensor([[1, 2],\n        [2, 3],\n        [4, 5]])\n\n\n\nprint(tensor2d.T)\n\ntensor([[1, 3],\n        [2, 4],\n        [2, 5]])\n\n\n\nprint(tensor2d.matmul(tensor2d.T))\n\nprint(tensor2d @ tensor2d.T)\n\ntensor([[ 9, 21],\n        [21, 50]])\ntensor([[ 9, 21],\n        [21, 50]])\n\n\nszczegółowe info znajdziesz w dokumentacji",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#modelowanie-danych-ustrukturyzowanych",
    "href": "labs/lab3.html#modelowanie-danych-ustrukturyzowanych",
    "title": "Dane ustrukturyzowane",
    "section": "Modelowanie danych ustrukturyzowanych",
    "text": "Modelowanie danych ustrukturyzowanych\nRozważmy jedną zmienną (xs) od której zależy nasza zmienna wynikowa (ys - target).\nxs = np.array([-1,0,1,2,3,4])\nys = np.array([-3,-1,1,3,5,7])\nModelem który możemy zastosować jest regresja liniowa.\n\n# Regresja liniowa \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nxs = np.array([-1,0,1,2,3,4])\n# a raczej \nxs = xs.reshape(-1, 1)\n\nys = np.array([-3, -1, 1, 3, 5, 7])\n\nreg = LinearRegression()\nmodel = reg.fit(xs,ys)\n\nprint(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n\nmodel.predict(np.array([[1],[5]]))\n\nsolution: x1=2.0, x0=-1.0\n\n\narray([1., 9.])\n\n\nProsty kod realizuje w pełni nasze zadanie znalezienia modelu regresji liniowej.\nDo czego może nam posłużyc tak wygenerowany model?\nAby z niego skorzystac potrzebujemy wyeksportować go do pliku.\nWykorzystaj bibliotekę pickle w celu zapisu obiektu modelu\n\n# save model\nimport pickle\nwith open('model.pkl', \"wb\") as picklefile:\n    pickle.dump(model, picklefile)\n\nTeraz możemy go zaimportować (np na Github) i wykorzystać w innych projektach.\n\n# load model\nwith open('model.pkl',\"rb\") as picklefile:\n    mreg = pickle.load(picklefile)\n\n\nmreg.predict(xs)\n\narray([-3., -1.,  1.,  3.,  5.,  7.])\n\n\n\nfrom torch.nn import Linear\nx_t = torch.tensor([1,2,3,4,5]).view(-1,1)\nx_t=x_t.to(torch.float32)\nm = Linear(1,1)\nm(torch.tensor([1.]))\n\ntensor([-0.5447], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nm(x_t)\n\ntensor([[-0.5447],\n        [-0.8818],\n        [-1.2190],\n        [-1.5562],\n        [-1.8934]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n# forward \nimport torch.nn.functional as F\n\ny = torch.tensor([1.0])\nx1 = torch.tensor([1.1])\nw1 = torch.tensor([2.2], requires_grad=True)\nb  = torch.tensor([0.2], requires_grad=True)\nz = x1 * w1 + b\na = torch.sigmoid(z)\nloss = F.binary_cross_entropy(a,y)\n\n# automatic diff \nfrom torch.autograd import grad\n\ngrad_L_w1 = grad(loss, w1, retain_graph= True)\ngrad_L_b = grad(loss, b, retain_graph= True)\n\nloss.backward()\nprint(w1.grad)\n\ntensor([-0.0746])\n\n\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self, n_input:int, n_output: int):\n        super(LinearRegression, self).__init__()\n\n        self.layers = torch.nn.Sequential(\n                torch.nn.Linear(n_input, n_output)\n        )\n    def forward(self, x):\n        return self.layers(x)\n\n\nx = np.array(xs, dtype=np.float32)\ny = np.array(ys, dtype=np.float32)\n\nX_train = torch.from_numpy(x).view(-1,1)\ny_train = torch.from_numpy(y).view(-1,1)\n\n\nlr_model = LinearRegression(1,1)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)\n\n\nnum_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\nprint(f\"liczba trenowalnych parametrów: {num_params}\")\n\nliczba trenowalnych parametrów: 2\n\n\n\nfor layer in lr_model.layers:\n    if isinstance(layer, torch.nn.Linear):\n        print(f\"weight: {layer.state_dict()['weight']}\")\n        print(f\"bias: {layer.state_dict()['bias']}\")\n\nweight: tensor([[0.0460]])\nbias: tensor([0.4732])\n\n\n\nepochs = 10\n# petla uczaca \nfor epoch in range(epochs):\n    lr_model.train() # etap trenowania \n\n    y_pred = lr_model(X_train)\n    loss = criterion(y_pred, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.2f}')\n \n    lr_model.eval() # etap ewaluacji modelu\n\n# po treningu jeszcze raz generujemy predykcje\nlr_model.eval()\nwith torch.no_grad():\n    predicted = lr_model(X_train)\n\n\nlr_model.layers[0].weight, lr_model.layers[0].weight.shape\n\n(Parameter containing:\n tensor([[1.0423]], requires_grad=True),\n torch.Size([1, 1]))",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#inne-sposoby-pozyskiwania-danych",
    "href": "labs/lab3.html#inne-sposoby-pozyskiwania-danych",
    "title": "Dane ustrukturyzowane",
    "section": "Inne sposoby pozyskiwania danych",
    "text": "Inne sposoby pozyskiwania danych\n\nGotowe źródła w bibliotekach pythonowych\nDane z plików zewnętrznych (np. csv, json, txt) z lokalnego dysku lub z internetu\nDane z bazy danych (np. MySQL, PostgreSQL, MongoDB)\nDane generowane w sposób sztuczny pod wybrany problem modelowy.\nStrumienie danych\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\n# find all keys\nprint(iris.keys())\n\n# print description\nprint(iris.DESCR)\n\n\nimport pandas as pd\nimport numpy as np\n\n# create DataFrame\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n    - class:\n            - Iris-Setosa\n            - Iris-Versicolour\n            - Iris-Virginica\n\n:Summary Statistics:\n\n============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD   Class Correlation\n============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n============== ==== ==== ======= ===== ====================\n\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n:Creator: R.A. Fisher\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n:Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. dropdown:: References\n\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n    Mathematical Statistics\" (John Wiley, NY, 1950).\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n    Structure and Classification Rule for Recognition in Partially Exposed\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n    on Information Theory, May 1972, 431-433.\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n    conceptual clustering system finds 3 classes in the data.\n  - Many, many more ...\n\n\n\n\n# show last\ndf.tail(10)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n140\n6.7\n3.1\n5.6\n2.4\n2.0\n\n\n141\n6.9\n3.1\n5.1\n2.3\n2.0\n\n\n142\n5.8\n2.7\n5.1\n1.9\n2.0\n\n\n143\n6.8\n3.2\n5.9\n2.3\n2.0\n\n\n144\n6.7\n3.3\n5.7\n2.5\n2.0\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2.0\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2.0\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2.0\n\n\n\n\n\n\n\n\n# show info about NaN values and a type of each column.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\n 4   target             150 non-null    float64\ndtypes: float64(5)\nmemory usage: 6.0 KB\n\n\n\n# statistics\ndf.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n1.000000\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n0.819232\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n0.000000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n0.000000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n1.000000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n2.000000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n2.000000\n\n\n\n\n\n\n\n\n# new features\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\n# remove features (columns) \ndf = df.drop(columns=['target'])\n# filtering first 100 rows and 4'th column\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\niris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\nf, ax = plt.subplots(1, figsize=(15,9))\nsns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)\n\n\n\n\n\n\n\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\n\ny = np.where(y == 'setosa',-1,1)\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Perceptron\n\nper_clf = Perceptron()\nper_clf.fit(X,y)\n\ny_pred = per_clf.predict([[2, 0.5],[4,5.5]])\ny_pred\n\narray([-1,  1])\n\n\nźródła zewnętrzne\n\nIRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndf = pd.read_csv(IRIS_PATH, names=col_names)\n\n\n# save to sqlite\nimport sqlite3\n# generate database\nconn = sqlite3.connect(\"iris.db\")\n# pandas to_sql\n\ntry:\n    df.to_sql(\"iris\", conn, index=False)\nexcept:\n    print(\"tabela już istnieje\")\n\ntabela już istnieje\n\n\n\nresult = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length &gt; 5\", conn)\n\nSztuczne dane\n\n# Dane sztucznie generowane\nfrom sklearn import datasets\nX, y = datasets.make_classification(n_samples=10**4,\nn_features=20, n_informative=2, n_redundant=2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# podział na zbiór treningowy i testowy\ntrain_samples = 7000 # 70% danych treningowych\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\nrfc.predict(X_train[0].reshape(1, -1))\n\narray([0])\n\n\nAnaliza Laboratorium 3 - Chatgbt\n\nPraca z danymi ustrukturyzowanymi:\n\nĆwiczenia rozpoczynają się od reprezentacji danych klienta za pomocą zmiennych, list i struktur NumPy. Dzięki temu studenci uczą się, dlaczego listy nie są optymalne do przechowywania danych oraz jakie korzyści niesie ze sobą użycie tablic NumPy, takie jak efektywność obliczeniowa i możliwość wykonywania operacji wektorowych.\n\nWprowadzenie do PyTorch:\n\nLaboratorium wprowadza podstawy biblioteki PyTorch, prezentując jej możliwości w zakresie obliczeń tensorowych oraz automatycznego różniczkowania. To przygotowuje studentów do bardziej zaawansowanych zastosowań, takich jak budowa i trenowanie modeli uczenia maszynowego. ￼\nPropozycje rozszerzeń\nPropozycje do pracy dla studentów jako rozszerzenia:\n\nIntegracja z Pandas: Wprowadzenie biblioteki Pandas do pracy z danymi tabelarycznymi pozwoliłoby studentom lepiej zrozumieć manipulację danymi i ich przygotowanie do analizy.\n\n\nPandas DataFrames został omówiony jako podobieństwo obiektów pythona do tabel SQL.\n\n\nWizualizacja danych: Dodanie ćwiczeń z wykorzystaniem bibliotek takich jak Matplotlib czy Seaborn umożliwiłoby studentom lepsze zrozumienie danych poprzez ich graficzną reprezentację.\n\n\nCzęściowe przykłady pokazują jak wykorzystać analizę danych za pomocą bibliotek takich jak Matplotlib czy Seaborn, oraz jak interpretować otrzymywane wyniki.\n\n\nPraktyczne zastosowania: Zaproponowanie mini-projektu, w którym studenci analizują rzeczywiste dane (np. dane finansowe czy dane z mediów społecznościowych) przy użyciu poznanych narzędzi, zwiększyłoby zaangażowanie i pokazało praktyczne zastosowanie zdobytej wiedzy.\n\n\nProjekt analizy danych tabelarycznych realizowany jest jako jedno z zadań domowych.\n\n\nPorównanie z innymi bibliotekami: Przedstawienie różnic między NumPy, PyTorch i TensorFlow w kontekście analizy danych mogłoby pomóc studentom w wyborze odpowiednich narzędzi do konkretnych zadań.\n\nPodsumowanie\nLaboratorium 3 stanowi solidne wprowadzenie do pracy z danymi ustrukturyzowanymi i wykorzystania bibliotek NumPy oraz PyTorch. Dodanie powyższych rozszerzeń mogłoby jeszcze bardziej zwiększyć wartość edukacyjną zajęć, przygotowując studentów do realnych wyzwań w analizie danych w czasie rzeczywistym.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "",
    "text": "Na poprzednich zajęciach omawialiśmy wykorzystanie modelu regresji liniowej dla danych ustrukturyzowanych. W najprostszym przypadku dla jednej zmiennej X i jednej zmiennej celu moglibyśmy np. przypisać model w postaci:\nsatysfakcja_z_zycia = \\(\\alpha_0\\) + \\(\\alpha_1\\) PKB_per_capita\n\\(\\alpha_0\\) nazywamy punktem odcięcia (intercept) albo punktem obciążenia (bias)\nimport numpy as np\n\nnp.random.seed(42) \nm = 100\nX = 2*np.random.rand(m,1) \na_0, a_1 = 4, 3 \ny = a_0 + a_1 * X + np.random.randn(m,1)\nimport matplotlib.pyplot as plt\n\nplt.scatter(X, y)\nplt.show()\nW ogólności model liniowy: \\(\\hat{y} = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_n x_n\\) gdzie \\(\\hat{y}\\) to predykcja naszego modelu (wartość prognozowana), dla \\(n\\) cech przy wartościach cechy \\(x_i\\).\nW postaci zwektoryzowanej możemy napisać: \\(\\hat{y} = \\vec{\\alpha}^{T} \\vec{x}\\)\nW tej postaci widać dlaczego w tym modelu dokłada się kolumnę jedynek - wynikają one z wartości \\(x_0\\) dla \\(\\alpha_0\\).\n# dodajmy jedynkę do naszej tabeli \nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)\nPowiedzieliśmy, że możemy w tym modelu znaleźć funkcję kosztu\n\\(MSE(\\vec{x}, \\hat{y}) = \\sum_{i=1}^{m} \\left( \\vec{\\alpha}^{T} \\vec{x}^{(i)} - y^{(i)} \\right)^{2}\\)\nTak naprawdę możemy \\(MSE(\\vec{x}, \\hat{y}) = MSE(\\vec{\\alpha})\\)\nRozwiązanie analityczne: \\(\\vec{\\alpha} = (X^{T}X)^{-1} X^T y\\)\n# rozwiązanie analityczne \nalpha_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\nalpha_best, np.array([4,3])\nX_new = np.array([[0],[2]])\nX_new_b = add_dummy_feature(X_new)\ny_predict = X_new_b @ alpha_best\n\nimport matplotlib.pyplot as plt\n\nplt.plot(X_new, y_predict, \"r-\", label=\"prediction\")\nplt.plot(X,y, \"b.\")\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X,y) \n\nprint(f\"a_0={lin_reg.intercept_[0]}, a_1 = {lin_reg.coef_[0][0]}\")\n\nprint(\"predykcja\", lin_reg.predict(X_new))\n# Logistic Regression w scikit learn oparta jest o metodę lstsq \nalpha_best_svd, _, _, _ = np.linalg.lstsq(X_b, y, rcond=1e-6)\nalpha_best_svd"
  },
  {
    "objectID": "labs/lab5.html#gradient-prosty",
    "href": "labs/lab5.html#gradient-prosty",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Gradient prosty",
    "text": "Gradient prosty\nPamiętaj o standaryzacji zmiennych (aby były one reprezentowane w tej samej skali).\n\nWsadowy gradient prosty\nW celu implementacji musimy policzyć pochodne cząstkowe dla funkcji kosztu wobec każdego parametru \\(\\alpha_i\\).\n\\(\\frac{\\partial}{\\partial \\alpha_j}MSE(\\vec{x}, \\hat{y}) = 2 \\sum_{i=1}^{m} \\left( \\vec{\\alpha}^{T} \\vec{x}^{(i)} - y^{(i)} \\right) x_j^{(i)}\\)\nKomputery posiadają własność mnożenia macierzy co pozwala obliczyć nam wszystkie pochodne w jednym obliczeniu. Wzór i algorytm liczący wszystkie pochodne “na raz” wykorzystuje cały zbiór X dlatego też nazywamy go wsadowym.\nPo obliczeniu gradientu po prostu idziemy “w przeciwną stronę”\n$ {next} = - {} MSE()$\n\nImage(filename='./img/02_10.png', width=500) \n\n\neta = 0.1\nn_epochs = 1000\nm = len(X_b)\nnp.random.seed(42) \nalpha = np.random.randn(2,1) # losowo wybieramy rozwiązanie\n\nfor epoch in range(n_epochs):\n    gradients = 2/m* X_b.T @ (X_b @ alpha - y)\n    #print(alpha)\n    alpha = alpha - eta*gradients\n\n\nalpha\n\nsprawdz jak wygladają wyniki dla różnych eta dla 0.02, 0.1, 0.5"
  },
  {
    "objectID": "labs/lab5.html#stochastic-gradient-descent",
    "href": "labs/lab5.html#stochastic-gradient-descent",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nJednym z poważniejszych problemów wsadowego gradientu jest jego zależność od wykorzystania (w każdym kroku) całej macierzy danych. Korzystając z własności statystycznych możemy zobaczyć jak będzie realizowała się zbieżność rozwiązania jeśli za każdym razem wylosujemy próbkę danych i na niej określimy gradient. Ze względu, iż w pamięci przechowujemy tylko pewną porcję danych algorytm ten może być używany dla bardzo dużych zbiorów danych. Warto jednak mieć świadomość, że tak otrzymane wyniki mają charakter chaotyczny, co oznacza, że funkcja kosztu nie zbiega się w kierunku minimum lecz przeskakuje dążąc do minimun w sensie średniej.\n\nn_epochs = 50\nm = len(X_b)\n\n\ndef learning_schedule(t, t0=5, t1=50):\n    return t0/(t+t1)\n\nnp.random.seed(42)\nalpha = np.random.randn(2,1)\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1] \n        gradients = 2 * xi.T @ (xi @ alpha - yi)\n        eta = learning_schedule(epoch * m + iteration) \n        alpha = alpha - eta * gradients\n        \n\n\nalpha\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, \n                       penalty=None, eta0=0.01, \n                       n_iter_no_change=100, random_state=42)\n\nsgd_reg.fit(X, y.ravel())\n\n\nsgd_reg.intercept_, sgd_reg.coef_"
  },
  {
    "objectID": "labs/lab5.html#perceptron-i-oop",
    "href": "labs/lab5.html#perceptron-i-oop",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Perceptron i OOP",
    "text": "Perceptron i OOP\n\nfrom random import randint\nrandint(1,6)\n\n\nfrom random import randint\n\nclass Kosc():\n    \"\"\"opis\"\"\"\n    def __init__(self):\n        \"\"\"\n        To jest metoda uruchamiana podczas inicjalizacji obiektu\n\n        params:\n        sciany (int) \n        \"\"\"\n        # zdefiniuj zmienną sciany i przypisz do niej domyślną wartość 6\n        \n        \n    def roll(self):\n        \"\"\"opis metody\n        metoda realizująca rzut kością - zwraca liczbę losową w zakresie 1 do liczby scian\n        \"\"\"\n        return ...\n\n\na = Kosc()\n\n# rzuć 10 razy kością i wyniki zapisz do listy\n\n\n\nfrom random import choice\nchoice([0,1,2,3,4])\n\n\nfrom random import choice\n\nclass RandomWalk():\n    def __init__(self, num_points=5000):\n        self.num_points = num_points\n        self.x_values = [0]\n        self.y_values = [0]\n    \n    def fill_walk(self):\n        while len(self.x_values) &lt; self.num_points:\n            # ruch prawo-lewo \n            # wylosuj kierunek dodatni lub ujemy oraz odległość 0-5 i przypisz do zmiennych\n            \n            x_direction = ...\n            x_distance = ...\n            x_step = x_direction*x_distance\n            \n            y_direction = ...\n            y_distance = ...\n            y_step = y_direction*y_distance\n            \n            # napisz warunek pomijający krok gdy x i y step = 0 (użyj continue)\n            \n            next_x = self.x_values[-1] + x_step\n            next_y = self.y_values[-1] + y_step\n            \n            self.x_values.append(next_x)\n            self.y_values.append(next_y)\n\n\nrw = RandomWalk()\nrw.fill_walk()\n\nrw.x_values\n\n\n# wygeneruj błądzenie losowe dla 10_000 punktów\nrw2 = ...\nrw2....\n\n\n\nimport matplotlib.pyplot as  plt\npoint_number = list(range(rw.num_points))\nplt.scatter(rw.x_values, rw.y_values, c=point_number, cmap=plt.cm.Blues,\n           edgecolor='none', s=15)\nplt.scatter(0,0,c='green', edgecolor='none', s=100)\nplt.scatter(rw.x_values[-1], rw.y_values[-1],c='red', edgecolor='none', s=100)\n\nplt.show()\n\n\nSztuczne neurony - rys historyczny\nW 1943 roku W. McCulloch i W. Pitts zaprezentowali pierwszą koncepcję uproszczonego modelu komórki nerwowej tzw. Nuronu McCulloch-Pittsa (MCP). W.S. McCulloch, W. Pitts, A logical Calculus of the Ideas Immanent in Nervous Activity. “The Bulletin of Mathematical Biophysics” 1943 nr 5(4)\nNeuronami nazywamy wzajemnie połączone komórki nerwowe w mózgu, które są odpowiedzialne za przetwarzanie i przesyłanie sygnałów chemicznych i elektrycznych. Komórka taka opisana jest jako bramka logiczna zawierająca binarne wyjścia. Do dendrytów dociera duża liczba sygnałów, które są integrowane w ciele komórki i (jeżeli energia przekracza określoną wartość progową) zostaje wygenerowany sygnał wyjściowy przepuszczany przez akson.\n\nImage(filename='./img/02_01.png', width=800) \n\nPo kilku latach Frank Rosenblatt (na podstawie MCP) zaproponował pierwszą koncepcję reguły uczenia perceprtonu. F. Rosenblatt, The Perceptron, a Perceiving and Recognizing Automaton, Cornell Aeronautical Laboratory, 1957\n\n Image(filename='./img/02_04.png', width=800) \n\n\nZADANIE\n\n\nimport numpy as np\nexample_input = [1,.2,.1,.05,.2]\nexample_weights = [.2,.12,.4,.6,.90]\n\n\nprzekształć listę example_input na tablicę numpy\n\n\ninput_vector = ...\n\n\nprzekształć listę example_weights na tablicę numpy\n\n\nweights = ...\n\n\nbias_wegiht = .2\n\n\noblicz Net input\n\n\nactivation_level = ...\n\n\nprzyjmij próg aktywacji (threshold) na poziomie 0.5 i napisz kod przypisujący przewidywaną klasę do zmiennej perceptron_output\n\n\n# twoj kod \n\n\nprint(perceptron_output)\n\n\nOtrzymałeś predykcję 1. Prawdziwa klasa realizowała 0. Napisz kod modyfikujący wagi. bieżącą wartość wag zmodyfikuj o różnicę oczekiwanej warości i obliczonej predykcji, przemnożoną o wartość zmiennej w danym przebiegu.\n\n\nImage(filename='./img/02_02.png', width=800) \n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\ny = np.where(y == 0, -1, 1)\n\nimport matplotlib.pyplot as plt\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\ndziecko = Perceptron()\ndziecko.fit()\n\n# dziecko musi mieć parametr uczenia\ndziecko.eta\n\n# możemy sprawdzić jak szybko się uczy == ile błędów robi\n\ndziecko.errors_ \n\n# rozwiązania znajdą się w wagach\ndziecko.w_\n# w naszym przypadku dziecko uczy się dwóch wag !\n\n\nclass Perceptron():\n    def __init__(self, n_iter=10, eta=0.01):\n        self.n_iter = n_iter\n        self.eta = eta\n        \n    def fit(self, X, y):\n        self.w_ = np.zeros(1+X.shape[1])\n        self.errors_ = []\n        for _ in range(self.n_iter):\n            pass\n        return self\n\n\nimport random\n\nclass Perceptron():\n    \n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n    \n    def fit(self, X, y):\n        \n        #self.w_ = np.zeros(1+X.shape[1])\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] \n        self.errors_ = []\n        \n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X,y):\n                #print(xi, target)\n                update = self.eta*(target-self.predict(xi))\n                #print(update)\n                self.w_[1:] += update*xi\n                self.w_[0] += update\n                #print(self.w_)\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:])+self.w_[0]\n    \n    def predict(self, X):\n        return np.where(self.net_input(X)&gt;=0.0, 1, -1)\n\n\n# uzycie jak wszsytkie klasy sklearn\nppn = Perceptron()\nppn.fit(X,y)\n\n\nprint(ppn.errors_)\nprint(ppn.w_)\n\n\nppn.predict(np.array([-3, 5]))\n\n\n# dodatkowa funkcja\n\nfrom matplotlib.colors import ListedColormap\n\ndef plot_decision_regions(X,y,classifier, resolution=0.02):\n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max()+1\n    x2_min, x2_max = X[:,1].min() -1, X[:,1].max()+1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(),xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl,0], y=X[y==cl,1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)\n\n# dla kwiatków\n\n\nplot_decision_regions(X,y,classifier=ppn)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n\nImage(filename='./img/02_09.png', width=600) \n\n\n# ZADANIE - Opisz czym różni się poniższy algorytm od Perceprtona ? \nclass Adaline():\n    '''Klasyfikator  - ADAptacyjny LIniowy NEuron'''\n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X,y):\n        #self.w_ = np.zeros(1+X.shape[1])\n        import random\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])]\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(X)\n            errors = (y-output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        return self.net_input(X)\n\n    def predict(self, X):\n        return np.where(self.activation(X) &gt;= 0.0, 1, -1) \n\n\nad = Adaline(n_iter=20, eta=0.01)\n\nad.fit(X,y)\n\nprint(ad.w_)\n\nplot_decision_regions(X,y,classifier=ad)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n\nad.cost_[:10]\n\n\nad2 = Adaline(n_iter=100, eta=0.0001)\n\nad2.fit(X,y)\n\nplot_decision_regions(X,y,classifier=ad2)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n\nprint(ad2.w_)\n\nad2.cost_[-10:]\n\n\n%%file app.py\n\nimport pickle\nimport numpy as np\nfrom flask import Flask, request, jsonify\n\nclass Perceptron():\n    \n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n    \n    def fit(self, X, y):\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] \n        self.errors_ = []\n        \n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X,y):\n                update = self.eta*(target-self.predict(xi))\n                self.w_[1:] += update*xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:])+self.w_[0]\n    \n    def predict(self, X):\n        return np.where(self.net_input(X)&gt;=0.0, 1, -1)\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/predict_get', methods=['GET'])\ndef get_prediction():\n    # sepal length\n    sepal_length = float(request.args.get('sl'))\n    petal_length = float(request.args.get('pl'))\n    \n    features = [sepal_length, petal_length]\n\n    # Load pickled model file\n    with open('model.pkl',\"rb\") as picklefile:\n        model = pickle.load(picklefile)\n        \n    # Predict the class using the model\n    predicted_class = int(model.predict(features))\n    \n    # Return a json object containing the features and prediction\n    return jsonify(features=features, predicted_class=predicted_class)\n\n@app.route('/predict_post', methods=['POST'])\ndef post_predict():\n    data = request.get_json(force=True)\n    # sepal length\n    sepal_length = float(data.get('sl'))\n    petal_length = float(data.get('pl'))\n    \n    features = [sepal_length, petal_length]\n\n    # Load pickled model file\n    with open('model.pkl',\"rb\") as picklefile:\n        model = pickle.load(picklefile)\n        \n    # Predict the class using the model\n    predicted_class = int(model.predict(features))\n    output = dict(features=features, predicted_class=predicted_class)\n    # Return a json object containing the features and prediction\n    return jsonify(output)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n\n\nimport requests\nresponse = requests.get(\"http://127.0.0.1:5000/predict_get?sl=6.3&pl=2.6\")\nprint(response.content)\n\n\nimport requests\njson = {\"sl\":2.4, \"pl\":2.6}\nresponse = requests.post(\"http://127.0.0.1:5000/predict_post\", json=json)\nprint(response.content)"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: Szkoła Główna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJęzyk prowadzenia: polski\nPoziom przedmiotu: średnio-zaawansowany\nProwadzący: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspółczesny biznes opiera się na podejmowaniu decyzji opartych na danych. Coraz większa ilość informacji, rosnące wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiają, że analiza danych w czasie rzeczywistym staje się kluczowym elementem nowoczesnych procesów biznesowych.\nNa zajęciach studenci zapoznają się z metodami i technologiami umożliwiającymi przetwarzanie danych w czasie rzeczywistym. Szczególną uwagę poświęcimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz głębokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretować zjawiska biznesowe, ale także podejmować szybkie i trafne decyzje.\nW ramach kursu omówimy zarówno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dźwięk, strumieniowanie wideo). Studenci poznają architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a także wyzwania związane z modelowaniem danych w czasie rzeczywistym na dużą skalę.\nKurs obejmuje część teoretyczną oraz praktyczne laboratoria, podczas których studenci będą pracować z rzeczywistymi danymi w środowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. Dzięki temu studenci nie tylko zdobędą wiedzę na temat metod analitycznych, ale także nauczą się korzystać z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plików płaskich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (Wykład)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametrów modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykładzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego środowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego środowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 1.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 2.\nPrzygotowanie środowiska Microsoft Azure. Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzędzia IT do szybkiej analizy logów.\nNarzędzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-kształcenia",
    "href": "sylabus.html#efekty-kształcenia",
    "title": "Sylabus",
    "section": "Efekty kształcenia",
    "text": "Efekty kształcenia\n\nWiedza:\n\n\nZna historię i filozofię modeli przetwarzania danych\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiązania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nUmie wybrać strukturę IT dla danego problemu biznesowego\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejętności:\n\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiązania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować, przetwarzać oraz zachowywać dane generowane w czasie rzeczywistym\n\nPowiązania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\nPowiązania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosować i skonstruować system do przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem\n\nPowiązania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiązania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1️⃣ Zając S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2️⃣ Frątczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3️⃣ Bellemare A., Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę, O’Reilly 2021.\n4️⃣ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, O’Reilly 2022.\n5️⃣ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. Rozwiązania typowych problemów dotyczących przygotowania danych, konstruowania modeli i MLOps, O’Reilly 2021.\n6️⃣ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, O’Reilly 2022.\n7️⃣ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly 2018.\n8️⃣ Sebastián Ramírez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostępna online).\n9️⃣ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\n🔟 Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, O’Reilly 2019."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów.",
    "crumbs": [
      "222890-S",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów.",
    "crumbs": [
      "222890-S",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli VI bud G\n\n18-02-2025 (wtorek) 13:30-15:10 - Wykład 1\n25-02-2025 (wtorek) 13:30-15:10 - Wykład 2\n04-03-2025 (wtorek) 13:30-15:10 - Wykład 3 online\n11-03-2025 (wtorek) 13:30-15:10 - Wykład 4\n18-03-2025 (wtorek) 13:30-15:10 - Wykład 5\n\nWykład 5 kończy się TESTEM: 20 pytań - 30 minut. Test przeprowadzany jest za pośrednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć).\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp.",
    "crumbs": [
      "222890-S",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page.",
    "crumbs": [
      "222890-S",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html",
    "href": "lectures/wyklad2.html",
    "title": "Wykład 2",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, jak dane ewoluowały w różnych branżach i jakie narzędzia są dziś wykorzystywane do ich analizy.\nNa tym wykładzie przedstawimy ewolucję analizy danych, pokazując, jak zmieniały się technologie i podejścia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aż po nowoczesne podejście do strumieniowego przetwarzania danych.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "href": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "title": "Wykład 2",
    "section": "Dane tabelaryczne (tabele SQL)",
    "text": "Dane tabelaryczne (tabele SQL)\nPoczątkowo dane były przechowywane w postaci tabel, gdzie każda tabela zawierała zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).\nModele takie doskonale nadawały się do danych ustrukturyzowanych.\n\n📌 Cechy:\n✅ Dane podzielone na kolumny o stałej strukturze.\n✅ Możliwość stosowania operacji CRUD (Create, Read, Update, Delete).\n✅ Ścisłe reguły spójności i normalizacji.\n\n\n📌 Przykłady:\n➡️ Systemy bankowe, e-commerce, ERP, systemy CRM.\n\n\n🖥️ Przykładowy kod w Pythonie (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-grafowe",
    "href": "lectures/wyklad2.html#dane-grafowe",
    "title": "Wykład 2",
    "section": "Dane grafowe",
    "text": "Dane grafowe\nWraz z rozwojem potrzeb biznesowych pojawiły się dane grafowe, w których relacje między obiektami są reprezentowane jako wierzchołki i krawędzie.\n\n📌 Cechy:\n✅ Dane opisujące relacje i powiązania.\n✅ Elastyczna struktura (grafy zamiast tabel).\n✅ Możliwość analizy połączeń (np. algorytmy PageRank, centralność).\n\n\n📌 Przykłady:\n➡️ Sieci społecznościowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).\n\n\n🖥️ Przykładowy kod w Pythonie (Graf Karate - NetworkX):\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-półstrukturyzowane-json-xml-yaml",
    "href": "lectures/wyklad2.html#dane-półstrukturyzowane-json-xml-yaml",
    "title": "Wykład 2",
    "section": "Dane półstrukturyzowane (JSON, XML, YAML)",
    "text": "Dane półstrukturyzowane (JSON, XML, YAML)\nDane te nie są w pełni ustrukturyzowane jak w bazach SQL, ale mają pewien schemat.\n\n📌 Cechy:\n✅ Hierarchiczna struktura (np. klucz-wartość, obiekty zagnieżdżone).\n✅ Brak ścisłego schematu (możliwość dodawania nowych pól).\n✅ Popularność w systemach NoSQL i API.\n\n\n📌 Przykłady:\n➡️ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logów.\n\n\n🖥️ Przykładowy kod w Pythonie (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "href": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "title": "Wykład 2",
    "section": "Dane tekstowe (NLP)",
    "text": "Dane tekstowe (NLP)\nTekst stał się kluczowym źródłem informacji, szczególnie w analizie opinii, chatbotach czy wyszukiwarkach.\n\n📌 Cechy:\n✅ Nieustrukturyzowane dane wymagające przekształcenia.\n✅ Stosowanie embeddingów (np. Word2Vec, BERT, GPT).\n✅ Duże zastosowanie w analizie sentymentu i chatbotach.\n\n\n📌 Przykłady:\n➡️ Media społecznościowe, e-maile, chatboty, tłumaczenie maszynowe.\n\n\n🖥️ Przykładowy kod w Pythonie:\n\n\nCode\nimport ollama\n\n# Przykładowe zdanie\nsentence = \"Sztuczna inteligencja zmienia świat.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-1.6779385805130005, 3.0364203453063965, -6.6012187004089355, -1.7487436532974243]",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-multimedialne-obrazy-dźwięk-wideo",
    "href": "lectures/wyklad2.html#dane-multimedialne-obrazy-dźwięk-wideo",
    "title": "Wykład 2",
    "section": "Dane multimedialne (obrazy, dźwięk, wideo)",
    "text": "Dane multimedialne (obrazy, dźwięk, wideo)\nNowoczesne systemy analizy danych wykorzystują również obrazy i dźwięk.\n\n📌 Cechy:\n✅ Wymagają dużej mocy obliczeniowej (sztuczna inteligencja, deep learning).\n✅ Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dźwięk).\n\n\n📌 Przykłady:\n➡️ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treści wideo.\n\n\n🖥️ Przykładowy kod w Pythonie (Obraz - OpenCV):\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-strumieniowe",
    "href": "lectures/wyklad2.html#dane-strumieniowe",
    "title": "Wykład 2",
    "section": "Dane strumieniowe",
    "text": "Dane strumieniowe\nObecnie najbardziej dynamicznie rozwija się analiza danych strumieniowych, gdzie dane są analizowane na bieżąco, w miarę ich napływania.\n\n📌 Cechy:\n✅ Przetwarzanie w czasie rzeczywistym.\n✅ Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.\n\n\n📌 Przykłady:\n➡️ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.\n\n\n🖥️ Przykładowy kod w Pythonie (Strumieniowe transakcje bankowe):\n\n\nCode\nimport time\ntransactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]\nfor transaction in transactions:\n    print(f\"Processing transaction: {transaction}\")\n    time.sleep(1)\n\n\nProcessing transaction: {'id': 1, 'amount': 100}\nProcessing transaction: {'id': 2, 'amount': 200}",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "href": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "title": "Wykład 2",
    "section": "Dane sensoryczne i IoT",
    "text": "Dane sensoryczne i IoT\nDane z czujników i urządzeń IoT są kolejnym krokiem w ewolucji.\n\n📌 Cechy:\n✅ Często pochodzą z miliardów urządzeń (big data).\n✅ Wymagają analizy brzegowej (edge computing).\n\n\n📌 Przykłady:\n➡️ Smart home, wearables, samochody autonomiczne, systemy przemysłowe.\n\n\n🖥️ Przykładowy kod w Pythonie (Sensor - temperatura):\n\n\nCode\nimport random\ndef get_temperature():\n    return round(random.uniform(20.0, 25.0), 2)\nprint(f\"Current temperature: {get_temperature()}°C\")\n\n\nCurrent temperature: 23.75°C",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeń-na-big-data",
    "href": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeń-na-big-data",
    "title": "Wykład 2",
    "section": "Hadoop Map-Reduce – Skalowanie obliczeń na Big Data",
    "text": "Hadoop Map-Reduce – Skalowanie obliczeń na Big Data\nKiedy mówimy o skalowalnym przetwarzaniu danych, pierwszym skojarzeniem może być Google.\nAle co tak naprawdę sprawia, że możemy wyszukiwać informacje w ułamku sekundy, przetwarzając petabajty danych?\n👉 Czy wiesz, że nazwa “Google” pochodzi od słowa “Googol”, czyli liczby równej 10¹⁰⁰?\nTo więcej niż liczba atomów w znanym Wszechświecie! 🌌\n\n🔥 Wyzwanie: Czy uda Ci się zapisać liczbę Googol do końca zajęć?",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczają",
    "href": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczają",
    "title": "Wykład 2",
    "section": "🔍 Dlaczego SQL i klasyczne algorytmy nie wystarczają?",
    "text": "🔍 Dlaczego SQL i klasyczne algorytmy nie wystarczają?\nTradycyjne bazy danych SQL czy jednowątkowe algorytmy zawodzą, gdy skala danych przekracza pojedynczy komputer.\nW tym miejscu pojawia się MapReduce – rewolucyjny model obliczeniowy stworzony przez Google.\n\n🛠️ Rozwiązania Google dla Big Data:\n✅ Google File System (GFS) – rozproszony system plików.\n✅ Bigtable – system do przechowywania ogromnych ilości ustrukturyzowanych danych.\n✅ MapReduce – algorytm podziału pracy na wiele maszyn.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "href": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "title": "Wykład 2",
    "section": "Graficzne przedstawienie MapReduce",
    "text": "Graficzne przedstawienie MapReduce\n\nMapowanie rozdziela zadania (Map)\nKażde wejście dzielone jest na mniejsze części i przetwarzane równolegle.\n🌍 Wyobraź sobie, że masz książkę telefoniczną i chcesz znaleźć wszystkie osoby o nazwisku “Nowak”.\n➡️ Podziel książkę na fragmenty i daj każdemu do przeanalizowania jeden fragment.\n\n\nRedukcja zbiera wyniki (Reduce)\nWszystkie częściowe wyniki są łączone w jedną, końcową odpowiedź.\n🔄 Wszyscy uczniowie zgłaszają swoje wyniki, a jeden student zbiera i podsumowuje odpowiedź.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#klasyczny-przykład-liczenie-słów-w-tekście",
    "href": "lectures/wyklad2.html#klasyczny-przykład-liczenie-słów-w-tekście",
    "title": "Wykład 2",
    "section": "💡 Klasyczny przykład: Liczenie słów w tekście",
    "text": "💡 Klasyczny przykład: Liczenie słów w tekście\nZałóżmy, że mamy miliony książek i chcemy policzyć, ile razy występuje każde słowo.\n\n🖥️ Kod MapReduce w Pythonie (z użyciem multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Funkcja Map (podział tekstu na słowa)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Funkcja Reduce (sumowanie wyników)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\n🔹 Co tu się dzieje?\n✅ Każdy fragment tekstu jest przetwarzany niezależnie (map).\n✅ Wyniki są zbierane i sumowane (reduce).\n✅ Efekt: Możemy przetwarzać terabajty tekstu równolegle!",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wizualizacja-porównanie-klasycznego-podejścia-i-mapreduce",
    "href": "lectures/wyklad2.html#wizualizacja-porównanie-klasycznego-podejścia-i-mapreduce",
    "title": "Wykład 2",
    "section": "🎨 Wizualizacja – Porównanie klasycznego podejścia i MapReduce",
    "text": "🎨 Wizualizacja – Porównanie klasycznego podejścia i MapReduce\n📊 Stare podejście – Jeden komputer wykonuje wszystko sekwencyjnie.\n📊 Nowe podejście (MapReduce) – Każda maszyna liczy fragment i wyniki są agregowane.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "href": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "title": "Wykład 2",
    "section": "🚀 Wyzwanie dla Ciebie!",
    "text": "🚀 Wyzwanie dla Ciebie!\n🔹 Znajdź i uruchom swój własny algorytm MapReduce w dowolnym języku!\n🔹 Czy potrafisz zaimplementować własny MapReduce do innego zadania? (np. analiza logów, zliczanie kliknięć na stronie)",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#big-data",
    "href": "lectures/wyklad2.html#big-data",
    "title": "Wykład 2",
    "section": "Big Data",
    "text": "Big Data\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "href": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "title": "Wykład 2",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 2"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "Wykład 1",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h\n🎯 Cel wykładu\nZapoznanie studentów z podstawami real-time analytics, różnicami między trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez konieczności przechowywania i oczekiwania na późniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wniosków i reakcji na zmieniające się warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) – dane są analizowane w ciągu milisekund lub sekund od ich wygenerowania.\nStreaming vs. Batch Processing – analiza danych może odbywać się w sposób ciągły (streaming) lub w z góry określonych interwałach (batch).\nIntegracja z IoT, AI i ML – real-time analytics często współpracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym – np. natychmiastowa detekcja oszustw w transakcjach bankowych.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "Wykład 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowość\n\nWykrywanie oszustw – analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujących na oszustwa.\nAutomatyczny trading – systemy HFT (High-Frequency Trading) analizują miliony danych w ułamkach sekundy.\nDynamiczne oceny kredytowe – natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym – dynamiczne rekomendacje produktów na podstawie aktualnego zachowania użytkownika.\nDynamiczne ceny – np. Uber, Amazon i hotele stosują dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediów społecznościowych – analiza nastrojów klientów i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej – analiza logów w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystąpieniem.\nSmart Cities – analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji świetlnej.\nAnalityka IoT – urządzenia IoT generują strumienie danych, które można analizować w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentów – analiza sygnałów z urządzeń medycznych w celu natychmiastowego wykrycia zagrożenia życia.\nAnalityka epidemiologiczna – śledzenie rozprzestrzeniania się chorób na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemów informatycznych, który umożliwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branżach – od finansów, przez e-commerce, aż po ochronę zdrowia i IoT.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "lectures/wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "Wykład 1",
    "section": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstnieją trzy główne podejścia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKażde z nich różni się szybkością przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing – Przetwarzanie wsadowe\n📌 Definicja:\nBatch Processing polega na zbieraniu dużych ilości danych i ich przetwarzaniu w określonych odstępach czasu (np. co godzinę, codziennie, co tydzień).\n📌 Cechy:\n\n✅ Wysoka wydajność dla dużych zbiorów danych\n✅ Przetwarzanie danych po ich zgromadzeniu\n✅ Nie wymaga natychmiastowej analizy\n✅ Zwykle tańsze niż przetwarzanie w czasie rzeczywistym\n❌ Opóźnienia – wyniki są dostępne dopiero po zakończeniu przetwarzania\n\n📌 Przykłady zastosowań:\n\nGenerowanie raportów finansowych na koniec dnia/miesiąca\nAnaliza trendów sprzedaży na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\n📌 Przykładowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiąca\n\n# Agregacja danych - miesięczne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wyników do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyś chciał utworzyć dane do przykładu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics – Analiza niemal w czasie rzeczywistym\n📌 Definicja:\nNear Real-Time Analytics to analiza danych, która odbywa się z minimalnym opóźnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie pełna analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duże opóźnienia mogą wpłynąć na biznes.\n📌 Cechy:\n\n✅ Przetwarzanie danych w krótkich odstępach czasu (kilka sekund – minut)\n✅ Umożliwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\n✅ Optymalny balans między kosztami a szybkością\n❌ Nie nadaje się do systemów wymagających natychmiastowej reakcji\n\n📌 Przykłady zastosowań:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciągu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowań użytkowników\nAnaliza logów serwerów i sieci w celu wykrycia anomalii\n\n📌 Przykładowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logów IT)\nAmazon Kinesis\n\nPrzykład producenta danych realizującego tranzakcje wysyłane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generująca przykładowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota między 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# Zakończenie działania producenta\nproducer.flush()\nproducer.close()\nPrzykład consumenta - programu sparawdzającego zbyt duże transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\nPrzykładowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics – Analiza w czasie rzeczywistym\n📌 Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w ułamku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagających reakcji w czasie rzeczywistym, np. w transakcjach giełdowych, systemach IoT czy cyberbezpieczeństwie.\n📌 Cechy:\n\n✅ Bardzo niskie opóźnienie (milliseconds-seconds)\n✅ Umożliwia natychmiastową reakcję systemu\n✅ Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\n❌ Droższe i bardziej złożone technologicznie niż batch processing\n\n📌 Przykłady zastosowań:\n\nHigh-Frequency Trading (HFT) – analiza i podejmowanie decyzji w transakcjach giełdowych w milisekundach\nAutonomiczne samochody – analiza strumieni danych z kamer i sensorów w czasie rzeczywistym\nCyberbezpieczeństwo – detekcja ataków w sieciach komputerowych w ułamku sekundy\nAnalityka IoT – np. natychmiastowa detekcja anomalii w danych z czujników przemysłowych\n\n📌 Przykładowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\n🔎 Porównanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpóźnienie\nMinuty – godziny – dni\nSekundy – minuty\nMilisekundy – sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w pełni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\n📉 Niski\n📈 Średni\n📈📈 Wysoki\n\n\nZłożoność implementacji\n📉 Prosta\n📈 Średnia\n📈📈 Trudna\n\n\nPrzykłady zastosowań\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\n📌 Kiedy stosować Batch Processing?\n\n✅ Gdy nie wymagasz natychmiastowej analizy\n✅ Gdy masz duże ilości danych, ale przetwarzane są one okresowo\n✅ Gdy chcesz obniżyć koszty\n\n📌 Kiedy stosować Near Real-Time Analytics?\n\n✅ Gdy wymagasz analizy w krótkim czasie (sekundy – minuty)\n✅ Gdy potrzebujesz bardziej aktualnych danych, ale nie w pełnym real-time\n✅ Gdy szukasz kompromisu między wydajnością a kosztami\n\n📌 Kiedy stosować Real-Time Analytics?\n\n✅ Gdy każda milisekunda ma znaczenie (np. giełda, autonomiczne pojazdy)\n✅ Gdy chcesz wykrywać oszustwa, anomalie lub incydenty natychmiast\n✅ Gdy system musi natychmiast reagować na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne – w wielu przypadkach near real-time jest wystarczające i bardziej opłacalne. Kluczowe jest zrozumienie wymagań biznesowych przed wyborem odpowiedniego rozwiązania.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "href": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "title": "Wykład 1",
    "section": "Dlaczego Real-Time Analytics jest ważne?",
    "text": "Dlaczego Real-Time Analytics jest ważne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje się coraz bardziej istotna w wielu branżach, ponieważ umożliwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodów, dla których real-time analytics jest ważne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagować na zmiany i wydarzenia w czasie rzeczywistym. Dzięki temu można podejmować decyzje szybciej, co jest kluczowe w dynamicznych środowiskach, takich jak:\n\nMarketing: Reklamy mogą być dostosowane do zachowań użytkowników w czasie rzeczywistym (np. personalizacja treści reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie każda minuta może oznaczać różnicę w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogą monitorować kluczowe wskaźniki operacyjne na bieżąco. Przykłady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urządzeń w fabrykach, aby natychmiast wykrywać awarie i zapobiegać przestojom.\nHealthtech: Śledzenie parametrów życiowych pacjentów i wykrywanie anomalii, co może ratować życie.\n\n\n\nZwiększenie efektywności operacyjnej\nReal-time analytics umożliwia natychmiastowe wykrywanie i eliminowanie problemów operacyjnych, zanim staną się poważniejsze. Przykłady:\n\nLogistyka: Śledzenie przesyłek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywność i zmniejsza opóźnienia.\nRetail: Monitorowanie poziomu zapasów na bieżąco i dostosowywanie zamówień do aktualnych potrzeb.\n\n\n\nKonkurencyjność\nOrganizacje, które wykorzystują analitykę w czasie rzeczywistym, mają przewagę nad konkurencją, ponieważ mogą szybciej reagować na zmiany na rynku, nowe potrzeby klientów i sytuacje kryzysowe. Dzięki natychmiastowym informacjom:\n\nMożna podejmować decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywać lepsze relacje z klientami, reagując na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doświadczenia użytkowników (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z użytkownikami w trakcie ich trwania. Przykłady:\n\nE-commerce: Analiza koszyka zakupowego użytkownika w czasie rzeczywistym, aby np. zaoferować rabat lub przypomnieć o porzuconych produktach.\nStreaming: Optymalizacja jakości usługi wideo/streamingowej w zależności od dostępnej przepustowości łącza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym świecie pełnym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeństwa. Przykłady:\n\nCyberbezpieczeństwo: Real-time analytics umożliwia wykrywanie podejrzanych działań w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztów\nDzięki analizie w czasie rzeczywistym można optymalizować zasoby i zmniejszać koszty. Na przykład:\n\nZarządzanie energią: Analiza zużycia energii w czasie rzeczywistym, umożliwiająca optymalizację wydatków na energię w firmach.\nOptymalizacja łańcucha dostaw: Dzięki bieżącemu śledzeniu zapasów i dostaw można lepiej zarządzać kosztami magazynowania i transportu.\n\n\n\nZdolność do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, które mogą przewidywać przyszłe zachowania lub problemy, a także je eliminować zanim się pojawią. Na przykład:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w połączeniu z modelami predykcyjnymi pozwala przewidywać awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym można dostosowywać produkcję lub zapasy na podstawie bieżących trendów.\n\nReal-time analytics to nie tylko analiza danych – to kluczowy element strategii firm w świecie, który wymaga szybkich reakcji, elastyczności i dostosowywania się do zmieniającego się otoczenia. Firmy, które wdrażają te technologie, mogą znacząco poprawić swoje wyniki finansowe, obsługę klienta, wydajność operacyjną, a także przewagę konkurencyjną.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiąże się z wieloma wyzwaniami i trudnościami, które trzeba rozwiązać, aby systemy real-time działały efektywnie i niezawodnie. Pomimo ogromnego potencjału, jaki daje możliwość natychmiastowego przetwarzania danych, realizacja tych procesów w praktyce wiąże się z licznymi problemami technologicznymi, organizacyjnymi i dotyczącymi zarządzania danymi.\nPoniżej przedstawiamy najważniejsze wyzwania oraz możliwe rozwiązania, które należy uwzględnić podczas implementacji systemów analizy danych w czasie rzeczywistym.\n\nSkalowalność systemów\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadań. W miarę jak ilość generowanych danych rośnie, systemy muszą być w stanie obsługiwać większe obciążenie bez opóźnienia w przetwarzaniu.\nZwiększona ilość danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, ilość generowanych danych może być olbrzymia. Potrzebna jest elastyczność: System musi automatycznie dostosowywać zasoby w zależności od obciążenia.\n\n\nRozwiązanie:\nWykorzystanie skalowalnych systemów chmurowych, które pozwalają na dynamiczne zwiększanie zasobów obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarządzania kontenerami i automatycznego skalowania mikroserwisów. Technologie strumieniowe (Apache Kafka, Apache Flink) umożliwiające przetwarzanie danych w sposób wydajny i rozproszony.\n\n\n\nOpóźnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, każde opóźnienie w przetwarzaniu danych może mieć poważne konsekwencje. Dotyczy to zwłaszcza obszarów takich jak:\nWykrywanie oszustw: W przypadku systemów płatności online, opóźnienie w analizie transakcji może oznaczać przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentów: Opóźnienia mogą wpłynąć na skuteczność reakcji w sytuacjach kryzysowych.\n\n\nRozwiązanie:\nUżywanie algorytmów optymalizujących czas przetwarzania, np. stream processing z wykorzystaniem systemów takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliżej źródła (np. urządzenia IoT), aby zmniejszyć opóźnienia w transmisji danych do chmury.\n\n\n\nJakość danych i zarządzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizować dane w czasie rzeczywistym, ale także zapewnić ich wysoką jakość. W przeciwnym razie analizy mogą prowadzić do błędnych wniosków lub opóźnień w reagowaniu na nieprawidłowe dane.\nZanieczyszczone dane: W systemach real-time dane często są niepełne, brudne, błędne lub nieuporządkowane. Zmiana charakterystyki danych: Dane mogą zmieniać się w czasie, co może utrudniać ich przetwarzanie i analizę. #### Rozwiązanie:\nData cleansing i data validation na wstępnym etapie procesu. Automatyczne systemy monitorowania jakości danych w celu wykrywania błędów w czasie rzeczywistym. Zarządzanie danymi w strumieniu: Narzędzia takie jak Apache Kafka pozwalają na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZłożoność integracji systemów\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym często muszą współpracować z istniejącymi systemami IT i źródłami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemów, zwłaszcza w rozproszonej architekturze, może być skomplikowana.\n\n\nRozwiązanie:\nUżywanie API do łatwiejszej integracji z zewnętrznymi systemami. Mikroserwisy i konteneryzacja z pomocą narzędzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, które umożliwia łatwą integrację różnych źródeł danych oraz zapewnia elastyczność w dostosowywaniu systemów do rosnących potrzeb.\n\n\n\nBezpieczeństwo i prywatność\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiąże się z ogromną ilością wrażliwych informacji, szczególnie w branżach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, że dane są odpowiednio chronione przed nieautoryzowanym dostępem, jest kluczowe.\nOchrona danych w czasie transmisji: Muszą być szyfrowane zarówno podczas przesyłania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym może być celem ataków, takich jak DDoS czy SQL injection.\n\n\nRozwiązanie:\nSzyfrowanie danych zarówno w spoczynku, jak i podczas przesyłania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeństwa. Zgodność z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarządzanie błędami i awariami\n\nWyzwanie:\nBłędy i awarie w systemach real-time mogą prowadzić do poważnych konsekwencji, w tym utraty danych, opóźnień w analizach czy nawet usunięcia usług. W systemach rozproszonych trudno jest osiągnąć pełną niezawodność.\n\n\nRozwiązanie:\nRedundancja: Tworzenie kopii zapasowych systemów i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), które pozwalają na szybkie wykrycie i naprawienie problemów. Zarządzanie stanem: Dzięki użyciu narzędzi jak Apache Kafka, można ponownie przetwarzać dane, jeśli wystąpił błąd w transmisji.\n\n\n\nKoszty związane z infrastrukturą\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, która zapewni odpowiednią moc obliczeniową i pamięć. To może wiązać się z dużymi kosztami, szczególnie gdy dane muszą być przechowywane i przetwarzane w czasie rzeczywistym na dużą skalę.\n\n\nRozwiązanie:\nChmura obliczeniowa: Możliwość elastycznego skalowania zasobów w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalają na uruchamianie procesów bez potrzeby utrzymywania stałej infrastruktury.\nChociaż analiza danych w czasie rzeczywistym oferuje ogromne korzyści, wiąże się także z wieloma wyzwaniami. Właściwa architektura, narzędzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogą pomóc w przezwyciężeniu wielu z tych trudności. Warto również pamiętać o konieczności zapewnienia wysokiej jakości danych, ich bezpieczeństwa, a także elastyczności i skalowalności systemów, które będą w stanie sprostać rosnącym wymaganiom.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 1"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html",
    "href": "lectures/wyklad5.html",
    "title": "Wykład 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiązanie problemu biznesowego, warto zastanowić się nad złożonością Twojego problemu.\n\n\n1. Algorytmy przetwarzające duże ilości danych\nPrzetwarzanie ogromnych zbiorów danych wymaga odpowiedniego podejścia do ich organizacji i analizy. W sytuacji, gdy ilość danych przekracza dostępną pamięć jednostki obliczeniowej, często stosuje się iteracyjne sposoby ich przetwarzania.\n🔹 Przykład: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o użytkownikach, ich historii zakupów i oglądanych treści.\nPrzetwarza dane w sposób iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji użytkownika.\n\n🔹 Inne zastosowania:\n\nAnaliza logów serwerowych w czasie rzeczywistym (np. wykrywanie ataków DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorów w inteligentnym mieście).\n\n2. Algorytmy dokonujące wielu obliczeń\nWymagają dużej mocy obliczeniowej, ale zazwyczaj nie operują na wielkich zbiorach danych. Przykładem może być algorytm wyszukujący dużą liczbę pierwszą. Często wykorzystuje się tutaj podział obliczeń na równoległe procesy w celu optymalizacji wydajności.\n🔹 Przykład: Kryptografia i znalezienie dużej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duże liczby pierwsze, które są podstawą dla szyfrowania RSA.\nProces wymaga intensywnych obliczeń, ale nie operuje na ogromnych zbiorach danych.\nCzęsto wykorzystywane są metody równoległe, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszości.\n\n🔹 Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrótszej trasy w problemie komiwojażera).\n\n3. Algorytmy przetwarzające duże ilości danych i dokonujące wielu obliczeń\nŁączą wymagania obu poprzednich typów, potrzebując zarówno dużych zasobów obliczeniowych, jak i obsługi dużych zbiorów danych. Przykładem może być analiza sentymentu w transmisjach wideo na żywo.\n🔹 Przykład: Analiza sentymentu w transmisjach wideo na żywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarówno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarówno dużych zasobów obliczeniowych (przetwarzanie NLP i CV), jak i obsługi dużej ilości danych.\nMoże wykorzystywać modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dźwięku.\n\n🔹 Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby określić wymiar danych problemu, nie wystarczy podać jedynie ilości miejsca zajmowanego przez dane. Istotne są trzy główne aspekty:\n\nRozmiar wejścia – oczekiwany rozmiar danych do przetwarzania.\nSzybkość narastania – tempo generowania nowych danych podczas działania algorytmu.\nRóżnorodność struktury – typy danych, jakie algorytm musi obsłużyć.\n\n\n\n\nDotyczy zasobów procesowania i mocy obliczeniowej. Na przykład algorytmy uczenia głębokiego (DL) wymagają dużej mocy obliczeniowej, dlatego warto zapewnić zrównolegloną architekturę, wykorzystującą GPU lub TPU, co znacząco przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#algorytmy",
    "href": "lectures/wyklad5.html#algorytmy",
    "title": "Wykład 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiązanie problemu biznesowego, warto zastanowić się nad złożonością Twojego problemu.\n\n\n1. Algorytmy przetwarzające duże ilości danych\nPrzetwarzanie ogromnych zbiorów danych wymaga odpowiedniego podejścia do ich organizacji i analizy. W sytuacji, gdy ilość danych przekracza dostępną pamięć jednostki obliczeniowej, często stosuje się iteracyjne sposoby ich przetwarzania.\n🔹 Przykład: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o użytkownikach, ich historii zakupów i oglądanych treści.\nPrzetwarza dane w sposób iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji użytkownika.\n\n🔹 Inne zastosowania:\n\nAnaliza logów serwerowych w czasie rzeczywistym (np. wykrywanie ataków DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorów w inteligentnym mieście).\n\n2. Algorytmy dokonujące wielu obliczeń\nWymagają dużej mocy obliczeniowej, ale zazwyczaj nie operują na wielkich zbiorach danych. Przykładem może być algorytm wyszukujący dużą liczbę pierwszą. Często wykorzystuje się tutaj podział obliczeń na równoległe procesy w celu optymalizacji wydajności.\n🔹 Przykład: Kryptografia i znalezienie dużej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duże liczby pierwsze, które są podstawą dla szyfrowania RSA.\nProces wymaga intensywnych obliczeń, ale nie operuje na ogromnych zbiorach danych.\nCzęsto wykorzystywane są metody równoległe, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszości.\n\n🔹 Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrótszej trasy w problemie komiwojażera).\n\n3. Algorytmy przetwarzające duże ilości danych i dokonujące wielu obliczeń\nŁączą wymagania obu poprzednich typów, potrzebując zarówno dużych zasobów obliczeniowych, jak i obsługi dużych zbiorów danych. Przykładem może być analiza sentymentu w transmisjach wideo na żywo.\n🔹 Przykład: Analiza sentymentu w transmisjach wideo na żywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarówno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarówno dużych zasobów obliczeniowych (przetwarzanie NLP i CV), jak i obsługi dużej ilości danych.\nMoże wykorzystywać modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dźwięku.\n\n🔹 Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby określić wymiar danych problemu, nie wystarczy podać jedynie ilości miejsca zajmowanego przez dane. Istotne są trzy główne aspekty:\n\nRozmiar wejścia – oczekiwany rozmiar danych do przetwarzania.\nSzybkość narastania – tempo generowania nowych danych podczas działania algorytmu.\nRóżnorodność struktury – typy danych, jakie algorytm musi obsłużyć.\n\n\n\n\nDotyczy zasobów procesowania i mocy obliczeniowej. Na przykład algorytmy uczenia głębokiego (DL) wymagają dużej mocy obliczeniowej, dlatego warto zapewnić zrównolegloną architekturę, wykorzystującą GPU lub TPU, co znacząco przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#wyjaśnialność-algorytmów",
    "href": "lectures/wyklad5.html#wyjaśnialność-algorytmów",
    "title": "Wykład 5",
    "section": "Wyjaśnialność algorytmów",
    "text": "Wyjaśnialność algorytmów\nW wielu przypadkach modelowanie jest wykorzystywane w sytuacjach krytycznych, np. w oprogramowaniu do podawania leków. W takich sytuacjach kluczowe staje się wyjaśnienie przyczyny każdego wyniku działania algorytmu. Jest to konieczne, aby zapewnić, że decyzje podejmowane na jego podstawie są wolne od błędów i uprzedzeń.\nZdolność algorytmu do wskazania mechanizmów generujących wyniki nazywamy możliwością wyjaśnienia. Analiza etyczna stanowi standardowy element procesu walidacji algorytmu.\nUzyskanie wysokiej wyjaśnialności jest szczególnie trudne w przypadku algorytmów uczenia maszynowego (ML) i głębokiego uczenia (DL). Na przykład banki korzystające z algorytmów do podejmowania decyzji kredytowych muszą zapewnić transparentność i wskazać powody wydanej decyzji.\nJedną z metod poprawy wyjaśnialności algorytmów jest LIME (Local Interpretable Model-Agnostic Explanations), opublikowana w 2016 roku. Metoda ta polega na wprowadzaniu niewielkich zmian w danych wejściowych i analizowaniu ich wpływu na wynik, co pozwala określić lokalne zasady podejmowania decyzji przez model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podział na zbiór treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykładu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wybór losowego przykładu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wyświetlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nJak działa ten kod? 1. Ładowanie danych i trenowanie modelu\n\nUżywamy zbioru Iris, który zawiera 150 przykładów kwiatów z trzema gatunkami:\nSetosa\nVersicolor\nVirginica\nModel RandomForestClassifier trenuje się na tych danych.\n\n\nTworzenie interpretowalnego modelu za pomocą LIME\n\n\nLIME generuje lokalne wyjaśnienia, czyli interpretuje model dla pojedynczych predykcji.\nWybieramy losowy przykład z danych testowych.\n\n\nEksploracja wyniku dla jednego przykładu\n\n\nLIME modyfikuje lekko wartości wejściowe i obserwuje, jak zmienia się wynik predykcji.\nTworzy „lokalny” model liniowy, który pokazuje, które cechy miały największy wpływ na decyzję.\n\nZałóżmy, że nasz model wybrał przykładową roślinę i sklasyfikował ją jako Virginica.\nOto interpretacja wyników:\n\nNajważniejsze cechy wpływające na decyzję modelu:\n\n\nDługość płatka (petal length): największy wpływ na predykcję (np. im większa, tym większe prawdopodobieństwo, że to Virginica).\nSzerokość płatka (petal width): również istotny czynnik (np. powyżej pewnej wartości sugeruje Virginica).\nDługość kielicha (sepal length): mniejszy wpływ, ale nadal istotny.\nSzerokość kielicha (sepal width): zwykle najmniej istotna cecha.\n\n\nWizualizacja wyników\n\n\nLIME generuje wykres słupkowy, który pokazuje wpływ każdej cechy na klasyfikację.\nNa wykresie widać, które cechy zwiększały, a które zmniejszały prawdopodobieństwo przypisania do danej klasy.\n\n\nCo oznacza wynik?\n\n\nJeśli model przewidział klasę Virginica z wysokim prawdopodobieństwem, oznacza to, że kluczowe cechy (np. długi płatek) mocno wskazują na ten gatunek.\nJeśli cechy miały zróżnicowany wpływ, oznacza to, że model miał pewne trudności w klasyfikacji (np. szerokość płatka nie była jednoznaczna).",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#detekcja-anomalii",
    "href": "lectures/wyklad5.html#detekcja-anomalii",
    "title": "Wykład 5",
    "section": "Detekcja anomalii",
    "text": "Detekcja anomalii\n\nWartość odstająca (Outlier)\nWartość odstająca (ang. outlier) to obserwacja (wiersz w tabeli danych), która jest znacznie oddalona od pozostałych elementów próbki. Oznacza to, że zależność między zmiennymi niezależnymi i zależnymi dla tej obserwacji może różnić się od pozostałych przypadków.\nDla pojedynczych zmiennych wartości odstające można określić, korzystając z wykresu pudełkowego (box plot). Wykres ten bazuje na kwartylach:\n\nPierwszy kwartyl \\(Q_1\\) i trzeci kwartyl \\(Q_3\\) wyznaczają boki pudełka,\nDrugi kwartyl \\(Q_2\\) (mediana) jest zaznaczony wewnątrz pudełka,\n\nWartości odstające spełniają zależność:\n\\[\nx_{out} &lt; Q_1 - 1.5 \\times IQR \\quad \\text{lub} \\quad x_{out} &gt; Q_3 + 1.5 \\times IQR\n\\]\nGdzie: \\[\nIQR = Q_3 - Q_1\n\\]\nPrzykładem wartości odstającej może być bolid Formuły 1 – pod względem prędkości jest on anomalią wśród zwykłych samochodów.\n\n\nWykorzystanie detekcji anomalii\nWykrywanie wartości odstających ma szerokie zastosowanie, np.:\n\nFinanse – wykrywanie transakcji fraudowych w analizie danych bankowych,\nCyberbezpieczeństwo – identyfikacja intruzów w sieci na podstawie zachowań użytkowników,\nMedycyna – monitorowanie parametrów zdrowotnych i wykrywanie nieprawidłowości,\nPrzemysł – wykrywanie wadliwych komponentów poprzez analizę obrazu.\n\n\n\nMetody wykrywania anomalii\n\n1. Metody nadzorowane (supervised learning)\nStosowane, gdy mamy oznaczone dane (np. przypadki oszustw w transakcjach).\n\nSieci neuronowe,\nAlgorytm K-najbliższych sąsiadów (KNN),\nSieci Bayesowskie.\n\n\n\n2. Metody nienadzorowane (unsupervised learning)\nZakładają, że większość danych jest poprawna, a anomalie to niewielki odsetek przypadków.\n\nKlasteryzacja metodą K-średnich (K-Means),\nAutoenkodery w sieciach neuronowych,\nTesty statystyczne.\n\n\n\n\nMetoda klasyczna – detekcja na podstawie prawdopodobieństwa\nAby określić, czy dana obserwacja jest anomalią, można użyć prawdopodobieństwa \\(p(x)\\):\n\nJeśli \\(p(x) &lt; \\epsilon\\), uznajemy wartość za odstającą.\nW praktyce zakładamy, że dane mają rozkład normalny \\(N(\\mu, \\sigma)\\).\nSzacujemy parametry \\(\\mu\\) (średnia) i \\(\\sigma^2\\) (wariancja) na podstawie próbki.\nNastępnie dla każdej wartości obliczamy prawdopodobieństwo jej wystąpienia i porównujemy z \\(\\epsilon\\).\n\nPrzykład: Analiza wynagrodzeń w firmie\nWykrywamy, czy w danej firmie są osoby o wynagrodzeniach znacznie odbiegających od średniej.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przykładowe wynagrodzenia w firmie (w tysiącach)\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\n# Obliczenie kwartylów\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\n# Definicja wartości odstających\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\n# Znalezienie outlierów\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\n# Wizualizacja\nsns.boxplot(salaries)\nplt.title(\"Wykres pudełkowy wynagrodzeń\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nWynik: Na wykresie pudełkowym widać, że 150 tys. to anomalia.\n\n\nIsolation Forest – detekcja anomalii za pomocą lasu izolacyjnego\nIsolation Forest to algorytm bazujący na drzewach decyzyjnych, zaproponowany przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku. Identyfikuje anomalie poprzez izolowanie wartości odstających w procesie podziału danych:\n\nWybiera losowo cechę oraz wartość podziału,\nWartości odstające szybciej zostają odizolowane (są bliżej korzenia drzewa),\nWynik jest agregowany na podstawie wielu drzew.\n\nJego zalety to niskie wymagania obliczeniowe i skuteczność w analizie wielowymiarowych danych.\nMetody detekcji anomalii sklearn\nPrzykład: Wykrywanie oszustw bankowych\nBank analizuje transakcje kartą kredytową i wykrywa te, które mogą być nieautoryzowane.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Przykładowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duża kwota, rzadkość)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nWynik: Transakcja 5000 zł zostanie wykryta jako anomalia.",
    "crumbs": [
      "222890-S",
      "Wykłady",
      "Wykład 5"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html",
    "href": "kafka_codes/kafka3.html",
    "title": "Consumer w środowisku Python",
    "section": "",
    "text": "Przygotuj środowisko i uruchom skrypt producenta.\nRozpatrzmy kod konsumenta czytającego z topicu oraz realizującego prostą regułę decyzyjną.\n\n%%file konsument.py\nfrom kafka import KafkaConsumer\nimport json  \n\nSERVER = \"broker:9092\"\nTOPIC  = \"streaming\"\n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    TOPIC,\n    bootstrap_servers=SERVER,\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"values\"] &gt; 80:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\n\nKod ten uruchom poleceniem:\npython konsument.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Consumer w środowisku Python"
    ]
  }
]