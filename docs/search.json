[
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Lecture 1: Introduction to Real-Time Data Analysis\nLecture 2: Data Ingestion and Processing for Real-Time Analysis\nLecture 3: Real-Time Data Analysis Techniques\nLecture 4: Real-Time Data Visualization and Communication\nLecture 5: Case Studies and Implementation\nThroughout the lectures, I’ll incorporate interactive elements, such as:\nThis comprehensive lecture plan will provide students with a solid foundation in real-time data analysis, covering the technical aspects of data ingestion, processing, and visualization, as well as practical applications and best practices."
  },
  {
    "objectID": "plan.html#nowy-program-przedmiotu",
    "href": "plan.html#nowy-program-przedmiotu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Nowy program przedmiotu",
    "text": "Nowy program przedmiotu\n\nBatch vs. Real-Time vs. Streaming Analytics – Różnice między trybami przetwarzania danych, kluczowe koncepcje i zastosowania.\nModele przetwarzania danych w Big Data – Od plików płaskich do Data Lake, wady i zalety podejścia real-time. Mity i fakty o przetwarzaniu w czasie rzeczywistym.\nArchitektura IT dla przetwarzania w czasie rzeczywistym – Omówienie architektur Lambda i Kappa w kontekście strumieniowego przetwarzania danych.\nSystemy przetwarzania danych w czasie rzeczywistym – Przegląd technologii: Apache Kafka, Apache Spark Streaming, Apache Flink i ich zastosowania w Pythonie.\nPodstawy uczenia maszynowego w czasie rzeczywistym – Porównanie offline learning vs. online learning, problemy związane z przyrostowym uczeniem maszynowym.\n\n🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej?\n2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce.\n3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym.\n4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów.\n5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych."
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\n\n22-02-2025 (sobota) 08:00-09:30 - Wykład 1\n08-03-2025 (sobota) 08:00-09:30 - Wykład 2\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć) 20 pytań.\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "old_lectures/old_wyklad1.html",
    "href": "old_lectures/old_wyklad1.html",
    "title": "Od plików płaskich do Data Mash",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych i biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu klastrów komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop,Apache Kafka , Apache Spark, Apache Flink i ich chmurowe odpowiedniki, używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\n\n\nDane ustrukturyzowane zorganizowane są w kolumnach cech charakteryzujących każdą obserwację (wiersze). Kolumny posiadają etykietę, która wskazuje na ich interpretację.\nPrzykładem kolumn mogą być takie cechy jak: płeć, wzrost czy ilość kedytów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie.\n\n\nCode\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nTakie przewidywanie również oznaczane jest jako cecha (ang. target).\n\n\nCode\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0\n\n\n\n\n\nW wielu językach programowania domyślnym pojemnikiem na przechowywanie wartości są zmienne.\nwiek = 47\nstan_cywilny = 'kawaler'\nNie są one jednak praktyczną formą do przechowywania i manipulowania danymi.\nJednym z rozwiązań jest przechowywanie wszystkich cech (np. klienta) w jednym obiekcie.\nW Pythonie zadanie to moze realizować obiekt listy, który pozwala przechowywać rózne typy danych w jednym obiekcie.\n\nklient = [38, 'kawaler', 1, 56.3]\nprint(f\"dane klienta {klient} w obiekcie: {type(klient)}\")\n\ndane klienta [38, 'kawaler', 1, 56.3] w obiekcie: &lt;class 'list'&gt;\n\n\nZ punktu widzenia przerwarzania i modelowania mozliwość ta wprowadza więcej problemów niz korzyści. Sprawdźmy domyślne operacje:\n\n\nCode\na = [1,2,3]\nb = [4,5,6]\nprint(f\"a={a}, b={b}\")\nprint(f\"a+b={a+b}\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na+b=[1, 2, 3, 4, 5, 6]\n\n\nnatomiast:\n\n\nCode\nprint(f\"a={a}, b={b}\")\nprint(f\"a*b\")\ntry:\n    print(f\"a*b= {a*b}\")\nexcept TypeError:\n    print(\"operacja niezdefiniowana\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na*b\noperacja niezdefiniowana\n\n\nBiblioteka Numpy:\n\nimport numpy as np\naa = np.array([1,2,3])\nbb = np.array([4,5,6])\n\nprint(type(aa))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nCode\nprint(f\"aa={aa}, bb={bb}\")\nprint(f\"aa+bb={aa+bb}\")\nprint(f\"aa*bb={aa*bb}\")\n\n\naa=[1 2 3], bb=[4 5 6]\naa+bb=[5 7 9]\naa*bb=[ 4 10 18]\n\n\nDzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel. \nDane nieustrukturyzowane to takie, które nie są ułożone w tabelarycznej postaci.\n\n!Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycznej.\n\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "old_lectures/old_wyklad1.html#dane",
    "href": "old_lectures/old_wyklad1.html#dane",
    "title": "Od plików płaskich do Data Mash",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych i biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu klastrów komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop,Apache Kafka , Apache Spark, Apache Flink i ich chmurowe odpowiedniki, używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\n\n\nDane ustrukturyzowane zorganizowane są w kolumnach cech charakteryzujących każdą obserwację (wiersze). Kolumny posiadają etykietę, która wskazuje na ich interpretację.\nPrzykładem kolumn mogą być takie cechy jak: płeć, wzrost czy ilość kedytów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie.\n\n\nCode\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nTakie przewidywanie również oznaczane jest jako cecha (ang. target).\n\n\nCode\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0\n\n\n\n\n\nW wielu językach programowania domyślnym pojemnikiem na przechowywanie wartości są zmienne.\nwiek = 47\nstan_cywilny = 'kawaler'\nNie są one jednak praktyczną formą do przechowywania i manipulowania danymi.\nJednym z rozwiązań jest przechowywanie wszystkich cech (np. klienta) w jednym obiekcie.\nW Pythonie zadanie to moze realizować obiekt listy, który pozwala przechowywać rózne typy danych w jednym obiekcie.\n\nklient = [38, 'kawaler', 1, 56.3]\nprint(f\"dane klienta {klient} w obiekcie: {type(klient)}\")\n\ndane klienta [38, 'kawaler', 1, 56.3] w obiekcie: &lt;class 'list'&gt;\n\n\nZ punktu widzenia przerwarzania i modelowania mozliwość ta wprowadza więcej problemów niz korzyści. Sprawdźmy domyślne operacje:\n\n\nCode\na = [1,2,3]\nb = [4,5,6]\nprint(f\"a={a}, b={b}\")\nprint(f\"a+b={a+b}\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na+b=[1, 2, 3, 4, 5, 6]\n\n\nnatomiast:\n\n\nCode\nprint(f\"a={a}, b={b}\")\nprint(f\"a*b\")\ntry:\n    print(f\"a*b= {a*b}\")\nexcept TypeError:\n    print(\"operacja niezdefiniowana\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na*b\noperacja niezdefiniowana\n\n\nBiblioteka Numpy:\n\nimport numpy as np\naa = np.array([1,2,3])\nbb = np.array([4,5,6])\n\nprint(type(aa))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nCode\nprint(f\"aa={aa}, bb={bb}\")\nprint(f\"aa+bb={aa+bb}\")\nprint(f\"aa*bb={aa*bb}\")\n\n\naa=[1 2 3], bb=[4 5 6]\naa+bb=[5 7 9]\naa*bb=[ 4 10 18]\n\n\nDzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel. \nDane nieustrukturyzowane to takie, które nie są ułożone w tabelarycznej postaci.\n\n!Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycznej.\n\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "old_lectures/old_wyklad1.html#źródła-danych",
    "href": "old_lectures/old_wyklad1.html#źródła-danych",
    "title": "Od plików płaskich do Data Mash",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\ndane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics)."
  },
  {
    "objectID": "old_lectures/old_wyklad1.html#big-data",
    "href": "old_lectures/old_wyklad1.html#big-data",
    "title": "Od plików płaskich do Data Mash",
    "section": "Big Data",
    "text": "Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962."
  },
  {
    "objectID": "old_lectures/old_wyklad1.html#modele-przetwarzania-danych",
    "href": "old_lectures/old_wyklad1.html#modele-przetwarzania-danych",
    "title": "Od plików płaskich do Data Mash",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe.\n\n\nWiedza:\n\nZna historię i filozofię modeli przetwarzania danych.\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\n\n\nUmiejętności:\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym."
  },
  {
    "objectID": "old_lectures/wyklad3.html",
    "href": "old_lectures/wyklad3.html",
    "title": "Mikroserwisy",
    "section": "",
    "text": "Komunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe i big data znacząco zmieniły sposób budowania systemów informatycznych i wykonywnia na niach pracy.\nPorównaj to jak “narzędzia” do realizacji przekazu (gazeta, radio, telewizja, internet, komunikatory, media społecznościowe) zmieniły interakcje międzyludzkie i struktury społeczne.\nKoncepcja mikrousługi (mikroserwisu) jest bardzo popularnym sposobem budowania systemów informatycznych jak i koncepcją przy tworzeniu oprogramowania czy realizacji firmy w duchu Data-Driven. Koncepcja ta pozwala zachować wydajność (rób jedną rzecz ale dobrze), elastyczność i jasną postać całej struktury.\nChociaż istnieją inne sposoby architektury projektów oprogramowania, „mikroserwisy” są często używane nie bez powodu. Idea mikroserwisów tkwi w nazwie: oprogramowanie jest reprezentowane jako wiele małych usług, które działają indywidualnie. Patrząc na ogólną architekturę, każda mikrousługa znajduje się w małej czarnej skrzynce z jasno zdefiniowanymi wejściami i wyjściami. Możesz porównać tego typu zachowanie do “czystej funkcji” w programowaniu funkcyjnym.\nW celu umożliwienia komunikacji różnych mikroserwisów często wybieranym rozwiązaniem jest wykorzystanie Application Programming Interfaces API ."
  },
  {
    "objectID": "old_lectures/wyklad3.html#komunikacja-przez-api",
    "href": "old_lectures/wyklad3.html#komunikacja-przez-api",
    "title": "Mikroserwisy",
    "section": "Komunikacja przez API",
    "text": "Komunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API. API to część, która pozwala na połączenie dwóch mikroserwisów. Interfejsy API są bardzo podobne do stron internetowych. Podobnie jak strona internetowa, serwer wysyła do Ciebie kod reprezentujący stronę internetową. Twoja przeglądarka internetowa interpretuje ten kod i wyświetla stronę internetową.\nWeźmy przypadek biznesowy z modelem ML jako usługą. Załóżmy, że pracujesz dla firmy sprzedającej mieszkania w Bostonie. Chcesz zwiększać sprzedaż i oferować naszym klientom lepszą jakość usług dzięki nowej aplikacji mobilnej, z której może korzystać nawet 1 000 000 osób jednocześnie. Możemy to osiągnąć, udostępniając prognozę wartości domu, gdy użytkownik prosi o wycenę przez Internet.\n\nCzym jest serwowanie modelu ML\n\nSzkolenie dobrego modelu ML to TYLKO pierwsza część całego procesu: Musisz udostępnić swój model użytkownikom końcowym. Robisz to, zapewniając dostęp do modelu na swoim serwerze.\nAby udostępnić model potrzebujesz: modelu, interpretera, danych wsadowych.\nWażne metryki\n\n\nczas oczekiwania,\nkoszty,\nliczba zapytać w jednostce czasu\n\n\nUdostępnianie danych między dwoma lub więcej systemami zawsze było podstawowym wymogiem tworzenia oprogramowania – DevOps vs. MLOps.\n\nGdy wywołasz interfejs API, otrzyma on Twoje żądanie. Żądanie wyzwala kod do uruchomienia na serwerze i generuje odpowiedź odesłaną do Ciebie. Jeśli coś pójdzie nie tak, możesz nie otrzymać żadnej odpowiedzi lub otrzymać kod błędu jako kod stanu HTTP.\n\nKlient-Serwer: Klient (system A) przesyła żądanie przez HTTP do adresu URL hostowanego przez system B, który zwraca odpowiedź. Identycznie działa np przeglądarka internetowa. Żądanie jest kierowane do serwera WWW, który zwraca tekstową stronę HTML.\n\n\nBezstanowe: Żądanie klienta powinno zawierać wszystkie informacje niezbędne do udzielenia pełnej odpowiedzi.\n\nInterfejsy API można wywoływać za pomocą wielu różnych narzędzi. Czasami możesz nawet użyć przeglądarki internetowej. Narzędzia takie jak CURL wykonują zadanie w wierszu poleceń. Możesz używać narzędzi, takich jak Postman, do wywoływania interfejsów API za pomocą interfejsu użytkownika.\n\nCała komunikacja jest objęta ustalonymi zasadami i praktykami, które są nazywane protokołem HTTP.\n\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n- domenę, \n- port, \n- dodatkowe ścieżki, \n- zapytanie\nMetody HTTP:\n- GET, \n- POST\nNagłówki HTTP zawierają:\n- informacje o autoryzacji, \n- cookies metadata\n\nCała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens 4. Ciało zapytania\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidłowe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.\n\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nUmie wybrać strukturę IT dla danego problemu biznesowego\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym."
  },
  {
    "objectID": "old_lectures/wyklad1S.html",
    "href": "old_lectures/wyklad1S.html",
    "title": "Wykład 1",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\n\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych czy biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu domowych komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop, Apache Spark, Apache Flink dzięki rozwiązaniom chmurowym używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\nWszystkie algorytmy uczenia maszynowego wymagają danych ustrukturyzowanych zapisanych w tabelarycznej postaci (tensory).\nZorganizowane są one w kolumnach cech charakteryzujących każdą obserwację (wiersze). Przykładem mogą być takie cechy jak: płeć, wzrost czy ilość posiadanych samochodów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie. Takie przewidywanie również oznaczane jest jako cecha. Zmienne te dobierane są tak, by łatwo można je było pozyskać. Dzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel.\nDane nieustrukturyzowane to takie, które nie są ułożone w~tabelarycznej postaci. &gt; !Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycnzej.\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości~czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "old_lectures/wyklad1S.html#od-plikow-płaskich-do-data-lake",
    "href": "old_lectures/wyklad1S.html#od-plikow-płaskich-do-data-lake",
    "title": "Wykład 1",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\n\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych czy biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu domowych komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop, Apache Spark, Apache Flink dzięki rozwiązaniom chmurowym używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\nWszystkie algorytmy uczenia maszynowego wymagają danych ustrukturyzowanych zapisanych w tabelarycznej postaci (tensory).\nZorganizowane są one w kolumnach cech charakteryzujących każdą obserwację (wiersze). Przykładem mogą być takie cechy jak: płeć, wzrost czy ilość posiadanych samochodów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie. Takie przewidywanie również oznaczane jest jako cecha. Zmienne te dobierane są tak, by łatwo można je było pozyskać. Dzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel.\nDane nieustrukturyzowane to takie, które nie są ułożone w~tabelarycznej postaci. &gt; !Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycnzej.\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości~czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "old_lectures/wyklad1S.html#źródła-danych",
    "href": "old_lectures/wyklad1S.html#źródła-danych",
    "title": "Wykład 1",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w~portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\nDane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics)."
  },
  {
    "objectID": "old_lectures/wyklad1S.html#big-data",
    "href": "old_lectures/wyklad1S.html#big-data",
    "title": "Wykład 1",
    "section": "Big Data",
    "text": "Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962."
  },
  {
    "objectID": "old_lectures/wyklad1S.html#modele-przetwarzania-danych",
    "href": "old_lectures/wyklad1S.html#modele-przetwarzania-danych",
    "title": "Wykład 1",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe."
  },
  {
    "objectID": "old_lectures/wyklad1S.html#źródła-danych-przesyłanych-strumieniowo-obejmują",
    "href": "old_lectures/wyklad1S.html#źródła-danych-przesyłanych-strumieniowo-obejmują",
    "title": "Wykład 1",
    "section": "Źródła danych przesyłanych strumieniowo obejmują:",
    "text": "Źródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego.\n\nAnaliza danych w czasie rzeczywistym a przetwarzanie strumienia zdarzeń\nŁatwo jest połączyć analizę w czasie rzeczywistym i analizę strumieniową (lub przetwarzanie strumienia zdarzeń). Ale chociaż technologie analizy strumieniowej mogą umożliwiać analizę w czasie rzeczywistym, to nie to samo!\nAnaliza strumieniowa polega na przetwarzaniu danych w ruchu. Analityka w czasie rzeczywistym to dowolna metoda przetwarzania danych, która skutkuje okresem opóźnienia określanym jako „w czasie rzeczywistym”.\nZazwyczaj systemy analizy czasu rzeczywistego są definiowane jako twarde i miękkie systemy czasu rzeczywistego. Niedotrzymanie terminu w twardych systemach czasu rzeczywistego, takich jak samolot, jest katastrofalne, a w miękkich systemach czasu rzeczywistego, takich jak stacja pogodowa, niedotrzymanie terminów może prowadzić do bezużytecznych danych.\nPonadto, podczas gdy analiza strumieniowa implikuje istnienie architektury strumieniowej, analiza w czasie rzeczywistym nie implikuje żadnej konkretnej architektury.\nWszystko, co implikuje analityka w czasie rzeczywistym, polega na tym, że tworzenie i przetwarzanie danych odbywa się w dowolnym czasie, który firma definiuje jako „w czasie rzeczywistym”."
  },
  {
    "objectID": "old_lectures/wyklad1S.html#uzasadnienie-biznesowe",
    "href": "old_lectures/wyklad1S.html#uzasadnienie-biznesowe",
    "title": "Wykład 1",
    "section": "Uzasadnienie biznesowe",
    "text": "Uzasadnienie biznesowe\nAnalityka służy do znajdowania znaczących wzorców w danych i odkrywania nowej wiedzy. Dotyczy to zarówno transmisji strumieniowych, jak i tradycyjnych analiz.\nAle w dzisiejszym świecie natura „znajdowania sensownych wzorców w danych” uległa zmianie, ponieważ zmienił się charakter danych. Szybkość, objętość i rodzaje danych eksplodowały.\nTwitter produkuje ponad 500 milionów tweetów dziennie. IDC przewiduje, że do 2025 roku urządzenia Internetu rzeczy (IoT) będą w stanie wygenerować 79,4 zettabajtów (ZB) danych. I te trendy nie wykazują oznak spowolnienia.\nBiorąc pod uwagę nowy charakter danych, główną zaletą analizy strumieniowej jest to, że pomaga ona firmom znajdować znaczące wzorce w danych i odkrywać nową wiedzę ,,w czasie rzeczywistym” lub zbliżonym do rzeczywistego.\n\nktóry pojazd firmowej floty ma prawie pusty bak i~gdzie wysłać prowadzącego pojazd do tankowania.\nKtóry pojazd floty zużywa najwięcej paliwa i~dlaczego?\nKtóre urządzenia w~zakładzie czy fabryce mogą ulec awarii w~ciągu najbliższych dni?\nJakie części zamienne trzeba będzie wymienić iwktórych maszynach w~najbliższym czasie ?\nIlu klientów aktualnie robi zakupy w~sklepie i~czy można im coś zaproponować ?\nCzy klient dzwoni w~celu zerwania umowy ?\ni wiele wiele innych.\n\n8 najlepszych przykładów\nBiznesowe zastosowania"
  },
  {
    "objectID": "old_lectures/wyklad1S.html#definicje",
    "href": "old_lectures/wyklad1S.html#definicje",
    "title": "Wykład 1",
    "section": "Definicje",
    "text": "Definicje\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 - Zdarzenie czyli wszystko co możemy zaobserwować w pewnej chwili czasu. Definicja 2 - W przypadku danych zdarzenie rozumiemy jako niezmienialny rekord w strumieniu danych zakodowany jako JSON, XML, CSV lub binarnie. Definicja 3 - Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie np. logi z urządzenia.\n\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\n\nDefinicja 4 - Strumień danych to dane tworzone przyrostowo w czasie, generowane ze statycznych danych (baza danych, czytanie lini z pliku) bądź w sposób dynamiczny (logi, sensory, funkcje).\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzen (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nGenerowane są jako bezpośredni skutek działania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów)."
  },
  {
    "objectID": "old_lectures/wyklad1S.html#aplikacje-dla-strumieniowania-danych",
    "href": "old_lectures/wyklad1S.html#aplikacje-dla-strumieniowania-danych",
    "title": "Wykład 1",
    "section": "Aplikacje dla strumieniowania danych",
    "text": "Aplikacje dla strumieniowania danych\nAplikacja przetwarzająca strumień zdarzeń powinna umożliwiać przetworzenie i zapisanie zdarzenia oraz dostęp (w tym samym czasie) do innych danych tak by móc dane zdarzenie przetworzyć (wykonać na nim dowolne przeliczenie) i zapisać jako stan lokalny. Stan ten może być zapisywany w wielu miejscach np. zmienne w programie, pliki lokalne, wew i zew bazy danych. Jedną z najbardziej znanych aplikacji tego typu jest Apache Kafka, którą można łączyć np. z Apache Spark bądź Apache Flink.\nPorównanie z aplikacją w trybie batch"
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "wyklad2.html",
    "href": "wyklad2.html",
    "title": "Wykład 2",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, jak dane ewoluowały w różnych branżach i jakie narzędzia są dziś wykorzystywane do ich analizy.\nW tym wstępie przedstawimy ewolucję analizy danych, pokazując, jak zmieniały się technologie i podejścia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aż po nowoczesne podejście do strumieniowego przetwarzania danych."
  },
  {
    "objectID": "wyklad2.html#zmiany-w-analizie-danych",
    "href": "wyklad2.html#zmiany-w-analizie-danych",
    "title": "Wykład 2",
    "section": "Zmiany w analizie danych:",
    "text": "Zmiany w analizie danych:\n\nDane tabelaryczne (tabele SQL):\nPoczątkowo dane były przechowywane w postaci tabel, gdzie każda tabela zawierała zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL). Modele takie doskonale nadawały się do danych ustrukturyzowanych.\n\n\nDane grafowe:\nWraz z rozwojem potrzeb biznesowych pojawiły się dane grafowe, w których relacje między obiektami (np. użytkownikami, produktami) są reprezentowane jako wierzchołki i krawędzie grafu. Przykłady takich danych to sieci społecznościowe, linki internetowe czy zależności między produktami.\n\n\nDane tekstowe:\nKolejnym etapem była analiza danych tekstowych, które zaczęły odgrywać istotną rolę w analizie danych z mediów społecznościowych, e-maili, forów czy opinii. Technologie takie jak NLP (przetwarzanie języka naturalnego) pozwalają wydobywać sens z nieustrukturyzowanego tekstu.\n\n\nDane strumieniowe:\nObecnie najbardziej dynamicznie rozwija się analiza danych strumieniowych, gdzie dane są analizowane na bieżąco, w miarę ich napływania. Przykładem takiego podejścia jest monitorowanie zdarzeń w czasie rzeczywistym, np. transakcje finansowe, analiza social media, detekcja oszustw."
  },
  {
    "objectID": "wyklad2.html#popularne-narzędzia-analiz-danych",
    "href": "wyklad2.html#popularne-narzędzia-analiz-danych",
    "title": "Wykład 2",
    "section": "Popularne narzędzia analiz danych:",
    "text": "Popularne narzędzia analiz danych:\n\nSQL:\nTradycyjne bazy danych SQL są idealne do pracy z danymi ustrukturyzowanymi, które wymagają ścisłej struktury tabel (np. dane o transakcjach, klientach). SQL jest standardem w analizie takich danych, ale nie jest wystarczający w przypadku pracy z dużymi ilościami danych nieustrukturyzowanych.\n\n\nNoSQL:\nBazy danych NoSQL, takie jak MongoDB, Cassandra, pozwalają na bardziej elastyczne podejście do danych, które mogą przybierać różne formy (np. dokumenty JSON, dane z sensorów, obrazy). NoSQL świetnie sprawdza się w przypadku danych, które nie pasują do tradycyjnych struktur tabelarycznych.\n\n\nData Lake:\nData Lake to systemy przechowywania ogromnych zbiorów danych, zarówno ustrukturyzowanych, jak i nieustrukturyzowanych. Dzięki Data Lake organizacje mogą przechowywać dane w pierwotnej formie i analizować je później w miarę potrzeb, bez potrzeby wcześniejszego przetwarzania. ### Apache Kafka: Kafka to system przetwarzania strumieniowego, który umożliwia zbieranie, przechowywanie i przetwarzanie danych w czasie rzeczywistym. Jest szeroko wykorzystywany w aplikacjach wymagających szybkiej reakcji na dane napływające w czasie rzeczywistym. ### Apache Flink: Apache Flink to narzędzie do przetwarzania strumieniowego, które umożliwia analizowanie danych w czasie rzeczywistym z minimalnym opóźnieniem. W przeciwieństwie do Kafki, Flink nie tylko zbiera dane, ale także je przetwarza, co jest szczególnie przydatne w analizach złożonych.\n\n\nPrzykład: Jak Netflix analizuje dane w czasie rzeczywistym?\nNetflix to doskonały przykład firmy, która z powodzeniem wykorzystuje dane strumieniowe w celu dostarczania użytkownikom spersonalizowanych rekomendacji i analizowania ich zachowań. Dzięki technologii strumieniowej, Netflix monitoruje działania swoich użytkowników (co oglądają, jakie treści przewijają, jak długo oglądają), analizując te dane w czasie rzeczywistym. Na podstawie tej analizy Netflix jest w stanie dostarczać rekomendacje filmów i seriali w czasie rzeczywistym, dostosowując je do zmieniających się preferencji użytkownika."
  },
  {
    "objectID": "wyklad2.html#przykład-strumieniowego-przetwarzania-w-netflix",
    "href": "wyklad2.html#przykład-strumieniowego-przetwarzania-w-netflix",
    "title": "Wykład 2",
    "section": "Przykład strumieniowego przetwarzania w Netflix:",
    "text": "Przykład strumieniowego przetwarzania w Netflix:\nKiedy użytkownik zaczyna oglądać film, system śledzi jego reakcje (np. przerwanie filmu, przewijanie) i dostosowuje rekomendacje do jego preferencji. Takie dane mogą być przesyłane do systemu analitycznego za pomocą narzędzi takich jak Apache Kafka i przetwarzane na żywo w Apache Flink."
  },
  {
    "objectID": "wyklad2.html#podsumowanie",
    "href": "wyklad2.html#podsumowanie",
    "title": "Wykład 2",
    "section": "Podsumowanie",
    "text": "Podsumowanie\nDzięki ewolucji technologii, analiza danych przeszła długą drogę, od prostych tabel SQL, przez złożone dane grafowe i tekstowe, aż do analiz strumieniowych. Zrozumienie różnic między tymi podejściami pozwala na lepsze dobieranie narzędzi do odpowiednich typów analiz, w tym wykorzystania nowoczesnych systemów przetwarzania danych w czasie rzeczywistym, takich jak Kafka i Flink."
  },
  {
    "objectID": "wyklad2.html#ewolucja-analizy-danych-od-struktur-tabelarycznych-do-strumieni",
    "href": "wyklad2.html#ewolucja-analizy-danych-od-struktur-tabelarycznych-do-strumieni",
    "title": "Wykład 2",
    "section": "Ewolucja analizy danych: Od struktur tabelarycznych do strumieni",
    "text": "Ewolucja analizy danych: Od struktur tabelarycznych do strumieni\nRozwój technologii informatycznych stworzył nowe możliwości przetwarzania ogromnych ilości danych, zarówno ustrukturyzowanych, jak i nieustrukturyzowanych. W ciągu ostatnich kilku dekad obserwujemy wzrost dostępnych danych oraz technologii do ich przechowywania i analizy. Dzięki temu dane stały się jednym z najcenniejszych zasobów współczesnych organizacji."
  },
  {
    "objectID": "wyklad2.html#dane-ustrukturyzowane",
    "href": "wyklad2.html#dane-ustrukturyzowane",
    "title": "Wykład 2",
    "section": "Dane ustrukturyzowane",
    "text": "Dane ustrukturyzowane\nDane ustrukturyzowane to dane, które są przechowywane w sposób zorganizowany i jednoznacznie określony. Przykładami takich danych są tabele w bazach SQL, w których każda kolumna reprezentuje jedną cechę, a każdy wiersz to pojedynczy rekord. Tego typu dane są najczęściej wykorzystywane w klasycznych algorytmach uczenia maszynowego, takich jak regresja logistyczna, XGBoost czy modele klasyfikacji.\nPrzykład: Załóżmy, że mamy tabelę, w której każda linia przedstawia dane o kliencie: jego płeć, wzrost, ilość kredytów itd.\n\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nNa tej podstawie możemy przewidywać prawdopodobieństwo spłaty kredytu (regresja logistyczna). Takie przewidywanie również oznaczane jest jako cecha (ang. target).\n\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0"
  },
  {
    "objectID": "wyklad2.html#dane-nieustrukturyzowane",
    "href": "wyklad2.html#dane-nieustrukturyzowane",
    "title": "Wykład 2",
    "section": "Dane nieustrukturyzowane",
    "text": "Dane nieustrukturyzowane\nDane nieustrukturyzowane to dane, które nie mają określonej, tabelarycznej formy. Należą do nich obrazy, dźwięki, wideo i tekst. Choć te dane wydają się chaotyczne, są one również cennym źródłem informacji, które można przetwarzać za pomocą odpowiednich algorytmów.\nPrzykład: Analiza obrazów (np. weryfikacja, czy zdjęcie przedstawia psa czy kota) lub analizy tekstu (np. sentymentu w tweetach) są przykładami pracy z danymi nieustrukturyzowanymi.\n\nJakie wyzwania niesie analiza danych w różnych formach?\n\nZwiększający się wolumen danych, a także ich różnorodność, stawia przed nami kolejne wyzwania związane z przetwarzaniem i analizowaniem tych danych.\n\nKluczowe pytanie brzmi: jak efektywnie przetwarzać ogromne ilości danych w czasie rzeczywistym?"
  },
  {
    "objectID": "wyklad2.html#przetwarzanie-wsadowe-vs.-przetwarzanie-strumieniowe",
    "href": "wyklad2.html#przetwarzanie-wsadowe-vs.-przetwarzanie-strumieniowe",
    "title": "Wykład 2",
    "section": "Przetwarzanie wsadowe vs. przetwarzanie strumieniowe",
    "text": "Przetwarzanie wsadowe vs. przetwarzanie strumieniowe\nW tradycyjnych systemach analizy danych, takich jak bazy danych SQL, przetwarzanie danych odbywa się w trybie wsadowym (batch processing). Oznacza to, że dane są zbierane przez pewien czas, a następnie przetwarzane w partiach (np. raz dziennie, raz w tygodniu). Jest to podejście, które świetnie sprawdza się w analizach historycznych i podejmowaniu decyzji na podstawie dużych zbiorów danych.\n\nPrzykład batch processing:\nPrzetwarzanie wszystkich transakcji dokonanych przez klientów w ciągu dnia, aby na końcu dnia wyciągnąć raporty i wnioski dotyczące aktywności.\nNatomiast przetwarzanie strumieniowe (stream processing) pozwala na bieżące analizowanie danych w momencie ich napływania. Dzięki temu, dane mogą być przetwarzane i analizowane w czasie rzeczywistym, co jest niezwykle ważne w przypadku takich zastosowań jak wykrywanie nadużyć w czasie rzeczywistym, personalizowanie treści czy monitorowanie urządzeń IoT.\n\n\nPrzykład stream processing:\nSystem detekcji oszustw w kartach kredytowych, który monitoruje transakcje na żywo, analizując je pod kątem podejrzanych działań (np. przekroczenie typowego wzorca wydatków).\nWspółczesna analiza danych nie ogranicza się do prostych zbiorów danych w formie tabelarycznej. Przechodzimy od danych ustrukturyzowanych do zaawansowanego przetwarzania strumieniowego, które pozwala na podejmowanie decyzji w czasie rzeczywistym. Zrozumienie różnicy między przetwarzaniem wsadowym a strumieniowym to klucz do pracy z nowoczesnymi technologiami i narzędziami, które napędzają innowacje w wielu dziedzinach."
  },
  {
    "objectID": "wyklad2.html#źródła-danych",
    "href": "wyklad2.html#źródła-danych",
    "title": "Wykład 2",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\ndane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics)."
  },
  {
    "objectID": "wyklad2.html#architektura-systemów-real-time",
    "href": "wyklad2.html#architektura-systemów-real-time",
    "title": "Wykład 2",
    "section": "Architektura systemów real-time",
    "text": "Architektura systemów real-time\nWykorzystanie systemów real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, która będzie mogła szybko przetwarzać ogromne ilości danych oraz reagować na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, ponieważ umożliwia szybsze podejmowanie decyzji, monitorowanie procesów w czasie rzeczywistym i reagowanie na zdarzenia bez opóźnienia.\nOmówimy główne elementy architektury systemów real-time, popularne wzorce architektoniczne oraz technologie, które są wykorzystywane do budowy takich systemów.\n\nPodstawowe elementy systemu real-time\nSystemy real-time muszą spełniać szereg wymagań związanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentów w architekturze systemu, które zapewniają jego prawidłowe funkcjonowanie.\n\nProducent danych (Data Producer)\nDane w systemie real-time pochodzą z różnych źródeł, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urządzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z giełdy.\nDane użytkowników: np. logi użytkowników w aplikacjach mobilnych, dane z mediów społecznościowych.\n\n\n\nPrzesyłanie danych (Data Transport)\nDane muszą być szybko przesyłane do systemów, które mogą je analizować. W tym celu wykorzystywane są technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyłania danych w czasie rzeczywistym, zapewniający wysoką wydajność i niezawodność.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z dużą ilością subskrybentów.\n\n\n\nPrzetwarzanie danych (Data Processing)\nDane w systemach real-time są często przetwarzane w strumieniu. Dwa główne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, które może mieć opóźnienie, ale przetwarza dane w sposób efektywny. Może być wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opóźnień, w którym dane są natychmiastowo analizowane i przetwarzane.\n\n\n\nSkładowanie danych (Data Storage)\nPrzechowywanie danych w systemie real-time zależy od wymagań aplikacji. Dwa główne rodzaje przechowywania to:\n\nData Lake: składowanie ogromnych ilości nieprzetworzonych danych w postaci surowych plików. Bazy danych NoSQL: takie jak Cassandra, które umożliwiają szybki dostęp do danych w czasie rzeczywistym.\nData Warehouse: składowanie przetworzonych danych w celu ich analizy.\n\n\n\nAnaliza i wizualizacja danych (Data Analytics and Visualization)\nPo przetworzeniu danych w czasie rzeczywistym należy wykonać ich analizę i prezentację w sposób zrozumiały dla użytkownika:\n\nDashboardy: narzędzia takie jak Grafana lub Kibana, które służą do wizualizacji wyników w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmów uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\nPopularne architektury systemów real-time\n\nLambda Architecture\nLambda Architecture to popularna koncepcja przetwarzania danych, która łączy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura używana w systemach przetwarzania Big Data, która zakłada dwie warstwy:\n\nBatch Layer: przetwarzanie (dużych ilości) danych wsadowych, które są później wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napływające dane strumieniowe, np. z sensorów, social media, transakcji, w celu uzyskania natychmiastowych wyników.\nServing Layer: warstwa, która łączy wyniki obu poprzednich warstw i dostarcza je do użytkownika np. za pomocą API.\n\n \n\n\nZalety i Wady Lambda Architecture:\n\n✅ Możliwość łączenia przetwarzania wsadowego i strumieniowego,\n✅ wsparcie dla dużych zbiorów danych,\n✅ elastyczność w przetwarzaniu złożonych zapytań.\n❌ Wymaga utrzymywania dwóch oddzielnych systemów do przetwarzania danych (batch i stream), co prowadzi do złożoności implementacji i utrzymania.\n\n\n\nKappa Architecture\nKappa Architecture jest uproszczoną wersją Lambda Architecture. Zamiast używać dwóch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jedną warstwę przetwarzania strumieniowego, co upraszcza cały system.\nJest to bardziej elastyczne podejście do budowy systemów real-time, zwłaszcza w przypadku, gdy dane są przetwarzane tylko w jednym trybie (streaming).\n \n\n\nZalety i Wady Kappa Architecture:\n\n✅ Prostota: Jako że przetwarzanie danych odbywa się tylko w jednym strumieniu, cały system jest prostszy i bardziej spójny.\n✅ Skalowalność: Dzięki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekście analizy danych w czasie rzeczywistym.\n✅ Idealne dla ML: Kappa Architecture świetnie sprawdza się w zastosowaniach związanych z Machine Learning, ponieważ przetwarzanie danych odbywa się na bieżąco, co pozwala na szybsze uczenie i wdrażanie modeli ML w czasie rzeczywistym.\n❌ Może być mniej wydajna przy bardzo dużych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\nMicroservices Architecture\nArchitektura mikroserwisów jest powszechnie wykorzystywana w systemach real-time, ponieważ umożliwia:\n\nPodział aplikacji na mniejsze, autonomiczne jednostki.\nElastyczność i skalowalność systemu.\nMożliwość przetwarzania różnych rodzajów danych przez różne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomości.\n\n\n\nPrzykład\nUber to przykład firmy, która skutecznie wykorzystuje narzędzia do przetwarzania strumieniowego, by monitorować ruch drogowy w czasie rzeczywistym. Dzięki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdów oraz czasach oczekiwania na przejazd, które są następnie analizowane na żywo.\nDane wejściowe: Informacje o czasie i miejscu podróży, dane GPS z pojazdów, natężenie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyłania tych danych w czasie rzeczywistym do systemów takich jak Apache Flink lub Spark Streaming, które analizują je na bieżąco.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasę w czasie rzeczywistym.\nWynik: Użytkownicy Ubera otrzymują prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowców), co umożliwia optymalizację transportu."
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: Szkoła Główna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJęzyk prowadzenia: polski\nPoziom przedmiotu: średnio-zaawansowany\nProwadzący: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspółczesny biznes opiera się na podejmowaniu decyzji opartych na danych. Coraz większa ilość informacji, rosnące wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiają, że analiza danych w czasie rzeczywistym staje się kluczowym elementem nowoczesnych procesów biznesowych.\nNa zajęciach studenci zapoznają się z metodami i technologiami umożliwiającymi przetwarzanie danych w czasie rzeczywistym. Szczególną uwagę poświęcimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz głębokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretować zjawiska biznesowe, ale także podejmować szybkie i trafne decyzje.\nW ramach kursu omówimy zarówno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dźwięk, strumieniowanie wideo). Studenci poznają architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a także wyzwania związane z modelowaniem danych w czasie rzeczywistym na dużą skalę.\nKurs obejmuje część teoretyczną oraz praktyczne laboratoria, podczas których studenci będą pracować z rzeczywistymi danymi w środowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. Dzięki temu studenci nie tylko zdobędą wiedzę na temat metod analitycznych, ale także nauczą się korzystać z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plików płaskich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (Wykład)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametrów modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykładzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego środowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego środowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 1.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 2.\nPrzygotowanie środowiska Microsoft Azure. Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzędzia IT do szybkiej analizy logów.\nNarzędzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-kształcenia",
    "href": "sylabus.html#efekty-kształcenia",
    "title": "Sylabus",
    "section": "Efekty kształcenia",
    "text": "Efekty kształcenia\n\nWiedza:\n\n\nZna historię i filozofię modeli przetwarzania danych\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiązania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nUmie wybrać strukturę IT dla danego problemu biznesowego\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejętności:\n\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiązania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować, przetwarzać oraz zachowywać dane generowane w czasie rzeczywistym\n\nPowiązania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\nPowiązania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosować i skonstruować system do przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem\n\nPowiązania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiązania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1️⃣ Zając S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2️⃣ Frątczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3️⃣ Bellemare A., Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę, O’Reilly 2021.\n4️⃣ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, O’Reilly 2022.\n5️⃣ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. Rozwiązania typowych problemów dotyczących przygotowania danych, konstruowania modeli i MLOps, O’Reilly 2021.\n6️⃣ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, O’Reilly 2022.\n7️⃣ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly 2018.\n8️⃣ Sebastián Ramírez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostępna online).\n9️⃣ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\n🔟 Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, O’Reilly 2019."
  },
  {
    "objectID": "wyklad1.html",
    "href": "wyklad1.html",
    "title": "Wykład 1",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu"
  },
  {
    "objectID": "wyklad1.html#zapoznanie-studentów-z-podstawami-real-time-analytics-różnicami-między-trybami-przetwarzania-danych-batch-streaming-real-time-oraz-kluczowymi-zastosowaniami-i-wyzwaniami.",
    "href": "wyklad1.html#zapoznanie-studentów-z-podstawami-real-time-analytics-różnicami-między-trybami-przetwarzania-danych-batch-streaming-real-time-oraz-kluczowymi-zastosowaniami-i-wyzwaniami.",
    "title": "Wykład 1",
    "section": "Zapoznanie studentów z podstawami real-time analytics, różnicami między trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami.",
    "text": "Zapoznanie studentów z podstawami real-time analytics, różnicami między trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami."
  },
  {
    "objectID": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez konieczności przechowywania i oczekiwania na późniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wniosków i reakcji na zmieniające się warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) – dane są analizowane w ciągu milisekund lub sekund od ich wygenerowania.\nStreaming vs. Batch Processing – analiza danych może odbywać się w sposób ciągły (streaming) lub w z góry określonych interwałach (batch).\nIntegracja z IoT, AI i ML – real-time analytics często współpracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym – np. natychmiastowa detekcja oszustw w transakcjach bankowych."
  },
  {
    "objectID": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "Wykład 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowość\n\nWykrywanie oszustw – analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujących na oszustwa.\nAutomatyczny trading – systemy HFT (High-Frequency Trading) analizują miliony danych w ułamkach sekundy.\nDynamiczne oceny kredytowe – natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym – dynamiczne rekomendacje produktów na podstawie aktualnego zachowania użytkownika.\nDynamiczne ceny – np. Uber, Amazon i hotele stosują dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediów społecznościowych – analiza nastrojów klientów i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej – analiza logów w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystąpieniem.\nSmart Cities – analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji świetlnej.\nAnalityka IoT – urządzenia IoT generują strumienie danych, które można analizować w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentów – analiza sygnałów z urządzeń medycznych w celu natychmiastowego wykrycia zagrożenia życia.\nAnalityka epidemiologiczna – śledzenie rozprzestrzeniania się chorób na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemów informatycznych, który umożliwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branżach – od finansów, przez e-commerce, aż po ochronę zdrowia i IoT."
  },
  {
    "objectID": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "Wykład 1",
    "section": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstnieją trzy główne podejścia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKażde z nich różni się szybkością przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing – Przetwarzanie wsadowe\n📌 Definicja:\nBatch Processing polega na zbieraniu dużych ilości danych i ich przetwarzaniu w określonych odstępach czasu (np. co godzinę, codziennie, co tydzień).\n📌 Cechy:\n\n✅ Wysoka wydajność dla dużych zbiorów danych\n✅ Przetwarzanie danych po ich zgromadzeniu\n✅ Nie wymaga natychmiastowej analizy\n✅ Zwykle tańsze niż przetwarzanie w czasie rzeczywistym\n❌ Opóźnienia – wyniki są dostępne dopiero po zakończeniu przetwarzania\n\n📌 Przykłady zastosowań:\n\nGenerowanie raportów finansowych na koniec dnia/miesiąca\nAnaliza trendów sprzedaży na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\n📌 Przykładowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiąca\n\n# Agregacja danych - miesięczne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wyników do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyś chciał utworzyć dane do przykładu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics – Analiza niemal w czasie rzeczywistym\n📌 Definicja:\nNear Real-Time Analytics to analiza danych, która odbywa się z minimalnym opóźnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie pełna analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duże opóźnienia mogą wpłynąć na biznes.\n📌 Cechy:\n\n✅ Przetwarzanie danych w krótkich odstępach czasu (kilka sekund – minut)\n✅ Umożliwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\n✅ Optymalny balans między kosztami a szybkością\n❌ Nie nadaje się do systemów wymagających natychmiastowej reakcji\n\n📌 Przykłady zastosowań:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciągu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowań użytkowników\nAnaliza logów serwerów i sieci w celu wykrycia anomalii\n\n📌 Przykładowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logów IT)\nAmazon Kinesis\n\nPrzykład producenta danych realizującego tranzakcje wysyłane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generująca przykładowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota między 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# Zakończenie działania producenta\nproducer.flush()\nproducer.close()\nPrzykład consumenta - programu sparawdzającego zbyt duże transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\nPrzykładowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics – Analiza w czasie rzeczywistym\n📌 Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w ułamku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagających reakcji w czasie rzeczywistym, np. w transakcjach giełdowych, systemach IoT czy cyberbezpieczeństwie.\n📌 Cechy:\n\n✅ Bardzo niskie opóźnienie (milliseconds-seconds)\n✅ Umożliwia natychmiastową reakcję systemu\n✅ Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\n❌ Droższe i bardziej złożone technologicznie niż batch processing\n\n📌 Przykłady zastosowań:\n\nHigh-Frequency Trading (HFT) – analiza i podejmowanie decyzji w transakcjach giełdowych w milisekundach\nAutonomiczne samochody – analiza strumieni danych z kamer i sensorów w czasie rzeczywistym\nCyberbezpieczeństwo – detekcja ataków w sieciach komputerowych w ułamku sekundy\nAnalityka IoT – np. natychmiastowa detekcja anomalii w danych z czujników przemysłowych\n\n📌 Przykładowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\n🔎 Porównanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpóźnienie\nMinuty – godziny – dni\nSekundy – minuty\nMilisekundy – sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w pełni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\n📉 Niski\n📈 Średni\n📈📈 Wysoki\n\n\nZłożoność implementacji\n📉 Prosta\n📈 Średnia\n📈📈 Trudna\n\n\nPrzykłady zastosowań\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\n📌 Kiedy stosować Batch Processing?\n\n✅ Gdy nie wymagasz natychmiastowej analizy\n✅ Gdy masz duże ilości danych, ale przetwarzane są one okresowo\n✅ Gdy chcesz obniżyć koszty\n\n📌 Kiedy stosować Near Real-Time Analytics?\n\n✅ Gdy wymagasz analizy w krótkim czasie (sekundy – minuty)\n✅ Gdy potrzebujesz bardziej aktualnych danych, ale nie w pełnym real-time\n✅ Gdy szukasz kompromisu między wydajnością a kosztami\n\n📌 Kiedy stosować Real-Time Analytics?\n\n✅ Gdy każda milisekunda ma znaczenie (np. giełda, autonomiczne pojazdy)\n✅ Gdy chcesz wykrywać oszustwa, anomalie lub incydenty natychmiast\n✅ Gdy system musi natychmiast reagować na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne – w wielu przypadkach near real-time jest wystarczające i bardziej opłacalne. Kluczowe jest zrozumienie wymagań biznesowych przed wyborem odpowiedniego rozwiązania."
  },
  {
    "objectID": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "href": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "title": "Wykład 1",
    "section": "Dlaczego Real-Time Analytics jest ważne?",
    "text": "Dlaczego Real-Time Analytics jest ważne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje się coraz bardziej istotna w wielu branżach, ponieważ umożliwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodów, dla których real-time analytics jest ważne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagować na zmiany i wydarzenia w czasie rzeczywistym. Dzięki temu można podejmować decyzje szybciej, co jest kluczowe w dynamicznych środowiskach, takich jak:\n\nMarketing: Reklamy mogą być dostosowane do zachowań użytkowników w czasie rzeczywistym (np. personalizacja treści reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie każda minuta może oznaczać różnicę w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogą monitorować kluczowe wskaźniki operacyjne na bieżąco. Przykłady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urządzeń w fabrykach, aby natychmiast wykrywać awarie i zapobiegać przestojom.\nHealthtech: Śledzenie parametrów życiowych pacjentów i wykrywanie anomalii, co może ratować życie.\n\n\n\nZwiększenie efektywności operacyjnej\nReal-time analytics umożliwia natychmiastowe wykrywanie i eliminowanie problemów operacyjnych, zanim staną się poważniejsze. Przykłady:\n\nLogistyka: Śledzenie przesyłek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywność i zmniejsza opóźnienia.\nRetail: Monitorowanie poziomu zapasów na bieżąco i dostosowywanie zamówień do aktualnych potrzeb.\n\n\n\nKonkurencyjność\nOrganizacje, które wykorzystują analitykę w czasie rzeczywistym, mają przewagę nad konkurencją, ponieważ mogą szybciej reagować na zmiany na rynku, nowe potrzeby klientów i sytuacje kryzysowe. Dzięki natychmiastowym informacjom:\n\nMożna podejmować decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywać lepsze relacje z klientami, reagując na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doświadczenia użytkowników (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z użytkownikami w trakcie ich trwania. Przykłady:\n\nE-commerce: Analiza koszyka zakupowego użytkownika w czasie rzeczywistym, aby np. zaoferować rabat lub przypomnieć o porzuconych produktach.\nStreaming: Optymalizacja jakości usługi wideo/streamingowej w zależności od dostępnej przepustowości łącza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym świecie pełnym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeństwa. Przykłady:\n\nCyberbezpieczeństwo: Real-time analytics umożliwia wykrywanie podejrzanych działań w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztów\nDzięki analizie w czasie rzeczywistym można optymalizować zasoby i zmniejszać koszty. Na przykład:\n\nZarządzanie energią: Analiza zużycia energii w czasie rzeczywistym, umożliwiająca optymalizację wydatków na energię w firmach.\nOptymalizacja łańcucha dostaw: Dzięki bieżącemu śledzeniu zapasów i dostaw można lepiej zarządzać kosztami magazynowania i transportu.\n\n\n\nZdolność do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, które mogą przewidywać przyszłe zachowania lub problemy, a także je eliminować zanim się pojawią. Na przykład:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w połączeniu z modelami predykcyjnymi pozwala przewidywać awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym można dostosowywać produkcję lub zapasy na podstawie bieżących trendów.\n\nReal-time analytics to nie tylko analiza danych – to kluczowy element strategii firm w świecie, który wymaga szybkich reakcji, elastyczności i dostosowywania się do zmieniającego się otoczenia. Firmy, które wdrażają te technologie, mogą znacząco poprawić swoje wyniki finansowe, obsługę klienta, wydajność operacyjną, a także przewagę konkurencyjną."
  },
  {
    "objectID": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiąże się z wieloma wyzwaniami i trudnościami, które trzeba rozwiązać, aby systemy real-time działały efektywnie i niezawodnie. Pomimo ogromnego potencjału, jaki daje możliwość natychmiastowego przetwarzania danych, realizacja tych procesów w praktyce wiąże się z licznymi problemami technologicznymi, organizacyjnymi i dotyczącymi zarządzania danymi.\nPoniżej przedstawiamy najważniejsze wyzwania oraz możliwe rozwiązania, które należy uwzględnić podczas implementacji systemów analizy danych w czasie rzeczywistym.\n\nSkalowalność systemów\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadań. W miarę jak ilość generowanych danych rośnie, systemy muszą być w stanie obsługiwać większe obciążenie bez opóźnienia w przetwarzaniu.\nZwiększona ilość danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, ilość generowanych danych może być olbrzymia. Potrzebna jest elastyczność: System musi automatycznie dostosowywać zasoby w zależności od obciążenia.\n\n\nRozwiązanie:\nWykorzystanie skalowalnych systemów chmurowych, które pozwalają na dynamiczne zwiększanie zasobów obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarządzania kontenerami i automatycznego skalowania mikroserwisów. Technologie strumieniowe (Apache Kafka, Apache Flink) umożliwiające przetwarzanie danych w sposób wydajny i rozproszony.\n\n\n\nOpóźnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, każde opóźnienie w przetwarzaniu danych może mieć poważne konsekwencje. Dotyczy to zwłaszcza obszarów takich jak:\nWykrywanie oszustw: W przypadku systemów płatności online, opóźnienie w analizie transakcji może oznaczać przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentów: Opóźnienia mogą wpłynąć na skuteczność reakcji w sytuacjach kryzysowych.\n\n\nRozwiązanie:\nUżywanie algorytmów optymalizujących czas przetwarzania, np. stream processing z wykorzystaniem systemów takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliżej źródła (np. urządzenia IoT), aby zmniejszyć opóźnienia w transmisji danych do chmury.\n\n\n\nJakość danych i zarządzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizować dane w czasie rzeczywistym, ale także zapewnić ich wysoką jakość. W przeciwnym razie analizy mogą prowadzić do błędnych wniosków lub opóźnień w reagowaniu na nieprawidłowe dane.\nZanieczyszczone dane: W systemach real-time dane często są niepełne, brudne, błędne lub nieuporządkowane. Zmiana charakterystyki danych: Dane mogą zmieniać się w czasie, co może utrudniać ich przetwarzanie i analizę. #### Rozwiązanie:\nData cleansing i data validation na wstępnym etapie procesu. Automatyczne systemy monitorowania jakości danych w celu wykrywania błędów w czasie rzeczywistym. Zarządzanie danymi w strumieniu: Narzędzia takie jak Apache Kafka pozwalają na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZłożoność integracji systemów\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym często muszą współpracować z istniejącymi systemami IT i źródłami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemów, zwłaszcza w rozproszonej architekturze, może być skomplikowana.\n\n\nRozwiązanie:\nUżywanie API do łatwiejszej integracji z zewnętrznymi systemami. Mikroserwisy i konteneryzacja z pomocą narzędzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, które umożliwia łatwą integrację różnych źródeł danych oraz zapewnia elastyczność w dostosowywaniu systemów do rosnących potrzeb.\n\n\n\nBezpieczeństwo i prywatność\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiąże się z ogromną ilością wrażliwych informacji, szczególnie w branżach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, że dane są odpowiednio chronione przed nieautoryzowanym dostępem, jest kluczowe.\nOchrona danych w czasie transmisji: Muszą być szyfrowane zarówno podczas przesyłania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym może być celem ataków, takich jak DDoS czy SQL injection.\n\n\nRozwiązanie:\nSzyfrowanie danych zarówno w spoczynku, jak i podczas przesyłania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeństwa. Zgodność z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarządzanie błędami i awariami\n\nWyzwanie:\nBłędy i awarie w systemach real-time mogą prowadzić do poważnych konsekwencji, w tym utraty danych, opóźnień w analizach czy nawet usunięcia usług. W systemach rozproszonych trudno jest osiągnąć pełną niezawodność.\n\n\nRozwiązanie:\nRedundancja: Tworzenie kopii zapasowych systemów i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), które pozwalają na szybkie wykrycie i naprawienie problemów. Zarządzanie stanem: Dzięki użyciu narzędzi jak Apache Kafka, można ponownie przetwarzać dane, jeśli wystąpił błąd w transmisji.\n\n\n\nKoszty związane z infrastrukturą\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, która zapewni odpowiednią moc obliczeniową i pamięć. To może wiązać się z dużymi kosztami, szczególnie gdy dane muszą być przechowywane i przetwarzane w czasie rzeczywistym na dużą skalę.\n\n\nRozwiązanie:\nChmura obliczeniowa: Możliwość elastycznego skalowania zasobów w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalają na uruchamianie procesów bez potrzeby utrzymywania stałej infrastruktury.\nChociaż analiza danych w czasie rzeczywistym oferuje ogromne korzyści, wiąże się także z wieloma wyzwaniami. Właściwa architektura, narzędzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogą pomóc w przezwyciężeniu wielu z tych trudności. Warto również pamiętać o konieczności zapewnienia wysokiej jakości danych, ich bezpieczeństwa, a także elastyczności i skalowalności systemów, które będą w stanie sprostać rosnącym wymaganiom."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli VI bud G\n\n18-02-2025 (wtorek) 13:30-15:10 - Wykład 1\n25-02-2025 (wtorek) 13:30-15:10 - Wykład 2\n04-03-2025 (wtorek) 13:30-15:10 - Wykład 3\n11-03-2025 (wtorek) 13:30-15:10 - Wykład 4\n18-03-2025 (wtorek) 13:30-15:10 - Wykład 5\n\nWykład 5 kończy się TESTEM: 20 pytań - 30 minut. Test przeprowadzany jest za pośrednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć).\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "old_lectures/wyklad2.html",
    "href": "old_lectures/wyklad2.html",
    "title": "Analiza strumieni danych",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "old_lectures/wyklad2.html#batch-vs-stream-processing",
    "href": "old_lectures/wyklad2.html#batch-vs-stream-processing",
    "title": "Analiza strumieni danych",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "old_lectures/wyklad2.html#strumienie-danych",
    "href": "old_lectures/wyklad2.html#strumienie-danych",
    "title": "Analiza strumieni danych",
    "section": "Strumienie danych",
    "text": "Strumienie danych\nStrumieniowanie możesz kojarzyć z serwisów przesyłających video w trybie online. Gdy oglądasz swój ulubiony serial (tak jak teraz na zajęciach) serwis odpowiadający za strumieniowanie w nieprzerwany sposób przesyła do ciebie kolejne “porcje” video. Identycznie koncepcja ta realizowana jest w przypadku danych strumieniowych. Format przesyłanych porcji nie musi być plikiem video, wszystko zależy od celu realizowanego biznesowo. Np. ciągły pomiar z różnego rodzaju czujników w farbykach, elektrowniach itp. Warto odnotować, że masz do czynienia z ciągłym strumieniem danych, które przetwarzać musisz w czasie rzeczywistym. Nie możesz czekać do zatrzymania linii produkcyjnych w celu wykonania analizy, wszystkie pojawiające się problemy chcesz rejestrować natychmiast i jak najszybciej na nie reagować.\n\nAnaliza strumieni danych to ciągłe przetwarzanie i analiza dużych zbiorów danych w ruchu.\n\nPorównuj to do wsakazanych powyżej elementów Big Data. Przetwarzanie Batchowe jest przeciwieństwem do przetwarzania strumieniowego. Najpierw zbierasz duże ilości danych a potem realizujesz analizy. Możesz oczywiście zawsze pobrać video w całości zanim je obejrzysz, ale czy miałoby to sens? Istnieją przypadki gdy takie podejście nie stanowi problemu, ale już tu widzisz, że przetwarzanie strumieniowe może przynieść dla biznesu dodatkowe wartości dodane, których trudno oczekiwać przy wsadowym przetwarzaniu.\nciekawe informacje\n\nAnaliza danych w czasie rzeczywistym a przetwarzanie strumienia zdarzeń\nŁatwo jest połączyć analizę w czasie rzeczywistym i analizę strumieniową (lub przetwarzanie strumienia zdarzeń). Ale chociaż technologie analizy strumieniowej mogą umożliwiać analizę w czasie rzeczywistym, to nie to samo!\nAnaliza strumieniowa polega na przetwarzaniu danych w ruchu. Analityka w czasie rzeczywistym to dowolna metoda przetwarzania danych, która skutkuje okresem opóźnienia określanym jako „w czasie rzeczywistym”.\nZazwyczaj systemy analizy czasu rzeczywistego są definiowane jako twarde i miękkie systemy czasu rzeczywistego. Niedotrzymanie terminu w twardych systemach czasu rzeczywistego, takich jak samolot, jest katastrofalne, a w miękkich systemach czasu rzeczywistego, takich jak stacja pogodowa, niedotrzymanie terminów może prowadzić do bezużytecznych danych.\nPonadto, podczas gdy analiza strumieniowa implikuje istnienie architektury strumieniowej, analiza w czasie rzeczywistym nie implikuje żadnej konkretnej architektury.\nWszystko, co implikuje analityka w czasie rzeczywistym, polega na tym, że tworzenie i przetwarzanie danych odbywa się w dowolnym czasie, który firma definiuje jako „w czasie rzeczywistym”.\n\n\nŹródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego."
  },
  {
    "objectID": "old_lectures/wyklad2.html#uzasadnienie-biznesowe",
    "href": "old_lectures/wyklad2.html#uzasadnienie-biznesowe",
    "title": "Analiza strumieni danych",
    "section": "Uzasadnienie biznesowe",
    "text": "Uzasadnienie biznesowe\nAnalityka służy do znajdowania znaczących wzorców w danych i odkrywania nowej wiedzy. Dotyczy to zarówno transmisji strumieniowych, jak i tradycyjnych analiz.\nAle w dzisiejszym świecie natura „znajdowania sensownych wzorców w danych” uległa zmianie, ponieważ zmienił się charakter danych. Szybkość, objętość i rodzaje danych eksplodowały.\nTwitter produkuje ponad 500 milionów tweetów dziennie. IDC przewiduje, że do 2025 roku urządzenia Internetu rzeczy (IoT) będą w stanie wygenerować 79,4 zettabajtów (ZB) danych. I te trendy nie wykazują oznak spowolnienia.\nBiorąc pod uwagę nowy charakter danych, główną zaletą analizy strumieniowej jest to, że pomaga ona firmom znajdować znaczące wzorce w danych i odkrywać nową wiedzę ,,w czasie rzeczywistym” lub zbliżonym do rzeczywistego.\n\nktóry pojazd firmowej floty ma prawie pusty bak i gdzie wysłać prowadzącego pojazd do tankowania.\nKtóry pojazd floty zużywa najwięcej paliwa i dlaczego?\nKtóre urządzenia w zakładzie czy fabryce mogą ulec awarii w ciągu najbliższych dni?\nJakie części zamienne trzeba będzie wymienić i w których maszynach w najbliższym czasie ?\nIlu klientów aktualnie robi zakupy w sklepie i czy można im coś zaproponować ?\nCzy klient dzwoni w celu zerwania umowy ?\ni wiele wiele innych.\n\n\nPrzykładowe biznesowe zastosowania\n\nDane z sensorów IoT i detekcja anomalii\nStock Trading (problemy regresyjne) - czas reagowania na zmiany i czas zakupy i sprzedaży akcji.\nClickstream for websites (problem klasyfikacji) - śledzenie i analiza gości na stronie serwisu internetowego - personalizacja strony i treści.\n\n8 najlepszych przykładów analizy w czasie rzeczywistym\nBiznesowe zastosowania"
  },
  {
    "objectID": "old_lectures/wyklad2.html#definicje",
    "href": "old_lectures/wyklad2.html#definicje",
    "title": "Analiza strumieni danych",
    "section": "Definicje",
    "text": "Definicje\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 - Zdarzenie czyli wszystko co możemy zaobserwować w pewnej chwili czasu. Generowane są jako bezpośredni skutek działania.\nDefinicja 2 - W przypadku danych zdarzenie rozumiemy jako niezmienialny rekord w strumieniu danych zakodowany jako JSON, XML, CSV lub binarnie.\nDefinicja 3 - Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie np. logi z urządzenia.\nDefinicja 4 - Strumień danych to dane tworzone przyrostowo w czasie, generowane ze statycznych danych (baza danych, czytanie lini z pliku) bądź w sposób dynamiczny (logi, sensory, funkcje).\n\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów)."
  },
  {
    "objectID": "old_lectures/wyklad2.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "old_lectures/wyklad2.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Analiza strumieni danych",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego przetwarzamy dane historyczne i czas uruchomienia procesu przetwarzania nie ma nic wspólnego z czasem występowania analizowanych zdarzeń.\nDla danych strumieniowych mamy dwie koncepcje czasu:\n\nczas zdarzenia (event time) - czas w którym zdarzenie się wydarzyło.\nczas przetwarzania (processing time) - czas w którym system przetwarza zdarzenie.\n\nW przypadku idealnej sytuacji:\n\nW rzeczywistości przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co reprezentowane jest przez punkty pojawiające się poniżej funkcji dla sytuacji idealnej (poniżej diagonalnej).\n\nW aplikacjach przetwarzania strumieniowego istotne okazują się różnice miedzy czasem powstania zdarzenia i jego procesowania. Do najczęstszych przyczyn opóźnienia wyszczególnia się przesyłanie danych przez sieć czy brak komunikacji między urządzeniem a siecią. Prostym przykładem jest tu przejazd samochodem przez tunel i śledzenie położenia przez aplikację GPS.\nMożesz oczywiście zliczać ilość takich pominiętych zdarzeń i uruchomić alarm w sytuacji gdy takich odrzutów będzie za dużo. Drugim (chyba częściej) wykorzystywanym sposobem jest zastosowanie korekty z wykorzystaniem tzw. watermarkingu.\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić w postaci funkcji schodkowej, reprezentowanej na rysunku: \nJak można zauważyć nie wszystkie zdarzenia wnoszą wkład do analizy i przetwarzania. Realizację procesu przetwarzania wraz z uwzględnieniem dodatkowego czasu na pojawienie się zdarzeń (watermark) można przedstawić jako proces obejmujący wszystkie zdarzenia powyżej przerywanej linii. Dodatkowy czas pozwolił na przetworzenie dodatkowych zdarzeń, natomiast nadal mogą zdarzyć się punkty, które nie będą brane pod uwagę.  \nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie."
  },
  {
    "objectID": "old_lectures/wyklad2.html#okna-czasowe",
    "href": "old_lectures/wyklad2.html#okna-czasowe",
    "title": "Analiza strumieni danych",
    "section": "okna czasowe",
    "text": "okna czasowe\nOkno rozłączne (ang. tumbling window) czyli okno o stałej długości. Jego cechą charakterystyczną jest to, iż każde zdarzenie należy tylko do jednego okna.  \nOkno przesuwne (ang. sliding window) obejmuje wszystkie zdarzenia następujące w określonej długości między sobą.  \nOkno skokowe (ang. hopping window) tak jak okno rozłączne ma stałą długość, ale pozwala się w nim na zachodzenie jednych okien na inne. Stosowane zazwyczaj do wygładzenia danych."
  },
  {
    "objectID": "old_lectures/wyklad2.html#aplikacje-dla-strumieniowania-danych",
    "href": "old_lectures/wyklad2.html#aplikacje-dla-strumieniowania-danych",
    "title": "Analiza strumieni danych",
    "section": "Aplikacje dla strumieniowania danych",
    "text": "Aplikacje dla strumieniowania danych\nAplikacja przetwarzająca strumień zdarzeń powinna umożliwiać przetworzenie i zapisanie zdarzenia oraz dostęp (w tym samym czasie) do innych danych tak by móc dane zdarzenie przetworzyć (wykonać na nim dowolne przeliczenie) i zapisać jako stan lokalny. Stan ten może być zapisywany w wielu miejscach np. zmienne w programie, pliki lokalne, wew i zew bazy danych. Jedną z najbardziej znanych aplikacji tego typu jest Apache Kafka, którą można łączyć np. z Apache Spark bądź Apache Flink.\nPorównanie z aplikacją w trybie batch\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem"
  },
  {
    "objectID": "old_lectures/wyklad4.html",
    "href": "old_lectures/wyklad4.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Architektura przesyłania strumieniowego to określony zestaw technologii, które współpracują ze sobą w celu obsługi przetwarzania strumieniowego, co jest praktyką podejmowania działań na serii danych w momencie ich tworzenia. W wielu nowoczesnych wdrożeniach Apache Kafka działa jako magazyn danych przesyłanych strumieniowo, a następnie wiele procesorów strumieniowych może działać na danych przechowywanych w Kafce w celu wygenerowania wielu danych wyjściowych. Niektóre architektury przesyłania strumieniowego obejmują przepływy pracy zarówno do przetwarzania strumieniowego, jak i przetwarzania wsadowego, które obejmują inne technologie do obsługi przetwarzania wsadowego na dużą skalę lub wykorzystują Kafkę jako magazyn centralny, jak określono w architekturze Kappa.\nDoskonała architektura przetwarzania danych w czasie rzeczywistym musi być odporna na błędy i skalowalna; musi obsługiwać aktualizacje wsadowe i przyrostowe oraz być rozszerzalna.\nNa początku badamy dwie podstawowe architektury przetwarzania danych, Lambda i Kappa, które stanowią podstawę różnych aplikacji korporacyjnych.\n\n\nArchitektura Lambda obejmuje warstwę wsadową (batch layer), warstwę strumieniowa (stream layer) i warstwę serwowania.\nWarstwa wsadowa działa na pełnych danych, dzięki czemu system może generować najdokładniejsze wyniki. Jednak wyniki są okupione dużymi opóźnieniami wynikającymi z długiego czasu obliczeń. Warstwa wsadowa przechowuje surowe dane w miarę ich nadejścia i oblicza widoki wsadowe do wykorzystania. Naturalnie procesy wsadowe będą występować w pewnych odstępach czasu i będą długotrwałe. Zakres danych wynosi od godzin do kilku lat.\nWarstwa strumieniowa:\n\ngeneruje wyniki z małymi opóźnieniami i w czasie zbliżonym do rzeczywistego.\noblicza widoki w czasie rzeczywistym w celu uzupełnienia widoków wsadowych.\nodbiera napływające dane i aktualizuje wyniki warstwy wsadowej. Koszt obliczeń jest znacznie obniżony dzięki algorytmom przyrostowym zaimplementowanym w warstwie szybkości.\n\nWidoki wsadowe mogą być przetwarzane przy użyciu bardziej złożonych lub kosztownych reguł i mogą mieć lepszą jakość danych i mniej przekrzywień, podczas gdy widoki w czasie rzeczywistym zapewniają bieżący dostęp do najnowszych możliwych danych.\nWreszcie warstwa serwująca umożliwia różne zapytania o wyniki przesłane z warstw wsadowych i szybkich. Dane wyjściowe z warstwy wsadowej w postaci widoków wsadowych i warstwy szybkości w postaci opinii w czasie zbliżonym do rzeczywistego są przekazywane do warstwy obsługującej, która wykorzystuje te dane do obsługi oczekujących zapytań na zasadzie ad-hoc.\n  Implementacja:  \nDobre bo:\n\nDobra równowaga między szybkością, niezawodnością i skalowalnością.\nDostęp do wyników zarówno w czasie rzeczywistym, jak i offline, bardzo dobrze pokrywa wiele scenariuszy analizy danych.\nDostęp do pełnego zestawu danych w oknie wsadowym może przynieść określone optymalizacje, które sprawią, że Lambda będzie wydajniejsza i jeszcze prostsza do wdrożenia.\n\nKiepskie gdy:\n\nWewnętrzna logika przetwarzania jest taka sama (warstwy wsadowe i warstwy czasu rzeczywistego) - wiele zduplikowanych modułów i kodowania.\nZbiór danych modelowany za pomocą architektury Lambda jest trudny do migracji i reorganizacji.\n\n\n\n\nArchitektura Kappa to architektura oprogramowania używana do przetwarzania danych przesyłanych strumieniowo. Głównym założeniem Architektury Kappa jest możliwość wykonywania przetwarzania w czasie rzeczywistym i przetwarzania wsadowego, zwłaszcza w celach analitycznych, za pomocą jednego stosu technologicznego. Opiera się na architekturze przesyłania strumieniowego, w której przychodzące serie danych są najpierw przechowywane w silniku przesyłania wiadomości, takim jak Apache Kafka. Stamtąd silnik przetwarzania strumienia odczyta dane, przekształci je w format nadający się do analizy, a następnie zapisze je w analitycznej bazie danych, aby użytkownicy końcowi mogli wyszukiwać.\nArchitektura Kappa obsługuje analizy (prawie) w czasie rzeczywistym, gdy dane są odczytywane i przekształcane natychmiast po umieszczeniu ich w silniku przesyłania komunikatów. Dzięki temu najnowsze dane są szybko dostępne dla zapytań użytkowników końcowych. Obsługuje również analizę historyczną, odczytując zapisane dane przesyłane strumieniowo z mechanizmu przesyłania wiadomości później w sposób wsadowy, aby utworzyć dodatkowe możliwe do analizy dane wyjściowe dla większej liczby typów analiz.\nArchitektura Kappa jest prostszą alternatywą dla architektury Lambda, ponieważ wykorzystuje ten sam stos technologii do obsługi strumienia w czasie rzeczywistym i historycznego przetwarzania wsadowego. Obie architektury obejmują przechowywanie danych historycznych w celu umożliwienia analiz na dużą skalę. Obie architektury są również pomocne w rozwiązywaniu problemów związanych z „tolerancją błędów ludzkich”, w których problemy z kodem przetwarzania (błędy lub znane ograniczenia) można przezwyciężyć, aktualizując kod i ponownie uruchamiając go na danych historycznych. Główna różnica w stosunku do architektury Kappa polega na tym, że wszystkie dane są traktowane jako strumień, więc silnik przetwarzania strumienia działa jako jedyny silnik transformacji danych.\n \nImplementation Example:  \n\n\n\n\nAplikacje mogą odczytywać i zapisywać bezpośrednio do Kafki zgodnie z rozwojem. W przypadku istniejących źródeł zdarzeń detektory są teraz przyzwyczajone do przesyłania strumieniowego raportów z dzienników bazy danych, co eliminuje konieczność przetwarzania wsadowego podczas ruchu przychodzącego, co skutkuje mniejszą liczbą zasobów.\nZapytania muszą uwzględniać tylko jedną lokalizację serwowania, zamiast sprawdzać widoki partii i szybkości.\n\n\n\n\n\nniełatwe do wdrożenia, zwłaszcza w przypadku odtwarzania danych.\n\n\n\n\nObie architektury obsługują analizy w czasie rzeczywistym i historyczne w jednym środowisku. Jednak istotną zaletą architektury Kappa w porównaniu z architekturą Lambda jest to, że umożliwia ona zbudowanie systemu przesyłania strumieniowego i przetwarzania wsadowego na jednej technologii. Oznacza to, że możesz zbudować aplikację przetwarzającą strumienie do obsługi danych w czasie rzeczywistym, a jeśli musisz zmodyfikować dane wyjściowe, zaktualizuj swój kod, a następnie ponownie uruchom go na danych w mechanizmie przesyłania komunikatów w sposób wsadowy. Jak sugeruje architektura Lambda, nie ma osobnej technologii do obsługi przetwarzania wsadowego.\nPrzy wystarczająco szybkim silniku przetwarzania strumieniowego możesz nie potrzebować innej technologii zoptymalizowanej pod kątem przetwarzania wsadowego. Odczytujesz równolegle przechowywane dane przesyłane strumieniowo (zakładając, że dane w Kafce są odpowiednio podzielone na osobne kanały lub „partycje”) i przekształcasz dane tak, jakby pochodziły ze źródła strumieniowego. W przypadku niektórych środowisk możliwe do przeanalizowania dane wyjściowe można utworzyć na żądanie. Gdy nowe zapytanie zostanie przesłane przez użytkownika końcowego, dane mogą zostać przekształcone ad hoc, aby uzyskać optymalną odpowiedź na to zapytanie. Ponownie wymaga to szybkiego silnika przetwarzania strumieniowego, aby zapewnić małe opóźnienia.\nChociaż architektura Lambda nie określa technologii, których należy użyć, komponent przetwarzania wsadowego jest często wykonywany na platformie big data z wykorzystaniem Apache Hadoop. Rozproszony system plików Hadoop (HDFS) umożliwia ekonomiczne przechowywanie „surowych danych”, które można przekształcić za pomocą narzędzi Hadoop w format umożliwiający analizę. Podczas gdy Hadoop jest używany w komponencie systemu do przetwarzania wsadowego, oddzielny silnik przeznaczony do przetwarzania strumieniowego jest używany w komponencie analitycznym w czasie rzeczywistym. Jednak jedną z zalet architektury Lambda jest to, że znacznie większe zestawy danych (w zakresie petabajtów) można przechowywać i przetwarzać wydajniej w Hadoop w celu analizy historycznej na dużą skalę."
  },
  {
    "objectID": "old_lectures/wyklad4.html#architektury-strumieniowania-danych",
    "href": "old_lectures/wyklad4.html#architektury-strumieniowania-danych",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Architektura przesyłania strumieniowego to określony zestaw technologii, które współpracują ze sobą w celu obsługi przetwarzania strumieniowego, co jest praktyką podejmowania działań na serii danych w momencie ich tworzenia. W wielu nowoczesnych wdrożeniach Apache Kafka działa jako magazyn danych przesyłanych strumieniowo, a następnie wiele procesorów strumieniowych może działać na danych przechowywanych w Kafce w celu wygenerowania wielu danych wyjściowych. Niektóre architektury przesyłania strumieniowego obejmują przepływy pracy zarówno do przetwarzania strumieniowego, jak i przetwarzania wsadowego, które obejmują inne technologie do obsługi przetwarzania wsadowego na dużą skalę lub wykorzystują Kafkę jako magazyn centralny, jak określono w architekturze Kappa.\nDoskonała architektura przetwarzania danych w czasie rzeczywistym musi być odporna na błędy i skalowalna; musi obsługiwać aktualizacje wsadowe i przyrostowe oraz być rozszerzalna.\nNa początku badamy dwie podstawowe architektury przetwarzania danych, Lambda i Kappa, które stanowią podstawę różnych aplikacji korporacyjnych.\n\n\nArchitektura Lambda obejmuje warstwę wsadową (batch layer), warstwę strumieniowa (stream layer) i warstwę serwowania.\nWarstwa wsadowa działa na pełnych danych, dzięki czemu system może generować najdokładniejsze wyniki. Jednak wyniki są okupione dużymi opóźnieniami wynikającymi z długiego czasu obliczeń. Warstwa wsadowa przechowuje surowe dane w miarę ich nadejścia i oblicza widoki wsadowe do wykorzystania. Naturalnie procesy wsadowe będą występować w pewnych odstępach czasu i będą długotrwałe. Zakres danych wynosi od godzin do kilku lat.\nWarstwa strumieniowa:\n\ngeneruje wyniki z małymi opóźnieniami i w czasie zbliżonym do rzeczywistego.\noblicza widoki w czasie rzeczywistym w celu uzupełnienia widoków wsadowych.\nodbiera napływające dane i aktualizuje wyniki warstwy wsadowej. Koszt obliczeń jest znacznie obniżony dzięki algorytmom przyrostowym zaimplementowanym w warstwie szybkości.\n\nWidoki wsadowe mogą być przetwarzane przy użyciu bardziej złożonych lub kosztownych reguł i mogą mieć lepszą jakość danych i mniej przekrzywień, podczas gdy widoki w czasie rzeczywistym zapewniają bieżący dostęp do najnowszych możliwych danych.\nWreszcie warstwa serwująca umożliwia różne zapytania o wyniki przesłane z warstw wsadowych i szybkich. Dane wyjściowe z warstwy wsadowej w postaci widoków wsadowych i warstwy szybkości w postaci opinii w czasie zbliżonym do rzeczywistego są przekazywane do warstwy obsługującej, która wykorzystuje te dane do obsługi oczekujących zapytań na zasadzie ad-hoc.\n  Implementacja:  \nDobre bo:\n\nDobra równowaga między szybkością, niezawodnością i skalowalnością.\nDostęp do wyników zarówno w czasie rzeczywistym, jak i offline, bardzo dobrze pokrywa wiele scenariuszy analizy danych.\nDostęp do pełnego zestawu danych w oknie wsadowym może przynieść określone optymalizacje, które sprawią, że Lambda będzie wydajniejsza i jeszcze prostsza do wdrożenia.\n\nKiepskie gdy:\n\nWewnętrzna logika przetwarzania jest taka sama (warstwy wsadowe i warstwy czasu rzeczywistego) - wiele zduplikowanych modułów i kodowania.\nZbiór danych modelowany za pomocą architektury Lambda jest trudny do migracji i reorganizacji.\n\n\n\n\nArchitektura Kappa to architektura oprogramowania używana do przetwarzania danych przesyłanych strumieniowo. Głównym założeniem Architektury Kappa jest możliwość wykonywania przetwarzania w czasie rzeczywistym i przetwarzania wsadowego, zwłaszcza w celach analitycznych, za pomocą jednego stosu technologicznego. Opiera się na architekturze przesyłania strumieniowego, w której przychodzące serie danych są najpierw przechowywane w silniku przesyłania wiadomości, takim jak Apache Kafka. Stamtąd silnik przetwarzania strumienia odczyta dane, przekształci je w format nadający się do analizy, a następnie zapisze je w analitycznej bazie danych, aby użytkownicy końcowi mogli wyszukiwać.\nArchitektura Kappa obsługuje analizy (prawie) w czasie rzeczywistym, gdy dane są odczytywane i przekształcane natychmiast po umieszczeniu ich w silniku przesyłania komunikatów. Dzięki temu najnowsze dane są szybko dostępne dla zapytań użytkowników końcowych. Obsługuje również analizę historyczną, odczytując zapisane dane przesyłane strumieniowo z mechanizmu przesyłania wiadomości później w sposób wsadowy, aby utworzyć dodatkowe możliwe do analizy dane wyjściowe dla większej liczby typów analiz.\nArchitektura Kappa jest prostszą alternatywą dla architektury Lambda, ponieważ wykorzystuje ten sam stos technologii do obsługi strumienia w czasie rzeczywistym i historycznego przetwarzania wsadowego. Obie architektury obejmują przechowywanie danych historycznych w celu umożliwienia analiz na dużą skalę. Obie architektury są również pomocne w rozwiązywaniu problemów związanych z „tolerancją błędów ludzkich”, w których problemy z kodem przetwarzania (błędy lub znane ograniczenia) można przezwyciężyć, aktualizując kod i ponownie uruchamiając go na danych historycznych. Główna różnica w stosunku do architektury Kappa polega na tym, że wszystkie dane są traktowane jako strumień, więc silnik przetwarzania strumienia działa jako jedyny silnik transformacji danych.\n \nImplementation Example:  \n\n\n\n\nAplikacje mogą odczytywać i zapisywać bezpośrednio do Kafki zgodnie z rozwojem. W przypadku istniejących źródeł zdarzeń detektory są teraz przyzwyczajone do przesyłania strumieniowego raportów z dzienników bazy danych, co eliminuje konieczność przetwarzania wsadowego podczas ruchu przychodzącego, co skutkuje mniejszą liczbą zasobów.\nZapytania muszą uwzględniać tylko jedną lokalizację serwowania, zamiast sprawdzać widoki partii i szybkości.\n\n\n\n\n\nniełatwe do wdrożenia, zwłaszcza w przypadku odtwarzania danych.\n\n\n\n\nObie architektury obsługują analizy w czasie rzeczywistym i historyczne w jednym środowisku. Jednak istotną zaletą architektury Kappa w porównaniu z architekturą Lambda jest to, że umożliwia ona zbudowanie systemu przesyłania strumieniowego i przetwarzania wsadowego na jednej technologii. Oznacza to, że możesz zbudować aplikację przetwarzającą strumienie do obsługi danych w czasie rzeczywistym, a jeśli musisz zmodyfikować dane wyjściowe, zaktualizuj swój kod, a następnie ponownie uruchom go na danych w mechanizmie przesyłania komunikatów w sposób wsadowy. Jak sugeruje architektura Lambda, nie ma osobnej technologii do obsługi przetwarzania wsadowego.\nPrzy wystarczająco szybkim silniku przetwarzania strumieniowego możesz nie potrzebować innej technologii zoptymalizowanej pod kątem przetwarzania wsadowego. Odczytujesz równolegle przechowywane dane przesyłane strumieniowo (zakładając, że dane w Kafce są odpowiednio podzielone na osobne kanały lub „partycje”) i przekształcasz dane tak, jakby pochodziły ze źródła strumieniowego. W przypadku niektórych środowisk możliwe do przeanalizowania dane wyjściowe można utworzyć na żądanie. Gdy nowe zapytanie zostanie przesłane przez użytkownika końcowego, dane mogą zostać przekształcone ad hoc, aby uzyskać optymalną odpowiedź na to zapytanie. Ponownie wymaga to szybkiego silnika przetwarzania strumieniowego, aby zapewnić małe opóźnienia.\nChociaż architektura Lambda nie określa technologii, których należy użyć, komponent przetwarzania wsadowego jest często wykonywany na platformie big data z wykorzystaniem Apache Hadoop. Rozproszony system plików Hadoop (HDFS) umożliwia ekonomiczne przechowywanie „surowych danych”, które można przekształcić za pomocą narzędzi Hadoop w format umożliwiający analizę. Podczas gdy Hadoop jest używany w komponencie systemu do przetwarzania wsadowego, oddzielny silnik przeznaczony do przetwarzania strumieniowego jest używany w komponencie analitycznym w czasie rzeczywistym. Jednak jedną z zalet architektury Lambda jest to, że znacznie większe zestawy danych (w zakresie petabajtów) można przechowywać i przetwarzać wydajniej w Hadoop w celu analizy historycznej na dużą skalę."
  },
  {
    "objectID": "old_lectures/wyklad4.html#publikujsubskrybuj",
    "href": "old_lectures/wyklad4.html#publikujsubskrybuj",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyłania wiadomości „Publikuj/Subskrybuj” ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujący się tym, że nadawca (publikujący) fragmentu danych (wiadomości) nie kieruje go wprost do odbiorcy. pub/sub to systemy, które często posiadają brokera czyli centralny punkt, w którym znajdują się wiadomości."
  },
  {
    "objectID": "old_lectures/wyklad4.html#apache-kafka",
    "href": "old_lectures/wyklad4.html#apache-kafka",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicję:\n\nRozproszona platforma streamingowa\n\nCo to jest „platforma rozproszonego przesyłania strumieniowego”?\nNajpierw chcę przypomnieć, czym jest „strumień”. Strumienie to po prostu nieograniczone dane, dane, które nigdy się nie kończą. Ciągle ich przybywa i możesz przetwarzać je w czasie rzeczywistym.\nA „rozproszone”? Rozproszony oznacza, że ​​Kafka działa w klastrze, a każdy węzeł w grupie nazywa się Brokerem. Ci brokerzy to po prostu serwery wykonujące kopię Apache Kafka.\nTak więc Kafka to zestaw współpracujących ze sobą maszyn, aby móc obsługiwać i przetwarzać nieograniczone dane w czasie rzeczywistym.\nJego rozproszona architektura jest jednym z powodów, dla których Kafka stał się tak sławny. Brokerzy sprawiają, że jest odporny, niezawodny, skalowalny i odporny na błędy. Ale dlaczego panuje błędne przekonanie, że Kafka to kolejny „kolejkowy system przesyłania wiadomości”?\nAby odpowiedzieć na tę odpowiedź, musimy najpierw wyjaśnić, jak działa kolejkowe przesyłanie wiadomości.\n\nKolejkowy system przesyłania wiadomości\nPrzesyłanie wiadomości, to po prostu czynność wysyłania wiadomości z jednego miejsca do drugiego. Ma trzech głównych “aktorów”:\n\nProducent: Który tworzy i wysyła komunikaty do jednej lub więcej kolejek;\nKolejka: struktura danych bufora, która odbiera (od producentów) i dostarcza komunikaty (do konsumentów) w sposób FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jedną lub więcej kolejek i otrzymuje ich wiadomości po opublikowaniu.\n\nI to jest to; tak działa przesyłanie wiadomości. Jak widać, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Kafki\nDużo informacji znajdziesz pod tym linkiem.\nTeraz, gdy wiemy, jak działa przesyłanie wiadomości, zanurzmy się w świat Kafki. W Kafce mamy też „Producentów” i „Konsumentów”; działają w bardzo podobny sposób, jak w kolejkowych systemach, produkując i konsumując komunikaty.\n \nJak widać, jest to bardzo podobne do tego, o czym rozmawialiśmy o przesyłaniu wiadomości, ale tutaj nie mamy pojęcia „kolejki”. Zamiast tego mamy „Tematy” (Topic).\n„Temat” to szczególny typ strumienia danych; jest bardzo podobny do kolejki, odbiera i dostarcza wiadomości, ale jest kilka pojęć, które musimy zrozumieć w odniesieniu do tematów:\n\nTemat jest podzielony na partycje; każdy temat może mieć jedną lub więcej partycji i musimy określić tę liczbę podczas tworzenia topicu. Możesz sobie wyobrazić topic jako folder w systemie operacyjnym, a każdy folder wewnątrz niego jako partycję.\nKażda wiadomość zostanie zapisana na dysku brokera i otrzyma offset (unikalny identyfikator). Offset jest unikalny na poziomie partycji; każda partycja ma swój własny zbiór offsetów. To jeszcze jeden powód, który sprawia, że ​​Kafka jest tak wyjątkowa, przechowuje wiadomości na dysku (jak baza danych, a w rzeczywistości Kafka też jest bazą danych), aby w razie potrzeby odzyskać je później. W odróżnieniu od systemu przesyłania wiadomości, gdzie wiadomość jest usuwana po zużyciu;\nKosumenci używają offsetu do czytania wiadomości, od najstarszej do najnowszej. W przypadku awarii konsumenta zacznie odczytywać końcowe wartości realizowane po nawiązaniu połączenia.\n\n \n\n\nBrokerzy\nJak wspomniano wcześniej, Kafka działa w sposób rozproszony. W razie potrzeby klaster Kafka może zawierać wielu brokerów.\n \nKażdy broker w klastrze jest identyfikowany przez identyfikator i zawiera co najmniej jedną partycję tematyczną. Aby skonfigurować liczbę partycji w każdym brokerze, podczas tworzenia tematu musimy skonfigurować coś, co nazywa się współczynnikiem replikacji. Powiedzmy, że mamy trzech brokerów w naszym klastrze, temat z trzema partycjami i współczynnikiem replikacji równym trzy; w takim przypadku każdy broker będzie odpowiedzialny za jedną sekcję emisji.\nJak widać na powyższym obrazku, \\(Topic_1\\) ma trzy partycje; każdy broker jest odpowiedzialny za sekcję tematu, więc współczynnik replikacji \\(Topic_1\\) wynosi trzy. Liczba partycji musi być zgodna z liczbą brokerów; w ten sposób każdy broker będzie odpowiedzialny za jedną sekcję tematu.\n\n\nProducenci\nPodobnie jak w świecie kolejkowych systemów, „Producenci” w Kafce to ci, którzy tworzą i wysyłają wiadomości do tematów. Jak wspomniano wcześniej, wiadomości są wysyłane w sposób okrężny. Przykład: wiadomość 01 trafia do partycji 0 tematu 1, a wiadomość 02 do partycji 1 tego samego tematu. Oznacza to, że nie możemy zagwarantować, że wiadomości stworzone przez tego samego producenta zawsze będą dostarczane w tym samym numerze. Podczas wysyłania wiadomości musimy określić klucz; Kafka wygeneruje skrót na podstawie tego klucza i będzie wiedział, która partycja ma dostarczyć tę wiadomość. Ten skrót uwzględnia liczbę partycji tematu; dlatego tego numeru nie można zmienić, gdy temat jest już utworzony.\n\n\nKonsumenci i grupy konsumentów\nKonsumenci to aplikacje zasubskrybowane do jednego lub więcej tematów, które będą odczytywać wiadomości stamtąd. Mogą czytać z jednej lub więcej partycji. Gdy konsument odczytuje tylko z jednej partycji, możemy zapewnić kolejność odczytu, ale gdy pojedynczy konsument odczytuje z dwóch lub więcej partycji, będzie czytać równolegle, więc nie ma gwarancji kolejności odczytu. Na przykład wiadomość, która przyszła później, może zostać odczytana przed inną, która przyszła wcześniej. Dlatego musimy być ostrożni przy wyborze liczby partycji i podczas tworzenia wiadomości.\nInnym ważnym pojęciem Kafki są „Grupy konsumentów”. Jest to bardzo ważne, gdy musimy skalować odczytywanie wiadomości. Staje się to bardzo kosztowne, gdy pojedynczy konsument musi czytać z wielu partycji, więc musimy zrównoważyć obciążenie między naszymi konsumentami, wtedy wchodzą grupy konsumentów.\nDane z jednego tematu będą równoważone obciążeniem między konsumentami, dzięki czemu możemy zagwarantować, że nasi konsumenci będą w stanie obsługiwać i przetwarzać dane. Ideałem jest posiadanie takiej samej liczby konsumentów w grupie, jaką mamy jako partycje w temacie, w ten sposób każdy konsument czyta tylko z jednego. Podczas dodawania konsumentów do grupy należy uważać, jeśli liczba konsumentów jest większa niż liczba partycji, niektórzy konsumenci nie będą czytać z żadnego tematu i pozostaną bezczynni."
  },
  {
    "objectID": "old_lectures/wyklad2S.html",
    "href": "old_lectures/wyklad2S.html",
    "title": "Wykład 2",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "old_lectures/wyklad2S.html#batch-vs-stream-processing",
    "href": "old_lectures/wyklad2S.html#batch-vs-stream-processing",
    "title": "Wykład 2",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "old_lectures/wyklad2S.html#strumienie-danych",
    "href": "old_lectures/wyklad2S.html#strumienie-danych",
    "title": "Wykład 2",
    "section": "Strumienie danych",
    "text": "Strumienie danych\nStrumieniowanie możesz kojarzyć z serwisów przesyłających video w trybie online. Gdy oglądasz swój ulubiony serial (tak jak teraz na zajęciach) serwis odpowiadający za strumieniowanie w nieprzerwany sposób przesyła do ciebie kolejne “porcje” video. Identycznie koncepcja ta realizowana jest w przypadku danych strumieniowych. Format przesyłanych porcji nie musi być plikiem video, wszystko zależy od celu realizowanego biznesowo. Np. ciągły pomiar z różnego rodzaju czujników w farbykach, elektrowniach itp. Warto odnotować, że masz do czynienia z ciągłym strumieniem danych, które przetwarzać musisz w czasie rzeczywistym. Nie możesz czekać do zatrzymania linii produkcyjnych w celu wykonania analizy, wszystkie pojawiające się problemy chcesz rejestrować natychmiast i jak najszybciej na nie reagować.\n\nAnaliza strumieni danych to ciągłe przetwarzanie i analiza dużych zbiorów danych w ruchu.\n\nPorównuj to do wsakazanych powyżej elementów Big Data. Przetwarzanie Batchowe jest przeciwieństwem do przetwarzania strumieniowego. Najpierw zbierasz duże ilości danych a potem realizujesz analizy. Możesz oczywiście zawsze pobrać video w całości zanim je obejrzysz, ale czy miałoby to sens? Istnieją przypadki gdy takie podejście nie stanowi problemu, ale już tu widzisz, że przetwarzanie strumieniowe może przynieść dla biznesu dodatkowe wartości dodane, których trudno oczekiwać przy wsadowym przetwarzaniu.\nciekawe informacje\n\nŹródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego.\n\n\nPrzykładowe biznesowe zastosowania\n\nDane z sensorów IoT i detekcja anomalii\nStock Trading (problemy regresyjne) - czas reagowania na zmiany i czas zakupy i sprzedaży akcji.\nClickstream for websites (problem klasyfikacji) - śledzenie i analiza gości na stronie serwisu internetowego - personalizacja strony i treści.\n\n8 najlepszych przykładów analizy w czasie rzeczywistym\nBiznesowe zastosowania\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów)."
  },
  {
    "objectID": "old_lectures/wyklad2S.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "old_lectures/wyklad2S.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Wykład 2",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego przetwarzamy dane historyczne i czas uruchomienia procesu przetwarzania nie ma nic wspólnego z czasem występowania analizowanych zdarzeń.\nDla danych strumieniowych mamy dwie koncepcje czasu:\n\nczas zdarzenia (event time) - czas w którym zdarzenie się wydarzyło.\nczas przetwarzania (processing time) - czas w którym system przetwarza zdarzenie.\n\nW przypadku idealnej sytuacji:\n\nW rzeczywistości przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co reprezentowane jest przez punkty pojawiające się poniżej funkcji dla sytuacji idealnej (poniżej diagonalnej).\n\nW aplikacjach przetwarzania strumieniowego istotne okazują się różnice miedzy czasem powstania zdarzenia i jego procesowania. Do najczęstszych przyczyn opóźnienia wyszczególnia się przesyłanie danych przez sieć czy brak komunikacji między urządzeniem a siecią. Prostym przykładem jest tu przejazd samochodem przez tunel i śledzenie położenia przez aplikację GPS.\nMożesz oczywiście zliczać ilość takich pominiętych zdarzeń i uruchomić alarm w sytuacji gdy takich odrzutów będzie za dużo. Drugim (chyba częściej) wykorzystywanym sposobem jest zastosowanie korekty z wykorzystaniem tzw. watermarkingu.\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić w postaci funkcji schodkowej, reprezentowanej na rysunku: \nJak można zauważyć nie wszystkie zdarzenia wnoszą wkład do analizy i przetwarzania. Realizację procesu przetwarzania wraz z uwzględnieniem dodatkowego czasu na pojawienie się zdarzeń (watermark) można przedstawić jako proces obejmujący wszystkie zdarzenia powyżej przerywanej linii. Dodatkowy czas pozwolił na przetworzenie dodatkowych zdarzeń, natomiast nadal mogą zdarzyć się punkty, które nie będą brane pod uwagę.  \nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie."
  },
  {
    "objectID": "old_lectures/wyklad2S.html#okna-czasowe",
    "href": "old_lectures/wyklad2S.html#okna-czasowe",
    "title": "Wykład 2",
    "section": "okna czasowe",
    "text": "okna czasowe\nOkno rozłączne (ang. tumbling window) czyli okno o stałej długości. Jego cechą charakterystyczną jest to, iż każde zdarzenie należy tylko do jednego okna.  \nOkno przesuwne (ang. sliding window) obejmuje wszystkie zdarzenia następujące w określonej długości między sobą.  \nOkno skokowe (ang. hopping window) tak jak okno rozłączne ma stałą długość, ale pozwala się w nim na zachodzenie jednych okien na inne. Stosowane zazwyczaj do wygładzenia danych.  \nKomunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe i big data znacząco zmieniły sposób budowania systemów informatycznych i wykonywnia na niach pracy.\nPorównaj to jak “narzędzia” do realizacji przekazu (gazeta, radio, telewizja, internet, komunikatory, media społecznościowe) zmieniły interakcje międzyludzkie i struktury społeczne.\n\nKażde nowe informatyczne medium zmieniło stosunek ludzi do informatyki.\n\nKoncepcja mikrousługi (mikroserwisu) jest bardzo popularnym sposobem budowania systemów informatycznych jak i koncepcją przy tworzeniu oprogramowania czy realizacji firmy w duchu Data-Driven. Koncepcja ta pozwala zachować wydajność (rób jedną rzecz ale dobrze), elastyczność i jasną postać całej struktury.\nChociaż istnieją inne sposoby architektury projektów oprogramowania, „mikroserwisy” są często używane nie bez powodu. Idea mikroserwisów tkwi w nazwie: oprogramowanie jest reprezentowane jako wiele małych usług, które działają indywidualnie. Patrząc na ogólną architekturę, każda mikrousługa znajduje się w małej czarnej skrzynce z jasno zdefiniowanymi wejściami i wyjściami. Możesz porównać tego typu zachowanie do “czystej funkcji” w programowaniu funkcyjnym.\nW celu umożliwienia komunikacji różnych mikroserwisów często wybieranym rozwiązaniem jest wykorzystanie Application Programming Interfaces API .\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API. API to część, która pozwala na połączenie dwóch mikroserwisów. Interfejsy API są bardzo podobne do stron internetowych. Podobnie jak strona internetowa, serwer wysyła do Ciebie kod reprezentujący stronę internetową. Twoja przeglądarka internetowa interpretuje ten kod i wyświetla stronę internetową.\nWeźmy przypadek biznesowy z modelem ML jako usługą. Załóżmy, że pracujesz dla firmy sprzedającej mieszkania w Bostonie. Chcesz zwiększać sprzedaż i oferować naszym klientom lepszą jakość usług dzięki nowej aplikacji mobilnej, z której może korzystać nawet 1 000 000 osób jednocześnie. Możemy to osiągnąć, udostępniając prognozę wartości domu, gdy użytkownik prosi o wycenę przez Internet.\n\nCzym jest serwowanie modelu ML\n\nSzkolenie dobrego modelu ML to TYLKO pierwsza część całego procesu: Musisz udostępnić swój model użytkownikom końcowym. Robisz to, zapewniając dostęp do modelu na swoim serwerze.\nAby udostępnić model potrzebujesz: modelu, interpretera, danych wsadowych.\nWażne metryki\n\n\nczas oczekiwania,\nkoszty,\nliczba zapytać w jednostce czasu\n\n\nUdostępnianie danych między dwoma lub więcej systemami zawsze było podstawowym wymogiem tworzenia oprogramowania – DevOps vs. MLOps.\n\nGdy wywołasz interfejs API, otrzyma on Twoje żądanie. Żądanie wyzwala kod do uruchomienia na serwerze i generuje odpowiedź odesłaną do Ciebie. Jeśli coś pójdzie nie tak, możesz nie otrzymać żadnej odpowiedzi lub otrzymać kod błędu jako kod stanu HTTP.\n\nKlient-Serwer: Klient (system A) przesyła żądanie przez HTTP do adresu URL hostowanego przez system B, który zwraca odpowiedź. Identycznie działa np przeglądarka internetowa. Żądanie jest kierowane do serwera WWW, który zwraca tekstową stronę HTML.\n\n\nBezstanowe: Żądanie klienta powinno zawierać wszystkie informacje niezbędne do udzielenia pełnej odpowiedzi.\n\nInterfejsy API można wywoływać za pomocą wielu różnych narzędzi. Czasami możesz nawet użyć przeglądarki internetowej. Narzędzia takie jak CURL wykonują zadanie w wierszu poleceń. Możesz używać narzędzi, takich jak Postman, do wywoływania interfejsów API za pomocą interfejsu użytkownika.\n\nCała komunikacja jest objęta ustalonymi zasadami i praktykami, które są nazywane protokołem HTTP.\n\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera: - domenę, - port, - dodatkowe ścieżki, - zapytanie\nMetody HTTP: - GET, - POST\nNagłówki HTTP zawierają: - informacje o autoryzacji, - cookies metadata Cała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens\nCiało zapytania\n\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code: • 200 OK - prawidłowe wykonanie zapytania, • 40X Access Denied • 50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.\n\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem"
  },
  {
    "objectID": "plan_wyklady.html",
    "href": "plan_wyklady.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#plan-wykładu",
    "href": "plan_wyklady.html#plan-wykładu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  }
]