[
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Lecture 1: Introduction to Real-Time Data Analysis\nLecture 2: Data Ingestion and Processing for Real-Time Analysis\nLecture 3: Real-Time Data Analysis Techniques\nLecture 4: Real-Time Data Visualization and Communication\nLecture 5: Case Studies and Implementation\nThroughout the lectures, I’ll incorporate interactive elements, such as:\nThis comprehensive lecture plan will provide students with a solid foundation in real-time data analysis, covering the technical aspects of data ingestion, processing, and visualization, as well as practical applications and best practices."
  },
  {
    "objectID": "plan.html#nowy-program-przedmiotu",
    "href": "plan.html#nowy-program-przedmiotu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Nowy program przedmiotu",
    "text": "Nowy program przedmiotu\n\nBatch vs. Real-Time vs. Streaming Analytics – Różnice między trybami przetwarzania danych, kluczowe koncepcje i zastosowania.\nModele przetwarzania danych w Big Data – Od plików płaskich do Data Lake, wady i zalety podejścia real-time. Mity i fakty o przetwarzaniu w czasie rzeczywistym.\nArchitektura IT dla przetwarzania w czasie rzeczywistym – Omówienie architektur Lambda i Kappa w kontekście strumieniowego przetwarzania danych.\nSystemy przetwarzania danych w czasie rzeczywistym – Przegląd technologii: Apache Kafka, Apache Spark Streaming, Apache Flink i ich zastosowania w Pythonie.\nPodstawy uczenia maszynowego w czasie rzeczywistym – Porównanie offline learning vs. online learning, problemy związane z przyrostowym uczeniem maszynowym.\n\n🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej?\n2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce.\n3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym.\n4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów.\n5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych."
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli III bud G\n\n\n22-02-2025 (sobota) 08:00-09:30 - Wykład 1\n\n\n08-03-2025 (sobota) 08:00-09:30 - Wykład 2\n\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć) 20 pytań.\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "wyklad5.html",
    "href": "wyklad5.html",
    "title": "Wykład 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiązanie problemu biznesowego, warto zastanowić się nad złożonością Twojego problemu.\n\n\n\nAlgorytmy przetwarzające duże ilości danych\n\nPrzetwarzanie ogromnych zbiorów danych wymaga odpowiedniego podejścia do ich organizacji i analizy. W sytuacji, gdy ilość danych przekracza dostępną pamięć jednostki obliczeniowej, często stosuje się iteracyjne sposoby ich przetwarzania.\n🔹 Przykład: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o użytkownikach, ich historii zakupów i oglądanych treści.\nPrzetwarza dane w sposób iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji użytkownika.\n\n🔹 Inne zastosowania: - Analiza logów serwerowych w czasie rzeczywistym (np. wykrywanie ataków DDoS). - Monitoring sieci IoT (np. analiza danych z sensorów w inteligentnym mieście).\n\nAlgorytmy dokonujące wielu obliczeń\n\nWymagają dużej mocy obliczeniowej, ale zazwyczaj nie operują na wielkich zbiorach danych. Przykładem może być algorytm wyszukujący dużą liczbę pierwszą. Często wykorzystuje się tutaj podział obliczeń na równoległe procesy w celu optymalizacji wydajności.\n🔹 Przykład: Kryptografia i znalezienie dużej liczby pierwszej (np. RSA) - Algorytm generuje bardzo duże liczby pierwsze, które są podstawą dla szyfrowania RSA. - Proces wymaga intensywnych obliczeń, ale nie operuje na ogromnych zbiorach danych. - Często wykorzystywane są metody równoległe, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszości.\n🔹 Inne zastosowania: - Symulacje fizyczne (np. prognozowanie pogody, modele klimatyczne). - Algorytmy optymalizacyjne (np. znajdowanie najkrótszej trasy w problemie komiwojażera).\n\nAlgorytmy przetwarzające duże ilości danych i dokonujące wielu obliczeń\n\nŁączą wymagania obu poprzednich typów, potrzebując zarówno dużych zasobów obliczeniowych, jak i obsługi dużych zbiorów danych. Przykładem może być analiza sentymentu w transmisjach wideo na żywo.\n🔹 Przykład: Analiza sentymentu w transmisjach wideo na żywo (np. YouTube, Twitch) - Algorytm analizuje zarówno tekst (czat), jak i obraz/wideo w czasie rzeczywistym. - Wymaga zarówno dużych zasobów obliczeniowych (przetwarzanie NLP i CV), jak i obsługi dużej ilości danych. - Może wykorzystywać modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dźwięku.\n🔹 Inne zastosowania: - Autonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym). - Wyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\nAby określić wymiar danych problemu, nie wystarczy podać jedynie ilości miejsca zajmowanego przez dane. Istotne są trzy główne aspekty:\n\nRozmiar wejścia – oczekiwany rozmiar danych do przetwarzania.\nSzybkość narastania – tempo generowania nowych danych podczas działania algorytmu.\nRóżnorodność struktury – typy danych, jakie algorytm musi obsłużyć.\n\n\n\n\nDotyczy zasobów procesowania i mocy obliczeniowej. Na przykład algorytmy uczenia głębokiego (DL) wymagają dużej mocy obliczeniowej, dlatego warto zapewnić zrównolegloną architekturę, wykorzystującą GPU lub TPU, co znacząco przyspiesza obliczenia."
  },
  {
    "objectID": "wyklad5.html#algorytmy",
    "href": "wyklad5.html#algorytmy",
    "title": "Wykład 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiązanie problemu biznesowego, warto zastanowić się nad złożonością Twojego problemu.\n\n\n\nAlgorytmy przetwarzające duże ilości danych\n\nPrzetwarzanie ogromnych zbiorów danych wymaga odpowiedniego podejścia do ich organizacji i analizy. W sytuacji, gdy ilość danych przekracza dostępną pamięć jednostki obliczeniowej, często stosuje się iteracyjne sposoby ich przetwarzania.\n🔹 Przykład: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o użytkownikach, ich historii zakupów i oglądanych treści.\nPrzetwarza dane w sposób iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji użytkownika.\n\n🔹 Inne zastosowania: - Analiza logów serwerowych w czasie rzeczywistym (np. wykrywanie ataków DDoS). - Monitoring sieci IoT (np. analiza danych z sensorów w inteligentnym mieście).\n\nAlgorytmy dokonujące wielu obliczeń\n\nWymagają dużej mocy obliczeniowej, ale zazwyczaj nie operują na wielkich zbiorach danych. Przykładem może być algorytm wyszukujący dużą liczbę pierwszą. Często wykorzystuje się tutaj podział obliczeń na równoległe procesy w celu optymalizacji wydajności.\n🔹 Przykład: Kryptografia i znalezienie dużej liczby pierwszej (np. RSA) - Algorytm generuje bardzo duże liczby pierwsze, które są podstawą dla szyfrowania RSA. - Proces wymaga intensywnych obliczeń, ale nie operuje na ogromnych zbiorach danych. - Często wykorzystywane są metody równoległe, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszości.\n🔹 Inne zastosowania: - Symulacje fizyczne (np. prognozowanie pogody, modele klimatyczne). - Algorytmy optymalizacyjne (np. znajdowanie najkrótszej trasy w problemie komiwojażera).\n\nAlgorytmy przetwarzające duże ilości danych i dokonujące wielu obliczeń\n\nŁączą wymagania obu poprzednich typów, potrzebując zarówno dużych zasobów obliczeniowych, jak i obsługi dużych zbiorów danych. Przykładem może być analiza sentymentu w transmisjach wideo na żywo.\n🔹 Przykład: Analiza sentymentu w transmisjach wideo na żywo (np. YouTube, Twitch) - Algorytm analizuje zarówno tekst (czat), jak i obraz/wideo w czasie rzeczywistym. - Wymaga zarówno dużych zasobów obliczeniowych (przetwarzanie NLP i CV), jak i obsługi dużej ilości danych. - Może wykorzystywać modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dźwięku.\n🔹 Inne zastosowania: - Autonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym). - Wyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\nAby określić wymiar danych problemu, nie wystarczy podać jedynie ilości miejsca zajmowanego przez dane. Istotne są trzy główne aspekty:\n\nRozmiar wejścia – oczekiwany rozmiar danych do przetwarzania.\nSzybkość narastania – tempo generowania nowych danych podczas działania algorytmu.\nRóżnorodność struktury – typy danych, jakie algorytm musi obsłużyć.\n\n\n\n\nDotyczy zasobów procesowania i mocy obliczeniowej. Na przykład algorytmy uczenia głębokiego (DL) wymagają dużej mocy obliczeniowej, dlatego warto zapewnić zrównolegloną architekturę, wykorzystującą GPU lub TPU, co znacząco przyspiesza obliczenia."
  },
  {
    "objectID": "wyklad5.html#wyjaśnialność-algorytmów",
    "href": "wyklad5.html#wyjaśnialność-algorytmów",
    "title": "Wykład 5",
    "section": "Wyjaśnialność algorytmów",
    "text": "Wyjaśnialność algorytmów\nW wielu przypadkach modelowanie jest wykorzystywane w sytuacjach krytycznych, np. w oprogramowaniu do podawania leków. W takich sytuacjach kluczowe staje się wyjaśnienie przyczyny każdego wyniku działania algorytmu. Jest to konieczne, aby zapewnić, że decyzje podejmowane na jego podstawie są wolne od błędów i uprzedzeń.\nZdolność algorytmu do wskazania mechanizmów generujących wyniki nazywamy możliwością wyjaśnienia. Analiza etyczna stanowi standardowy element procesu walidacji algorytmu.\nUzyskanie wysokiej wyjaśnialności jest szczególnie trudne w przypadku algorytmów uczenia maszynowego (ML) i głębokiego uczenia (DL). Na przykład banki korzystające z algorytmów do podejmowania decyzji kredytowych muszą zapewnić transparentność i wskazać powody wydanej decyzji.\nJedną z metod poprawy wyjaśnialności algorytmów jest LIME (Local Interpretable Model-Agnostic Explanations), opublikowana w 2016 roku. Metoda ta polega na wprowadzaniu niewielkich zmian w danych wejściowych i analizowaniu ich wpływu na wynik, co pozwala określić lokalne zasady podejmowania decyzji przez model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podział na zbiór treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykładu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wybór losowego przykładu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wyświetlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nJak działa ten kod? 1. Ładowanie danych i trenowanie modelu - Używamy zbioru Iris, który zawiera 150 przykładów kwiatów z trzema gatunkami: - Setosa - Versicolor - Virginica - Model RandomForestClassifier trenuje się na tych danych.\n\nTworzenie interpretowalnego modelu za pomocą LIME\n\nLIME generuje lokalne wyjaśnienia, czyli interpretuje model dla pojedynczych predykcji.\nWybieramy losowy przykład z danych testowych.\n\nEksploracja wyniku dla jednego przykładu\n\nLIME modyfikuje lekko wartości wejściowe i obserwuje, jak zmienia się wynik predykcji.\nTworzy „lokalny” model liniowy, który pokazuje, które cechy miały największy wpływ na decyzję.\n\n\nZałóżmy, że nasz model wybrał przykładową roślinę i sklasyfikował ją jako Virginica.\nOto interpretacja wyników: 1. Najważniejsze cechy wpływające na decyzję modelu: - Długość płatka (petal length): największy wpływ na predykcję (np. im większa, tym większe prawdopodobieństwo, że to Virginica). - Szerokość płatka (petal width): również istotny czynnik (np. powyżej pewnej wartości sugeruje Virginica). - Długość kielicha (sepal length): mniejszy wpływ, ale nadal istotny. - Szerokość kielicha (sepal width): zwykle najmniej istotna cecha.\n\nWizualizacja wyników\n\nLIME generuje wykres słupkowy, który pokazuje wpływ każdej cechy na klasyfikację.\nNa wykresie widać, które cechy zwiększały, a które zmniejszały prawdopodobieństwo przypisania do danej klasy.\n\nCo oznacza wynik?\n\nJeśli model przewidział klasę Virginica z wysokim prawdopodobieństwem, oznacza to, że kluczowe cechy (np. długi płatek) mocno wskazują na ten gatunek.\nJeśli cechy miały zróżnicowany wpływ, oznacza to, że model miał pewne trudności w klasyfikacji (np. szerokość płatka nie była jednoznaczna)."
  },
  {
    "objectID": "wyklad5.html#detekcja-anomalii",
    "href": "wyklad5.html#detekcja-anomalii",
    "title": "Wykład 5",
    "section": "Detekcja anomalii",
    "text": "Detekcja anomalii\n\nWartość odstająca (Outlier)\nWartość odstająca (ang. outlier) to obserwacja (wiersz w tabeli danych), która jest znacznie oddalona od pozostałych elementów próbki. Oznacza to, że zależność między zmiennymi niezależnymi i zależnymi dla tej obserwacji może różnić się od pozostałych przypadków.\nDla pojedynczych zmiennych wartości odstające można określić, korzystając z wykresu pudełkowego (box plot). Wykres ten bazuje na kwartylach:\n\nPierwszy kwartyl ( Q_1 ) i trzeci kwartyl ( Q_3 ) wyznaczają boki pudełka,\nDrugi kwartyl ( Q_2 ) (mediana) jest zaznaczony wewnątrz pudełka,\n\nWartości odstające spełniają zależność:\n[ x_{out} &lt; Q_1 - 1.5 IQR x_{out} &gt; Q_3 + 1.5 IQR ]\nGdzie: [ IQR = Q_3 - Q_1 ]\nPrzykładem wartości odstającej może być bolid Formuły 1 – pod względem prędkości jest on anomalią wśród zwykłych samochodów.\n\n\nWykorzystanie detekcji anomalii\nWykrywanie wartości odstających ma szerokie zastosowanie, np.: - Finanse – wykrywanie transakcji fraudowych w analizie danych bankowych, - Cyberbezpieczeństwo – identyfikacja intruzów w sieci na podstawie zachowań użytkowników, - Medycyna – monitorowanie parametrów zdrowotnych i wykrywanie nieprawidłowości, - Przemysł – wykrywanie wadliwych komponentów poprzez analizę obrazu.\n\n\nMetody wykrywania anomalii\n\n1. Metody nadzorowane (supervised learning)\nStosowane, gdy mamy oznaczone dane (np. przypadki oszustw w transakcjach). - Sieci neuronowe, - Algorytm K-najbliższych sąsiadów (KNN), - Sieci Bayesowskie.\n\n\n2. Metody nienadzorowane (unsupervised learning)\nZakładają, że większość danych jest poprawna, a anomalie to niewielki odsetek przypadków. - Klasteryzacja metodą K-średnich (K-Means), - Autoenkodery w sieciach neuronowych, - Testy statystyczne.\n\n\n\nMetoda klasyczna – detekcja na podstawie prawdopodobieństwa\nAby określić, czy dana obserwacja jest anomalią, można użyć prawdopodobieństwa ( p(x) ):\n\nJeśli ( p(x) &lt; ), uznajemy wartość za odstającą.\nW praktyce zakładamy, że dane mają rozkład normalny ( N(, ) ).\nSzacujemy parametry ( ) (średnia) i ( ^2 ) (wariancja) na podstawie próbki.\nNastępnie dla każdej wartości obliczamy prawdopodobieństwo jej wystąpienia i porównujemy z ( ).\n\nPrzykład: Analiza wynagrodzeń w firmie\nWykrywamy, czy w danej firmie są osoby o wynagrodzeniach znacznie odbiegających od średniej.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przykładowe wynagrodzenia w firmie (w tysiącach)\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\n# Obliczenie kwartylów\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\n# Definicja wartości odstających\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\n# Znalezienie outlierów\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\n# Wizualizacja\nsns.boxplot(salaries)\nplt.title(\"Wykres pudełkowy wynagrodzeń\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nWynik: Na wykresie pudełkowym widać, że 150 tys. to anomalia.\n\n\nIsolation Forest – detekcja anomalii za pomocą lasu izolacyjnego\nIsolation Forest to algorytm bazujący na drzewach decyzyjnych, zaproponowany przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku. Identyfikuje anomalie poprzez izolowanie wartości odstających w procesie podziału danych:\n\nWybiera losowo cechę oraz wartość podziału,\nWartości odstające szybciej zostają odizolowane (są bliżej korzenia drzewa),\nWynik jest agregowany na podstawie wielu drzew.\n\nJego zalety to niskie wymagania obliczeniowe i skuteczność w analizie wielowymiarowych danych.\nMetody detekcji anomalii sklearn\nPrzykład: Wykrywanie oszustw bankowych\nBank analizuje transakcje kartą kredytową i wykrywa te, które mogą być nieautoryzowane.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Przykładowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duża kwota, rzadkość)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nWynik: Transakcja 5000 zł zostanie wykryta jako anomalia."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "wyklad3.html",
    "href": "wyklad3.html",
    "title": "Wykład 3",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, podstawowych sposobów przetwarzania i analizowania danych strumieniowych."
  },
  {
    "objectID": "wyklad3.html#definicje",
    "href": "wyklad3.html#definicje",
    "title": "Wykład 3",
    "section": "Definicje",
    "text": "Definicje\n\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 – Zdarzenie to wszystko, co można zaobserwować w danym momencie czasu. Jest generowane jako bezpośredni skutek działania.\nDefinicja 2 – W kontekście danych zdarzenie to niezmienialny rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.\nDefinicja 3 – Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie, np. logi z urządzeń.\nDefinicja 4 – Strumień danych to dane tworzone przyrostowo w czasie, generowane ze źródeł statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\n\n\n\nAnalityka strumieniowa\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) – czyli przetwarzaniem dużych ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologii, wszystkie dane powstają jako ciągły strumień zdarzeń – obejmuje to m.in.:\n- działania użytkowników na stronach internetowych,\n- logi systemowe,\n- pomiary z sensorów."
  },
  {
    "objectID": "wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Wykład 3",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma żadnego związku z momentem wystąpienia analizowanych zdarzeń.\nNatomiast w przetwarzaniu strumieniowym wyróżniamy dwie koncepcje czasu: 1. Czas zdarzenia (event time) – moment, w którym zdarzenie faktycznie miało miejsce. 2. Czas przetwarzania (processing time) – moment, w którym system przetwarza zdarzenie.\nIdealne przetwarzanie danych\nW idealnej sytuacji przetwarzanie następuje natychmiast po wystąpieniu zdarzenia:\n\nRzeczywiste przetwarzanie danych\nW praktyce przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co jest widoczne jako punkty poniżej linii idealnego przetwarzania (poniżej przekątnej na wykresie).\n\nW aplikacjach przetwarzania strumieniowego istotna jest różnica między czasem powstania zdarzenia a czasem jego przetwarzania. Do najczęstszych przyczyn opóźnień należą:\n\nprzesyłanie danych przez sieć,\nbrak komunikacji między urządzeniem a siecią.\n\nPrzykładem jest śledzenie położenia samochodu przez aplikację GPS – przejazd przez tunel może spowodować chwilową utratę danych.\nObsługa opóźnień w przetwarzaniu strumieniowym\nOpóźnienia w przetwarzaniu zdarzeń można obsłużyć na dwa sposoby: 1. Monitorowanie liczby pominiętych zdarzeń i wyzwalanie alarmu w przypadku zbyt dużej liczby odrzuceń. 2. Zastosowanie korekty za pomocą watermarkingu, czyli dodatkowego mechanizmu uwzględniającego opóźnione zdarzenia.\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić jako funkcję schodkową:\n\nNie wszystkie zdarzenia wnoszą wkład do analizy – niektóre mogą zostać odrzucone ze względu na zbyt duże opóźnienie.\nWykorzystanie watermarkingu pozwala na uwzględnienie dodatkowego czasu na pojawienie się opóźnionych zdarzeń. Proces ten obejmuje wszystkie zdarzenia powyżej przerywanej linii. Mimo to nadal mogą zdarzyć się przypadki, w których niektóre punkty zostaną pominięte.\n\nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie."
  },
  {
    "objectID": "wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "href": "wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "title": "Wykład 3",
    "section": "Okna czasowe w analizie strumieniowej",
    "text": "Okna czasowe w analizie strumieniowej\nW przetwarzaniu strumieniowym okna czasowe pozwalają na grupowanie danych w ograniczone czasowo segmenty, co umożliwia analizę zdarzeń w określonych przedziałach czasowych. W zależności od zastosowania stosuje się różne typy okien, dostosowane do charakterystyki danych i wymagań analitycznych.\n\n\n1. Okno rozłączne (Tumbling Window)\nJest to okno o stałej długości, które nie nakłada się na siebie – każde zdarzenie należy tylko do jednego okna.\n✅ Charakterystyka:\n- Stała długość okna\n- Brak nakładania się na siebie\n- Idealne do podziału danych na równe segmenty czasowe\n📌 Przykład: Analiza liczby zamówień w sklepie internetowym co 5 minut.\n\n\n\n\n2. Okno przesuwne (Sliding Window)\nObejmuje wszystkie zdarzenia następujące w określonym przedziale czasu, gdzie okno przesuwa się w sposób ciągły.\n✅ Charakterystyka:\n- Każde zdarzenie może należeć do kilku okien\n- Okno przesuwa się o zadany interwał\n- Przydatne do wykrywania trendów i anomalii\n📌 Przykład: Śledzenie średniej temperatury w ciągu ostatnich 10 minut, aktualizowane co 2 minuty.\n\n\n\n\n3. Okno skokowe (Hopping Window)\nJest podobne do okna rozłącznego, ale pozwala na nakładanie się okien na siebie, dzięki czemu jedno zdarzenie może należeć do kilku okien. Jest stosowane do wygładzania danych.\n✅ Charakterystyka:\n- Stała długość okna\n- Możliwość nakładania się na siebie\n- Przydatne do redukcji szumów w danych\n📌 Przykład: Analiza liczby odwiedzających stronę co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywać trendy.\n\n\n\n\n4. Okno sesyjne (Session Window)\nOkno sesyjne grupuje zdarzenia na podstawie okresów aktywności i zamyka się po określonym czasie braku aktywności.\n✅ Charakterystyka:\n- Dynamiczna długość okna\n- Definiowane przez aktywność użytkownika\n- Stosowane w analizie sesji użytkowników\n📌 Przykład: Analiza sesji użytkowników na stronie internetowej – sesja trwa tak długo, jak długo użytkownik wykonuje akcje, ale kończy się po 15 minutach braku aktywności.\n\n\n\nPodsumowanie\nRóżne rodzaje okien czasowych są stosowane w zależności od specyfiki danych i celów analizy. Wybór odpowiedniego okna wpływa na dokładność wyników i efektywność systemu analitycznego.\n\n\n\n\n\n\n\n\nTyp okna\nCharakterystyka\nZastosowanie\n\n\n\n\nRozłączne (Tumbling)\nStała długość, brak nakładania\nRaporty okresowe\n\n\nPrzesuwne (Sliding)\nStała długość, nakładające się okna\nTrendy, wykrywanie anomalii\n\n\nSkokowe (Hopping)\nStała długość, częściowe nakładanie\nWygładzanie danych\n\n\nSesyjne (Session)\nDynamiczna długość, zależna od aktywności\nAnaliza sesji użytkowników\n\n\n\nKażdy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. Wybór właściwej metody zależy od potrzeb biznesowych i charakterystyki analizowanych danych.\nW analizie danych strumieniowych interpretacja czasu jest złożonym zagadnieniem, ponieważ: 1. Różne systemy mają różne zegary, co może prowadzić do niespójności, 2. Dane mogą docierać z opóźnieniem, co wymaga technik watermarkingu i okien czasowych, 3. Różne podejścia do analizy czasu zdarzenia i czasu przetwarzania wpływają na dokładność wyników."
  },
  {
    "objectID": "wyklad2.html",
    "href": "wyklad2.html",
    "title": "Wykład 2",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h 🎯 Cel wykładu\nzrozumienie, jak dane ewoluowały w różnych branżach i jakie narzędzia są dziś wykorzystywane do ich analizy.\nNa tym wykładzie przedstawimy ewolucję analizy danych, pokazując, jak zmieniały się technologie i podejścia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aż po nowoczesne podejście do strumieniowego przetwarzania danych."
  },
  {
    "objectID": "wyklad2.html#dane-tabelaryczne-tabele-sql",
    "href": "wyklad2.html#dane-tabelaryczne-tabele-sql",
    "title": "Wykład 2",
    "section": "1. Dane tabelaryczne (tabele SQL)",
    "text": "1. Dane tabelaryczne (tabele SQL)\nPoczątkowo dane były przechowywane w postaci tabel, gdzie każda tabela zawierała zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).\nModele takie doskonale nadawały się do danych ustrukturyzowanych.\n\n📌 Cechy:\n✅ Dane podzielone na kolumny o stałej strukturze.\n✅ Możliwość stosowania operacji CRUD (Create, Read, Update, Delete).\n✅ Ścisłe reguły spójności i normalizacji.\n\n\n📌 Przykłady:\n➡️ Systemy bankowe, e-commerce, ERP, systemy CRM.\n\n\n🖥️ Przykładowy kod w Pythonie (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()"
  },
  {
    "objectID": "wyklad2.html#dane-grafowe",
    "href": "wyklad2.html#dane-grafowe",
    "title": "Wykład 2",
    "section": "2. Dane grafowe",
    "text": "2. Dane grafowe\nWraz z rozwojem potrzeb biznesowych pojawiły się dane grafowe, w których relacje między obiektami są reprezentowane jako wierzchołki i krawędzie.\n\n📌 Cechy:\n✅ Dane opisujące relacje i powiązania.\n✅ Elastyczna struktura (grafy zamiast tabel).\n✅ Możliwość analizy połączeń (np. algorytmy PageRank, centralność).\n\n\n📌 Przykłady:\n➡️ Sieci społecznościowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).\n\n\n🖥️ Przykładowy kod w Pythonie (Graf Karate - NetworkX):\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)"
  },
  {
    "objectID": "wyklad2.html#dane-półstrukturyzowane-json-xml-yaml",
    "href": "wyklad2.html#dane-półstrukturyzowane-json-xml-yaml",
    "title": "Wykład 2",
    "section": "3. Dane półstrukturyzowane (JSON, XML, YAML)",
    "text": "3. Dane półstrukturyzowane (JSON, XML, YAML)\nDane te nie są w pełni ustrukturyzowane jak w bazach SQL, ale mają pewien schemat.\n\n📌 Cechy:\n✅ Hierarchiczna struktura (np. klucz-wartość, obiekty zagnieżdżone).\n✅ Brak ścisłego schematu (możliwość dodawania nowych pól).\n✅ Popularność w systemach NoSQL i API.\n\n\n📌 Przykłady:\n➡️ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logów.\n\n\n🖥️ Przykładowy kod w Pythonie (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}"
  },
  {
    "objectID": "wyklad2.html#dane-tekstowe-nlp",
    "href": "wyklad2.html#dane-tekstowe-nlp",
    "title": "Wykład 2",
    "section": "4. Dane tekstowe (NLP)",
    "text": "4. Dane tekstowe (NLP)\nTekst stał się kluczowym źródłem informacji, szczególnie w analizie opinii, chatbotach czy wyszukiwarkach.\n\n📌 Cechy:\n✅ Nieustrukturyzowane dane wymagające przekształcenia.\n✅ Stosowanie embeddingów (np. Word2Vec, BERT, GPT).\n✅ Duże zastosowanie w analizie sentymentu i chatbotach.\n\n\n📌 Przykłady:\n➡️ Media społecznościowe, e-maile, chatboty, tłumaczenie maszynowe.\n\n\n🖥️ Przykładowy kod w Pythonie:\n\n\nCode\nimport ollama\n\n# Przykładowe zdanie\nsentence = \"Sztuczna inteligencja zmienia świat.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-1.6779385805130005, 3.0364203453063965, -6.6012187004089355, -1.7487436532974243]"
  },
  {
    "objectID": "wyklad2.html#dane-multimedialne-obrazy-dźwięk-wideo",
    "href": "wyklad2.html#dane-multimedialne-obrazy-dźwięk-wideo",
    "title": "Wykład 2",
    "section": "5. Dane multimedialne (obrazy, dźwięk, wideo)",
    "text": "5. Dane multimedialne (obrazy, dźwięk, wideo)\nNowoczesne systemy analizy danych wykorzystują również obrazy i dźwięk.\n\n📌 Cechy:\n✅ Wymagają dużej mocy obliczeniowej (sztuczna inteligencja, deep learning).\n✅ Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dźwięk).\n\n\n📌 Przykłady:\n➡️ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treści wideo.\n\n\n🖥️ Przykładowy kod w Pythonie (Obraz - OpenCV):\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "wyklad2.html#dane-strumieniowe",
    "href": "wyklad2.html#dane-strumieniowe",
    "title": "Wykład 2",
    "section": "6. Dane strumieniowe",
    "text": "6. Dane strumieniowe\nObecnie najbardziej dynamicznie rozwija się analiza danych strumieniowych, gdzie dane są analizowane na bieżąco, w miarę ich napływania.\n\n📌 Cechy:\n✅ Przetwarzanie w czasie rzeczywistym.\n✅ Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.\n\n\n📌 Przykłady:\n➡️ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.\n\n\n🖥️ Przykładowy kod w Pythonie (Strumieniowe transakcje bankowe):\n\n\nCode\nimport time\ntransactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]\nfor transaction in transactions:\n    print(f\"Processing transaction: {transaction}\")\n    time.sleep(1)\n\n\nProcessing transaction: {'id': 1, 'amount': 100}\nProcessing transaction: {'id': 2, 'amount': 200}"
  },
  {
    "objectID": "wyklad2.html#dane-sensoryczne-i-iot",
    "href": "wyklad2.html#dane-sensoryczne-i-iot",
    "title": "Wykład 2",
    "section": "7. Dane sensoryczne i IoT",
    "text": "7. Dane sensoryczne i IoT\nDane z czujników i urządzeń IoT są kolejnym krokiem w ewolucji.\n\n📌 Cechy:\n✅ Często pochodzą z miliardów urządzeń (big data).\n✅ Wymagają analizy brzegowej (edge computing).\n\n\n📌 Przykłady:\n➡️ Smart home, wearables, samochody autonomiczne, systemy przemysłowe.\n\n\n🖥️ Przykładowy kod w Pythonie (Sensor - temperatura):\n\n\nCode\nimport random\ndef get_temperature():\n    return round(random.uniform(20.0, 25.0), 2)\nprint(f\"Current temperature: {get_temperature()}°C\")\n\n\nCurrent temperature: 21.0°C"
  },
  {
    "objectID": "wyklad2.html#hadoop-map-reduce-skalowanie-obliczeń-na-big-data",
    "href": "wyklad2.html#hadoop-map-reduce-skalowanie-obliczeń-na-big-data",
    "title": "Wykład 2",
    "section": "Hadoop Map-Reduce – Skalowanie obliczeń na Big Data",
    "text": "Hadoop Map-Reduce – Skalowanie obliczeń na Big Data\nKiedy mówimy o skalowalnym przetwarzaniu danych, pierwszym skojarzeniem może być Google.\nAle co tak naprawdę sprawia, że możemy wyszukiwać informacje w ułamku sekundy, przetwarzając petabajty danych?\n👉 Czy wiesz, że nazwa “Google” pochodzi od słowa “Googol”, czyli liczby równej 10¹⁰⁰?\nTo więcej niż liczba atomów w znanym Wszechświecie! 🌌\n\n🔥 Wyzwanie: Czy uda Ci się zapisać liczbę Googol do końca zajęć?"
  },
  {
    "objectID": "wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczają",
    "href": "wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczają",
    "title": "Wykład 2",
    "section": "🔍 Dlaczego SQL i klasyczne algorytmy nie wystarczają?",
    "text": "🔍 Dlaczego SQL i klasyczne algorytmy nie wystarczają?\nTradycyjne bazy danych SQL czy jednowątkowe algorytmy zawodzą, gdy skala danych przekracza pojedynczy komputer.\nW tym miejscu pojawia się MapReduce – rewolucyjny model obliczeniowy stworzony przez Google.\n\n🛠️ Rozwiązania Google dla Big Data:\n✅ Google File System (GFS) – rozproszony system plików.\n✅ Bigtable – system do przechowywania ogromnych ilości ustrukturyzowanych danych.\n✅ MapReduce – algorytm podziału pracy na wiele maszyn."
  },
  {
    "objectID": "wyklad2.html#graficzne-przedstawienie-mapreduce",
    "href": "wyklad2.html#graficzne-przedstawienie-mapreduce",
    "title": "Wykład 2",
    "section": "🖼️ Graficzne przedstawienie MapReduce",
    "text": "🖼️ Graficzne przedstawienie MapReduce\n\n1. Mapowanie rozdziela zadania (Map)\nKażde wejście dzielone jest na mniejsze części i przetwarzane równolegle.\n🌍 Wyobraź sobie, że masz książkę telefoniczną i chcesz znaleźć wszystkie osoby o nazwisku “Nowak”.\n➡️ Podziel książkę na fragmenty i daj każdemu do przeanalizowania jeden fragment.\n\n\n2. Redukcja zbiera wyniki (Reduce)\nWszystkie częściowe wyniki są łączone w jedną, końcową odpowiedź.\n🔄 Wszyscy uczniowie zgłaszają swoje wyniki, a jeden student zbiera i podsumowuje odpowiedź."
  },
  {
    "objectID": "wyklad2.html#klasyczny-przykład-liczenie-słów-w-tekście",
    "href": "wyklad2.html#klasyczny-przykład-liczenie-słów-w-tekście",
    "title": "Wykład 2",
    "section": "💡 Klasyczny przykład: Liczenie słów w tekście",
    "text": "💡 Klasyczny przykład: Liczenie słów w tekście\nZałóżmy, że mamy miliony książek i chcemy policzyć, ile razy występuje każde słowo.\n\n🖥️ Kod MapReduce w Pythonie (z użyciem multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Funkcja Map (podział tekstu na słowa)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Funkcja Reduce (sumowanie wyników)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\n🔹 Co tu się dzieje?\n✅ Każdy fragment tekstu jest przetwarzany niezależnie (map).\n✅ Wyniki są zbierane i sumowane (reduce).\n✅ Efekt: Możemy przetwarzać terabajty tekstu równolegle!"
  },
  {
    "objectID": "wyklad2.html#wizualizacja-porównanie-klasycznego-podejścia-i-mapreduce",
    "href": "wyklad2.html#wizualizacja-porównanie-klasycznego-podejścia-i-mapreduce",
    "title": "Wykład 2",
    "section": "🎨 Wizualizacja – Porównanie klasycznego podejścia i MapReduce",
    "text": "🎨 Wizualizacja – Porównanie klasycznego podejścia i MapReduce\n📊 Stare podejście – Jeden komputer wykonuje wszystko sekwencyjnie.\n📊 Nowe podejście (MapReduce) – Każda maszyna liczy fragment i wyniki są agregowane."
  },
  {
    "objectID": "wyklad2.html#wyzwanie-dla-ciebie",
    "href": "wyklad2.html#wyzwanie-dla-ciebie",
    "title": "Wykład 2",
    "section": "🚀 Wyzwanie dla Ciebie!",
    "text": "🚀 Wyzwanie dla Ciebie!\n🔹 Znajdź i uruchom swój własny algorytm MapReduce w dowolnym języku!\n🔹 Czy potrafisz zaimplementować własny MapReduce do innego zadania? (np. analiza logów, zliczanie kliknięć na stronie)"
  },
  {
    "objectID": "wyklad2.html#big-data",
    "href": "wyklad2.html#big-data",
    "title": "Wykład 2",
    "section": "Big Data",
    "text": "Big Data\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962."
  },
  {
    "objectID": "wyklad2.html#modele-przetwarzania-danych",
    "href": "wyklad2.html#modele-przetwarzania-danych",
    "title": "Wykład 2",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe."
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: Szkoła Główna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJęzyk prowadzenia: polski\nPoziom przedmiotu: średnio-zaawansowany\nProwadzący: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspółczesny biznes opiera się na podejmowaniu decyzji opartych na danych. Coraz większa ilość informacji, rosnące wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiają, że analiza danych w czasie rzeczywistym staje się kluczowym elementem nowoczesnych procesów biznesowych.\nNa zajęciach studenci zapoznają się z metodami i technologiami umożliwiającymi przetwarzanie danych w czasie rzeczywistym. Szczególną uwagę poświęcimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz głębokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretować zjawiska biznesowe, ale także podejmować szybkie i trafne decyzje.\nW ramach kursu omówimy zarówno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dźwięk, strumieniowanie wideo). Studenci poznają architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a także wyzwania związane z modelowaniem danych w czasie rzeczywistym na dużą skalę.\nKurs obejmuje część teoretyczną oraz praktyczne laboratoria, podczas których studenci będą pracować z rzeczywistymi danymi w środowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. Dzięki temu studenci nie tylko zdobędą wiedzę na temat metod analitycznych, ale także nauczą się korzystać z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plików płaskich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (Wykład)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametrów modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykładzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego środowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego środowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 1.\nAnaliza 1 Detekcja wyłudzeń w zgłoszeniach szkód samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego środowiska. Cz 2.\nPrzygotowanie środowiska Microsoft Azure. Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartości odstających w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzędzia IT do szybkiej analizy logów.\nNarzędzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-kształcenia",
    "href": "sylabus.html#efekty-kształcenia",
    "title": "Sylabus",
    "section": "Efekty kształcenia",
    "text": "Efekty kształcenia\n\nWiedza:\n\n\nZna historię i filozofię modeli przetwarzania danych\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiązania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytań z kolokwium\n\nUmie wybrać strukturę IT dla danego problemu biznesowego\n\nPowiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\nPowiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejętności:\n\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiązania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować, przetwarzać oraz zachowywać dane generowane w czasie rzeczywistym\n\nPowiązania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\nPowiązania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosować i skonstruować system do przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotować raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiązania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem\n\nPowiązania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiązania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1️⃣ Zając S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2️⃣ Frątczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3️⃣ Bellemare A., Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę, O’Reilly 2021.\n4️⃣ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, O’Reilly 2022.\n5️⃣ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. Rozwiązania typowych problemów dotyczących przygotowania danych, konstruowania modeli i MLOps, O’Reilly 2021.\n6️⃣ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, O’Reilly 2022.\n7️⃣ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly 2018.\n8️⃣ Sebastián Ramírez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostępna online).\n9️⃣ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\n🔟 Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, O’Reilly 2019."
  },
  {
    "objectID": "w3.html",
    "href": "w3.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Wykorzystanie systemów real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, która będzie mogła szybko przetwarzać ogromne ilości danych oraz reagować na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, ponieważ umożliwia szybsze podejmowanie decyzji, monitorowanie procesów w czasie rzeczywistym i reagowanie na zdarzenia bez opóźnienia.\nOmówimy główne elementy architektury systemów real-time, popularne wzorce architektoniczne oraz technologie, które są wykorzystywane do budowy takich systemów.\n\n\nSystemy real-time muszą spełniać szereg wymagań związanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentów w architekturze systemu, które zapewniają jego prawidłowe funkcjonowanie.\n\n\nDane w systemie real-time pochodzą z różnych źródeł, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urządzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z giełdy.\nDane użytkowników: np. logi użytkowników w aplikacjach mobilnych, dane z mediów społecznościowych.\n\n\n\n\nDane muszą być szybko przesyłane do systemów, które mogą je analizować. W tym celu wykorzystywane są technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyłania danych w czasie rzeczywistym, zapewniający wysoką wydajność i niezawodność.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z dużą ilością subskrybentów.\n\n\n\n\nDane w systemach real-time są często przetwarzane w strumieniu. Dwa główne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, które może mieć opóźnienie, ale przetwarza dane w sposób efektywny. Może być wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opóźnień, w którym dane są natychmiastowo analizowane i przetwarzane.\n\n\n\n\nPrzechowywanie danych w systemie real-time zależy od wymagań aplikacji. Dwa główne rodzaje przechowywania to:\n\nData Lake: składowanie ogromnych ilości nieprzetworzonych danych w postaci surowych plików. Bazy danych NoSQL: takie jak Cassandra, które umożliwiają szybki dostęp do danych w czasie rzeczywistym.\nData Warehouse: składowanie przetworzonych danych w celu ich analizy.\n\n\n\n\nPo przetworzeniu danych w czasie rzeczywistym należy wykonać ich analizę i prezentację w sposób zrozumiały dla użytkownika:\n\nDashboardy: narzędzia takie jak Grafana lub Kibana, które służą do wizualizacji wyników w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmów uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\n\n\n\nLambda Architecture to popularna koncepcja przetwarzania danych, która łączy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura używana w systemach przetwarzania Big Data, która zakłada dwie warstwy:\n\nBatch Layer: przetwarzanie (dużych ilości) danych wsadowych, które są później wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napływające dane strumieniowe, np. z sensorów, social media, transakcji, w celu uzyskania natychmiastowych wyników.\nServing Layer: warstwa, która łączy wyniki obu poprzednich warstw i dostarcza je do użytkownika np. za pomocą API.\n\n \n\n\n\n\n✅ Możliwość łączenia przetwarzania wsadowego i strumieniowego,\n✅ wsparcie dla dużych zbiorów danych,\n✅ elastyczność w przetwarzaniu złożonych zapytań.\n❌ Wymaga utrzymywania dwóch oddzielnych systemów do przetwarzania danych (batch i stream), co prowadzi do złożoności implementacji i utrzymania.\n\n\n\n\nKappa Architecture jest uproszczoną wersją Lambda Architecture. Zamiast używać dwóch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jedną warstwę przetwarzania strumieniowego, co upraszcza cały system.\nJest to bardziej elastyczne podejście do budowy systemów real-time, zwłaszcza w przypadku, gdy dane są przetwarzane tylko w jednym trybie (streaming).\n \n\n\n\n\n✅ Prostota: Jako że przetwarzanie danych odbywa się tylko w jednym strumieniu, cały system jest prostszy i bardziej spójny.\n✅ Skalowalność: Dzięki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekście analizy danych w czasie rzeczywistym.\n✅ Idealne dla ML: Kappa Architecture świetnie sprawdza się w zastosowaniach związanych z Machine Learning, ponieważ przetwarzanie danych odbywa się na bieżąco, co pozwala na szybsze uczenie i wdrażanie modeli ML w czasie rzeczywistym.\n❌ Może być mniej wydajna przy bardzo dużych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\n\nArchitektura mikroserwisów jest powszechnie wykorzystywana w systemach real-time, ponieważ umożliwia:\n\nPodział aplikacji na mniejsze, autonomiczne jednostki.\nElastyczność i skalowalność systemu.\nMożliwość przetwarzania różnych rodzajów danych przez różne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomości.\n\n\n\n\nUber to przykład firmy, która skutecznie wykorzystuje narzędzia do przetwarzania strumieniowego, by monitorować ruch drogowy w czasie rzeczywistym. Dzięki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdów oraz czasach oczekiwania na przejazd, które są następnie analizowane na żywo.\nDane wejściowe: Informacje o czasie i miejscu podróży, dane GPS z pojazdów, natężenie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyłania tych danych w czasie rzeczywistym do systemów takich jak Apache Flink lub Spark Streaming, które analizują je na bieżąco.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasę w czasie rzeczywistym.\nWynik: Użytkownicy Ubera otrzymują prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowców), co umożliwia optymalizację transportu."
  },
  {
    "objectID": "w3.html#architektura-systemów-real-time",
    "href": "w3.html#architektura-systemów-real-time",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Wykorzystanie systemów real-time (czas rzeczywisty) w analizie danych wymaga odpowiedniej architektury, która będzie mogła szybko przetwarzać ogromne ilości danych oraz reagować na nie w czasie rzeczywistym. Architektura systemu real-time jest kluczowa, ponieważ umożliwia szybsze podejmowanie decyzji, monitorowanie procesów w czasie rzeczywistym i reagowanie na zdarzenia bez opóźnienia.\nOmówimy główne elementy architektury systemów real-time, popularne wzorce architektoniczne oraz technologie, które są wykorzystywane do budowy takich systemów.\n\n\nSystemy real-time muszą spełniać szereg wymagań związanych z czasem przetwarzania danych. Istnieje kilka kluczowych komponentów w architekturze systemu, które zapewniają jego prawidłowe funkcjonowanie.\n\n\nDane w systemie real-time pochodzą z różnych źródeł, takich jak:\n\nCzujniki IoT: np. monitorowanie maszyn w fabryce, urządzenia medyczne.\nTransakcje w czasie rzeczywistym: np. zakupy online, dane z giełdy.\nDane użytkowników: np. logi użytkowników w aplikacjach mobilnych, dane z mediów społecznościowych.\n\n\n\n\nDane muszą być szybko przesyłane do systemów, które mogą je analizować. W tym celu wykorzystywane są technologie strumieniowe, takie jak:\n\nApache Kafka: popularny system do przesyłania danych w czasie rzeczywistym, zapewniający wysoką wydajność i niezawodność.\nApache Pulsar: alternatywa dla Kafki, dedykowana do przetwarzania danych w czasie rzeczywistym z dużą ilością subskrybentów.\n\n\n\n\nDane w systemach real-time są często przetwarzane w strumieniu. Dwa główne modele przetwarzania to:\n\nBatch processing: Przetwarzanie danych w partiach, które może mieć opóźnienie, ale przetwarza dane w sposób efektywny. Może być wykorzystywane w kombinacji z systemami real-time do agregacji danych.\nStream processing: Przetwarzanie danych w czasie rzeczywistym, bez opóźnień, w którym dane są natychmiastowo analizowane i przetwarzane.\n\n\n\n\nPrzechowywanie danych w systemie real-time zależy od wymagań aplikacji. Dwa główne rodzaje przechowywania to:\n\nData Lake: składowanie ogromnych ilości nieprzetworzonych danych w postaci surowych plików. Bazy danych NoSQL: takie jak Cassandra, które umożliwiają szybki dostęp do danych w czasie rzeczywistym.\nData Warehouse: składowanie przetworzonych danych w celu ich analizy.\n\n\n\n\nPo przetworzeniu danych w czasie rzeczywistym należy wykonać ich analizę i prezentację w sposób zrozumiały dla użytkownika:\n\nDashboardy: narzędzia takie jak Grafana lub Kibana, które służą do wizualizacji wyników w czasie rzeczywistym.\nMachine Learning: zastosowanie algorytmów uczenia maszynowego w czasie rzeczywistym do klasyfikacji, wykrywania anomalii czy predykcji (np. wykrywanie oszustw).\n\n\n\n\n\n\n\nLambda Architecture to popularna koncepcja przetwarzania danych, która łączy przetwarzanie wsadowe z przetwarzaniem strumieniowym. To klasyczna architektura używana w systemach przetwarzania Big Data, która zakłada dwie warstwy:\n\nBatch Layer: przetwarzanie (dużych ilości) danych wsadowych, które są później wykorzystywane do analizy. Realizuje procesy przetwarzania w trybie offline\nSpeed Layer (Real-Time Layer): przetwarzanie danych w czasie rzeczywistym, czyli napływające dane strumieniowe, np. z sensorów, social media, transakcji, w celu uzyskania natychmiastowych wyników.\nServing Layer: warstwa, która łączy wyniki obu poprzednich warstw i dostarcza je do użytkownika np. za pomocą API.\n\n \n\n\n\n\n✅ Możliwość łączenia przetwarzania wsadowego i strumieniowego,\n✅ wsparcie dla dużych zbiorów danych,\n✅ elastyczność w przetwarzaniu złożonych zapytań.\n❌ Wymaga utrzymywania dwóch oddzielnych systemów do przetwarzania danych (batch i stream), co prowadzi do złożoności implementacji i utrzymania.\n\n\n\n\nKappa Architecture jest uproszczoną wersją Lambda Architecture. Zamiast używać dwóch osobnych warstw (batch i speed), Kappa wykorzystuje tylko jedną warstwę przetwarzania strumieniowego, co upraszcza cały system.\nJest to bardziej elastyczne podejście do budowy systemów real-time, zwłaszcza w przypadku, gdy dane są przetwarzane tylko w jednym trybie (streaming).\n \n\n\n\n\n✅ Prostota: Jako że przetwarzanie danych odbywa się tylko w jednym strumieniu, cały system jest prostszy i bardziej spójny.\n✅ Skalowalność: Dzięki eliminacji warstwy batch, system jest bardziej elastyczny i skalowalny w kontekście analizy danych w czasie rzeczywistym.\n✅ Idealne dla ML: Kappa Architecture świetnie sprawdza się w zastosowaniach związanych z Machine Learning, ponieważ przetwarzanie danych odbywa się na bieżąco, co pozwala na szybsze uczenie i wdrażanie modeli ML w czasie rzeczywistym.\n❌ Może być mniej wydajna przy bardzo dużych zbiorach danych, w przypadku, gdy wymagane jest skomplikowane przetwarzanie wsadowe.\n\n\n\n\nArchitektura mikroserwisów jest powszechnie wykorzystywana w systemach real-time, ponieważ umożliwia:\n\nPodział aplikacji na mniejsze, autonomiczne jednostki.\nElastyczność i skalowalność systemu.\nMożliwość przetwarzania różnych rodzajów danych przez różne mikroserwisy.\nWykorzystanie komunikacji asynchronicznej, np. przez kolejki wiadomości.\n\n\n\n\nUber to przykład firmy, która skutecznie wykorzystuje narzędzia do przetwarzania strumieniowego, by monitorować ruch drogowy w czasie rzeczywistym. Dzięki systemowi Apache Kafka, Uber gromadzi dane o ruchu drogowym, lokalizacji pojazdów oraz czasach oczekiwania na przejazd, które są następnie analizowane na żywo.\nDane wejściowe: Informacje o czasie i miejscu podróży, dane GPS z pojazdów, natężenie ruchu.\nProces przetwarzania: Uber wykorzystuje Apache Kafka do przesyłania tych danych w czasie rzeczywistym do systemów takich jak Apache Flink lub Spark Streaming, które analizują je na bieżąco.\nAnaliza: System przewiduje czas oczekiwania na przejazd, monitoruje warunki drogowe oraz optymalizuje trasę w czasie rzeczywistym.\nWynik: Użytkownicy Ubera otrzymują prognozy czasu przejazdu, a Uber dynamicznie dostosowuje zasoby (np. przydzielanie kierowców), co umożliwia optymalizację transportu."
  },
  {
    "objectID": "wyklad1.html",
    "href": "wyklad1.html",
    "title": "Wykład 1",
    "section": "",
    "text": "⏳ Czas trwania: 1,5h\n🎯 Cel wykładu\nZapoznanie studentów z podstawami real-time analytics, różnicami między trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami."
  },
  {
    "objectID": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez konieczności przechowywania i oczekiwania na późniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wniosków i reakcji na zmieniające się warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) – dane są analizowane w ciągu milisekund lub sekund od ich wygenerowania.\nStreaming vs. Batch Processing – analiza danych może odbywać się w sposób ciągły (streaming) lub w z góry określonych interwałach (batch).\nIntegracja z IoT, AI i ML – real-time analytics często współpracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym – np. natychmiastowa detekcja oszustw w transakcjach bankowych."
  },
  {
    "objectID": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "Wykład 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowość\n\nWykrywanie oszustw – analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujących na oszustwa.\nAutomatyczny trading – systemy HFT (High-Frequency Trading) analizują miliony danych w ułamkach sekundy.\nDynamiczne oceny kredytowe – natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym – dynamiczne rekomendacje produktów na podstawie aktualnego zachowania użytkownika.\nDynamiczne ceny – np. Uber, Amazon i hotele stosują dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediów społecznościowych – analiza nastrojów klientów i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej – analiza logów w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystąpieniem.\nSmart Cities – analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji świetlnej.\nAnalityka IoT – urządzenia IoT generują strumienie danych, które można analizować w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentów – analiza sygnałów z urządzeń medycznych w celu natychmiastowego wykrycia zagrożenia życia.\nAnalityka epidemiologiczna – śledzenie rozprzestrzeniania się chorób na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemów informatycznych, który umożliwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branżach – od finansów, przez e-commerce, aż po ochronę zdrowia i IoT."
  },
  {
    "objectID": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "wyklad1.html#różnice-między-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "Wykład 1",
    "section": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "Różnice między Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstnieją trzy główne podejścia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKażde z nich różni się szybkością przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing – Przetwarzanie wsadowe\n📌 Definicja:\nBatch Processing polega na zbieraniu dużych ilości danych i ich przetwarzaniu w określonych odstępach czasu (np. co godzinę, codziennie, co tydzień).\n📌 Cechy:\n\n✅ Wysoka wydajność dla dużych zbiorów danych\n✅ Przetwarzanie danych po ich zgromadzeniu\n✅ Nie wymaga natychmiastowej analizy\n✅ Zwykle tańsze niż przetwarzanie w czasie rzeczywistym\n❌ Opóźnienia – wyniki są dostępne dopiero po zakończeniu przetwarzania\n\n📌 Przykłady zastosowań:\n\nGenerowanie raportów finansowych na koniec dnia/miesiąca\nAnaliza trendów sprzedaży na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\n📌 Przykładowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiąca\n\n# Agregacja danych - miesięczne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wyników do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyś chciał utworzyć dane do przykładu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics – Analiza niemal w czasie rzeczywistym\n📌 Definicja:\nNear Real-Time Analytics to analiza danych, która odbywa się z minimalnym opóźnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie pełna analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duże opóźnienia mogą wpłynąć na biznes.\n📌 Cechy:\n\n✅ Przetwarzanie danych w krótkich odstępach czasu (kilka sekund – minut)\n✅ Umożliwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\n✅ Optymalny balans między kosztami a szybkością\n❌ Nie nadaje się do systemów wymagających natychmiastowej reakcji\n\n📌 Przykłady zastosowań:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciągu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowań użytkowników\nAnaliza logów serwerów i sieci w celu wykrycia anomalii\n\n📌 Przykładowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logów IT)\nAmazon Kinesis\n\nPrzykład producenta danych realizującego tranzakcje wysyłane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generująca przykładowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota między 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# Zakończenie działania producenta\nproducer.flush()\nproducer.close()\nPrzykład consumenta - programu sparawdzającego zbyt duże transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\nPrzykładowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics – Analiza w czasie rzeczywistym\n📌 Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w ułamku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagających reakcji w czasie rzeczywistym, np. w transakcjach giełdowych, systemach IoT czy cyberbezpieczeństwie.\n📌 Cechy:\n\n✅ Bardzo niskie opóźnienie (milliseconds-seconds)\n✅ Umożliwia natychmiastową reakcję systemu\n✅ Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\n❌ Droższe i bardziej złożone technologicznie niż batch processing\n\n📌 Przykłady zastosowań:\n\nHigh-Frequency Trading (HFT) – analiza i podejmowanie decyzji w transakcjach giełdowych w milisekundach\nAutonomiczne samochody – analiza strumieni danych z kamer i sensorów w czasie rzeczywistym\nCyberbezpieczeństwo – detekcja ataków w sieciach komputerowych w ułamku sekundy\nAnalityka IoT – np. natychmiastowa detekcja anomalii w danych z czujników przemysłowych\n\n📌 Przykładowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\n🔎 Porównanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpóźnienie\nMinuty – godziny – dni\nSekundy – minuty\nMilisekundy – sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w pełni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\n📉 Niski\n📈 Średni\n📈📈 Wysoki\n\n\nZłożoność implementacji\n📉 Prosta\n📈 Średnia\n📈📈 Trudna\n\n\nPrzykłady zastosowań\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\n📌 Kiedy stosować Batch Processing?\n\n✅ Gdy nie wymagasz natychmiastowej analizy\n✅ Gdy masz duże ilości danych, ale przetwarzane są one okresowo\n✅ Gdy chcesz obniżyć koszty\n\n📌 Kiedy stosować Near Real-Time Analytics?\n\n✅ Gdy wymagasz analizy w krótkim czasie (sekundy – minuty)\n✅ Gdy potrzebujesz bardziej aktualnych danych, ale nie w pełnym real-time\n✅ Gdy szukasz kompromisu między wydajnością a kosztami\n\n📌 Kiedy stosować Real-Time Analytics?\n\n✅ Gdy każda milisekunda ma znaczenie (np. giełda, autonomiczne pojazdy)\n✅ Gdy chcesz wykrywać oszustwa, anomalie lub incydenty natychmiast\n✅ Gdy system musi natychmiast reagować na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne – w wielu przypadkach near real-time jest wystarczające i bardziej opłacalne. Kluczowe jest zrozumienie wymagań biznesowych przed wyborem odpowiedniego rozwiązania."
  },
  {
    "objectID": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "href": "wyklad1.html#dlaczego-real-time-analytics-jest-ważne",
    "title": "Wykład 1",
    "section": "Dlaczego Real-Time Analytics jest ważne?",
    "text": "Dlaczego Real-Time Analytics jest ważne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje się coraz bardziej istotna w wielu branżach, ponieważ umożliwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodów, dla których real-time analytics jest ważne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagować na zmiany i wydarzenia w czasie rzeczywistym. Dzięki temu można podejmować decyzje szybciej, co jest kluczowe w dynamicznych środowiskach, takich jak:\n\nMarketing: Reklamy mogą być dostosowane do zachowań użytkowników w czasie rzeczywistym (np. personalizacja treści reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie każda minuta może oznaczać różnicę w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogą monitorować kluczowe wskaźniki operacyjne na bieżąco. Przykłady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urządzeń w fabrykach, aby natychmiast wykrywać awarie i zapobiegać przestojom.\nHealthtech: Śledzenie parametrów życiowych pacjentów i wykrywanie anomalii, co może ratować życie.\n\n\n\nZwiększenie efektywności operacyjnej\nReal-time analytics umożliwia natychmiastowe wykrywanie i eliminowanie problemów operacyjnych, zanim staną się poważniejsze. Przykłady:\n\nLogistyka: Śledzenie przesyłek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywność i zmniejsza opóźnienia.\nRetail: Monitorowanie poziomu zapasów na bieżąco i dostosowywanie zamówień do aktualnych potrzeb.\n\n\n\nKonkurencyjność\nOrganizacje, które wykorzystują analitykę w czasie rzeczywistym, mają przewagę nad konkurencją, ponieważ mogą szybciej reagować na zmiany na rynku, nowe potrzeby klientów i sytuacje kryzysowe. Dzięki natychmiastowym informacjom:\n\nMożna podejmować decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywać lepsze relacje z klientami, reagując na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doświadczenia użytkowników (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z użytkownikami w trakcie ich trwania. Przykłady:\n\nE-commerce: Analiza koszyka zakupowego użytkownika w czasie rzeczywistym, aby np. zaoferować rabat lub przypomnieć o porzuconych produktach.\nStreaming: Optymalizacja jakości usługi wideo/streamingowej w zależności od dostępnej przepustowości łącza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym świecie pełnym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeństwa. Przykłady:\n\nCyberbezpieczeństwo: Real-time analytics umożliwia wykrywanie podejrzanych działań w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztów\nDzięki analizie w czasie rzeczywistym można optymalizować zasoby i zmniejszać koszty. Na przykład:\n\nZarządzanie energią: Analiza zużycia energii w czasie rzeczywistym, umożliwiająca optymalizację wydatków na energię w firmach.\nOptymalizacja łańcucha dostaw: Dzięki bieżącemu śledzeniu zapasów i dostaw można lepiej zarządzać kosztami magazynowania i transportu.\n\n\n\nZdolność do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, które mogą przewidywać przyszłe zachowania lub problemy, a także je eliminować zanim się pojawią. Na przykład:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w połączeniu z modelami predykcyjnymi pozwala przewidywać awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym można dostosowywać produkcję lub zapasy na podstawie bieżących trendów.\n\nReal-time analytics to nie tylko analiza danych – to kluczowy element strategii firm w świecie, który wymaga szybkich reakcji, elastyczności i dostosowywania się do zmieniającego się otoczenia. Firmy, które wdrażają te technologie, mogą znacząco poprawić swoje wyniki finansowe, obsługę klienta, wydajność operacyjną, a także przewagę konkurencyjną."
  },
  {
    "objectID": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "Wykład 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiąże się z wieloma wyzwaniami i trudnościami, które trzeba rozwiązać, aby systemy real-time działały efektywnie i niezawodnie. Pomimo ogromnego potencjału, jaki daje możliwość natychmiastowego przetwarzania danych, realizacja tych procesów w praktyce wiąże się z licznymi problemami technologicznymi, organizacyjnymi i dotyczącymi zarządzania danymi.\nPoniżej przedstawiamy najważniejsze wyzwania oraz możliwe rozwiązania, które należy uwzględnić podczas implementacji systemów analizy danych w czasie rzeczywistym.\n\nSkalowalność systemów\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadań. W miarę jak ilość generowanych danych rośnie, systemy muszą być w stanie obsługiwać większe obciążenie bez opóźnienia w przetwarzaniu.\nZwiększona ilość danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, ilość generowanych danych może być olbrzymia. Potrzebna jest elastyczność: System musi automatycznie dostosowywać zasoby w zależności od obciążenia.\n\n\nRozwiązanie:\nWykorzystanie skalowalnych systemów chmurowych, które pozwalają na dynamiczne zwiększanie zasobów obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarządzania kontenerami i automatycznego skalowania mikroserwisów. Technologie strumieniowe (Apache Kafka, Apache Flink) umożliwiające przetwarzanie danych w sposób wydajny i rozproszony.\n\n\n\nOpóźnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, każde opóźnienie w przetwarzaniu danych może mieć poważne konsekwencje. Dotyczy to zwłaszcza obszarów takich jak:\nWykrywanie oszustw: W przypadku systemów płatności online, opóźnienie w analizie transakcji może oznaczać przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentów: Opóźnienia mogą wpłynąć na skuteczność reakcji w sytuacjach kryzysowych.\n\n\nRozwiązanie:\nUżywanie algorytmów optymalizujących czas przetwarzania, np. stream processing z wykorzystaniem systemów takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliżej źródła (np. urządzenia IoT), aby zmniejszyć opóźnienia w transmisji danych do chmury.\n\n\n\nJakość danych i zarządzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizować dane w czasie rzeczywistym, ale także zapewnić ich wysoką jakość. W przeciwnym razie analizy mogą prowadzić do błędnych wniosków lub opóźnień w reagowaniu na nieprawidłowe dane.\nZanieczyszczone dane: W systemach real-time dane często są niepełne, brudne, błędne lub nieuporządkowane. Zmiana charakterystyki danych: Dane mogą zmieniać się w czasie, co może utrudniać ich przetwarzanie i analizę. #### Rozwiązanie:\nData cleansing i data validation na wstępnym etapie procesu. Automatyczne systemy monitorowania jakości danych w celu wykrywania błędów w czasie rzeczywistym. Zarządzanie danymi w strumieniu: Narzędzia takie jak Apache Kafka pozwalają na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZłożoność integracji systemów\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym często muszą współpracować z istniejącymi systemami IT i źródłami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemów, zwłaszcza w rozproszonej architekturze, może być skomplikowana.\n\n\nRozwiązanie:\nUżywanie API do łatwiejszej integracji z zewnętrznymi systemami. Mikroserwisy i konteneryzacja z pomocą narzędzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, które umożliwia łatwą integrację różnych źródeł danych oraz zapewnia elastyczność w dostosowywaniu systemów do rosnących potrzeb.\n\n\n\nBezpieczeństwo i prywatność\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiąże się z ogromną ilością wrażliwych informacji, szczególnie w branżach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, że dane są odpowiednio chronione przed nieautoryzowanym dostępem, jest kluczowe.\nOchrona danych w czasie transmisji: Muszą być szyfrowane zarówno podczas przesyłania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym może być celem ataków, takich jak DDoS czy SQL injection.\n\n\nRozwiązanie:\nSzyfrowanie danych zarówno w spoczynku, jak i podczas przesyłania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeństwa. Zgodność z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarządzanie błędami i awariami\n\nWyzwanie:\nBłędy i awarie w systemach real-time mogą prowadzić do poważnych konsekwencji, w tym utraty danych, opóźnień w analizach czy nawet usunięcia usług. W systemach rozproszonych trudno jest osiągnąć pełną niezawodność.\n\n\nRozwiązanie:\nRedundancja: Tworzenie kopii zapasowych systemów i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), które pozwalają na szybkie wykrycie i naprawienie problemów. Zarządzanie stanem: Dzięki użyciu narzędzi jak Apache Kafka, można ponownie przetwarzać dane, jeśli wystąpił błąd w transmisji.\n\n\n\nKoszty związane z infrastrukturą\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, która zapewni odpowiednią moc obliczeniową i pamięć. To może wiązać się z dużymi kosztami, szczególnie gdy dane muszą być przechowywane i przetwarzane w czasie rzeczywistym na dużą skalę.\n\n\nRozwiązanie:\nChmura obliczeniowa: Możliwość elastycznego skalowania zasobów w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalają na uruchamianie procesów bez potrzeby utrzymywania stałej infrastruktury.\nChociaż analiza danych w czasie rzeczywistym oferuje ogromne korzyści, wiąże się także z wieloma wyzwaniami. Właściwa architektura, narzędzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogą pomóc w przezwyciężeniu wielu z tych trudności. Warto również pamiętać o konieczności zapewnienia wysokiej jakości danych, ich bezpieczeństwa, a także elastyczności i skalowalności systemów, które będą w stanie sprostać rosnącym wymaganiom."
  },
  {
    "objectID": "wyklad4.html",
    "href": "wyklad4.html",
    "title": "Wykład 4",
    "section": "",
    "text": "Rozwój technologii, szczególnie przejście od monolitów do mikroserwisów, miał ogromny wpływ na współczesne systemy informatyczne. Monolityczne aplikacje, które były dominującym podejściem w przeszłości, stanowiły jedną, dużą jednostkę kodu. Takie podejście miało swoje zalety, takie jak prostota na początkowych etapach rozwoju systemu, ale także istotne wady, w tym trudności w skalowaniu, ograniczoną elastyczność i skomplikowaną konserwację.\nW miarę jak technologia ewoluowała, pojawiły się mikroserwisy – podejście, które polega na dzieleniu aplikacji na mniejsze, niezależne usługi, z których każda odpowiada za określoną funkcjonalność. Przejście na mikroserwisy umożliwiło większą elastyczność, łatwiejsze skalowanie systemów oraz szybkie wdrażanie nowych funkcji. Ponadto każda usługa może być rozwijana, testowana i wdrażana niezależnie, co upraszcza zarządzanie kodem i zmniejsza ryzyko błędów.\nDzięki mikroserwisom organizacje mogą lepiej dostosować się do zmieniających się potrzeb biznesowych, poprawić dostępność systemów (poprzez izolowanie awarii do pojedynczych usług) oraz szybciej wprowadzać innowacje. Dodatkowo, mikroserwisy sprzyjają stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiązania, co dodatkowo ułatwia zarządzanie infrastrukturą i pozwala na lepsze wykorzystanie zasobów.\nJednakże, mimo wielu korzyści, przejście do mikroserwisów wiąże się również z wyzwaniami, takimi jak:\n\nzłożoność zarządzania komunikacją między usługami,\nkonieczność monitorowania i utrzymania większej liczby komponentów\nzarządzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzędzi i podejść do zarządzania oraz wdrożenia kultury DevOps.\nWraz z rozwojem mikroserwisów, pojawiły się także nowe technologie, takie jak serverless i konteneryzacja, które stanowią naturalne rozszerzenie elastyczności systemów. Te technologie jeszcze bardziej zwiększają efektywność zarządzania i skalowania nowoczesnych aplikacji, stając się kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w którym deweloperzy nie muszą zarządzać serwerami ani infrastrukturą. Zamiast tego, dostawcy chmurowi zajmują się całą infrastrukturą, a programiści koncentrują się jedynie na kodzie aplikacji. Kluczowym atutem tego podejścia jest jego skalowalność – aplikacje automatycznie skalują się w zależności od zapotrzebowania na zasoby. Systemy serverless pozwalają na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztów (płacisz tylko za faktyczne wykorzystanie zasobów). To podejście ułatwia zarządzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest także doskonałym uzupełnieniem dla mikroserwisów, pozwalając na uruchamianie niezależnych funkcji w odpowiedzi na różne zdarzenia, co daje jeszcze większą elastyczność. Może być wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsługa API czy automatyzacja zadań.\n\n\n\nKonteneryzacja (np. przy użyciu Docker) to kolejny krok w kierunku zwiększenia elastyczności. Dzięki kontenerom, aplikacje oraz ich zależności są zapakowane w izolowane jednostki, które można uruchamiać w różnych środowiskach w sposób spójny i przewidywalny. Kontenery są lekkie, szybkie do uruchomienia i oferują łatwość w przenoszeniu aplikacji między różnymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczególnie w połączeniu z narzędziami do zarządzania kontenerami, takimi jak Kubernetes, które automatycznie skalują aplikacje, monitorują ich stan, zapewniają wysoką dostępność oraz zarządzają ich cyklem życia. To podejście idealnie wspiera zarówno mikroserwisy, jak i serverless, umożliwiając łatwe wdrażanie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarówno serverless, jak i konteneryzacja, stanowią dalszy krok w kierunku elastyczności, oferując możliwość szybkiej reakcji na zmieniające się warunki i zapotrzebowanie. Wspólnie z mikroserwisami tworzą nowoczesne podejście do architektury aplikacji, które pozwala na rozdzielenie odpowiedzialności, łatwiejsze skalowanie, dynamiczne dostosowywanie zasobów i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umożliwia firmom szybkie wdrażanie nowych funkcji, reagowanie na zmieniające się potrzeby użytkowników oraz minimalizowanie kosztów poprzez optymalne wykorzystanie zasobów, co jest szczególnie istotne w dzisiejszym, dynamicznie zmieniającym się środowisku technologicznym."
  },
  {
    "objectID": "wyklad4.html#historia-podejścia-do-architektury",
    "href": "wyklad4.html#historia-podejścia-do-architektury",
    "title": "Wykład 4",
    "section": "",
    "text": "Rozwój technologii, szczególnie przejście od monolitów do mikroserwisów, miał ogromny wpływ na współczesne systemy informatyczne. Monolityczne aplikacje, które były dominującym podejściem w przeszłości, stanowiły jedną, dużą jednostkę kodu. Takie podejście miało swoje zalety, takie jak prostota na początkowych etapach rozwoju systemu, ale także istotne wady, w tym trudności w skalowaniu, ograniczoną elastyczność i skomplikowaną konserwację.\nW miarę jak technologia ewoluowała, pojawiły się mikroserwisy – podejście, które polega na dzieleniu aplikacji na mniejsze, niezależne usługi, z których każda odpowiada za określoną funkcjonalność. Przejście na mikroserwisy umożliwiło większą elastyczność, łatwiejsze skalowanie systemów oraz szybkie wdrażanie nowych funkcji. Ponadto każda usługa może być rozwijana, testowana i wdrażana niezależnie, co upraszcza zarządzanie kodem i zmniejsza ryzyko błędów.\nDzięki mikroserwisom organizacje mogą lepiej dostosować się do zmieniających się potrzeb biznesowych, poprawić dostępność systemów (poprzez izolowanie awarii do pojedynczych usług) oraz szybciej wprowadzać innowacje. Dodatkowo, mikroserwisy sprzyjają stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiązania, co dodatkowo ułatwia zarządzanie infrastrukturą i pozwala na lepsze wykorzystanie zasobów.\nJednakże, mimo wielu korzyści, przejście do mikroserwisów wiąże się również z wyzwaniami, takimi jak:\n\nzłożoność zarządzania komunikacją między usługami,\nkonieczność monitorowania i utrzymania większej liczby komponentów\nzarządzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzędzi i podejść do zarządzania oraz wdrożenia kultury DevOps.\nWraz z rozwojem mikroserwisów, pojawiły się także nowe technologie, takie jak serverless i konteneryzacja, które stanowią naturalne rozszerzenie elastyczności systemów. Te technologie jeszcze bardziej zwiększają efektywność zarządzania i skalowania nowoczesnych aplikacji, stając się kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w którym deweloperzy nie muszą zarządzać serwerami ani infrastrukturą. Zamiast tego, dostawcy chmurowi zajmują się całą infrastrukturą, a programiści koncentrują się jedynie na kodzie aplikacji. Kluczowym atutem tego podejścia jest jego skalowalność – aplikacje automatycznie skalują się w zależności od zapotrzebowania na zasoby. Systemy serverless pozwalają na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztów (płacisz tylko za faktyczne wykorzystanie zasobów). To podejście ułatwia zarządzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest także doskonałym uzupełnieniem dla mikroserwisów, pozwalając na uruchamianie niezależnych funkcji w odpowiedzi na różne zdarzenia, co daje jeszcze większą elastyczność. Może być wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsługa API czy automatyzacja zadań.\n\n\n\nKonteneryzacja (np. przy użyciu Docker) to kolejny krok w kierunku zwiększenia elastyczności. Dzięki kontenerom, aplikacje oraz ich zależności są zapakowane w izolowane jednostki, które można uruchamiać w różnych środowiskach w sposób spójny i przewidywalny. Kontenery są lekkie, szybkie do uruchomienia i oferują łatwość w przenoszeniu aplikacji między różnymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczególnie w połączeniu z narzędziami do zarządzania kontenerami, takimi jak Kubernetes, które automatycznie skalują aplikacje, monitorują ich stan, zapewniają wysoką dostępność oraz zarządzają ich cyklem życia. To podejście idealnie wspiera zarówno mikroserwisy, jak i serverless, umożliwiając łatwe wdrażanie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarówno serverless, jak i konteneryzacja, stanowią dalszy krok w kierunku elastyczności, oferując możliwość szybkiej reakcji na zmieniające się warunki i zapotrzebowanie. Wspólnie z mikroserwisami tworzą nowoczesne podejście do architektury aplikacji, które pozwala na rozdzielenie odpowiedzialności, łatwiejsze skalowanie, dynamiczne dostosowywanie zasobów i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umożliwia firmom szybkie wdrażanie nowych funkcji, reagowanie na zmieniające się potrzeby użytkowników oraz minimalizowanie kosztów poprzez optymalne wykorzystanie zasobów, co jest szczególnie istotne w dzisiejszym, dynamicznie zmieniającym się środowisku technologicznym."
  },
  {
    "objectID": "wyklad4.html#wpływ-technologii-na-systemy-informatyczne",
    "href": "wyklad4.html#wpływ-technologii-na-systemy-informatyczne",
    "title": "Wykład 4",
    "section": "Wpływ technologii na systemy informatyczne",
    "text": "Wpływ technologii na systemy informatyczne\nKomunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe oraz Big Data znacząco zmieniły sposób budowania systemów informatycznych i wykonywania w nich pracy.\nPodobnie, narzędzia do przekazu informacji – takie jak gazeta, radio, telewizja, internet, komunikatory i media społecznościowe – wpłynęły na interakcje międzyludzkie oraz struktury społeczne.\nKażde nowe medium technologiczne kształtuje sposób, w jaki ludzie korzystają z informatyki i postrzegają jej rolę w codziennym życiu."
  },
  {
    "objectID": "wyklad4.html#mikrousługi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "href": "wyklad4.html#mikrousługi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "title": "Wykład 4",
    "section": "Mikrousługi (Mikroserwisy) w nowoczesnej architekturze IT",
    "text": "Mikrousługi (Mikroserwisy) w nowoczesnej architekturze IT\nJednym z najpopularniejszych podejść do budowy systemów informatycznych jest koncepcja mikrousług (microservices).\nJest ona szeroko stosowana zarówno w tworzeniu oprogramowania, jak i w prowadzeniu firm opartych na analizie danych (Data-Driven).\n\nGłówne zalety mikroserwisów:\n\nWydajność – każda usługa realizuje jedno, dobrze określone zadanie (“rób jedną rzecz, ale dobrze”).\nElastyczność – umożliwiają łatwe modyfikacje i skalowanie systemu.\nPrzejrzystość architektury – system składa się z niewielkich, niezależnych modułów.\n\nMikroserwisy można porównać do czystych funkcji w programowaniu funkcyjnym – każda usługa działa niezależnie i posiada jasno określone wejścia oraz wyjścia.\nAby umożliwić komunikację między mikroserwisami, często wykorzystuje się Application Programming Interfaces (API), które pozwalają na wymianę danych i integrację różnych usług.\n\n\nPrzykład API w mikroserwisach – Python & FastAPI\nPoniżej znajduje się przykładowy mikroserwis REST API w Pythonie z użyciem FastAPI, który zwraca informacje o użytkownikach:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# Przykładowe dane użytkowników\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane użytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nJak to działa?\n\nUruchamiamy serwer FastAPI.\nMożemy uzyskać dane użytkownika, wysyłając żądanie GET do http://127.0.0.1:8000/users/1.\nAPI zwróci dane w formacie JSON, np.:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API (Application Programming Interface).\nAPI umożliwia komunikację i integrację między różnymi mikroserwisami, podobnie jak strona internetowa komunikuje się z przeglądarką.\nGdy odwiedzasz stronę internetową, serwer wysyła kod, który Twoja przeglądarka interpretuje i wyświetla jako stronę internetową.\nPodobnie działa API – wysyła odpowiedzi na zapytania klienta."
  },
  {
    "objectID": "wyklad4.html#przypadek-biznesowy-model-ml-jako-usługa",
    "href": "wyklad4.html#przypadek-biznesowy-model-ml-jako-usługa",
    "title": "Wykład 4",
    "section": "Przypadek biznesowy: Model ML jako usługa",
    "text": "Przypadek biznesowy: Model ML jako usługa\nZałóżmy, że pracujesz w firmie zajmującej się sprzedażą nieruchomości w Bostonie.\nChcesz zwiększyć sprzedaż i poprawić jakość obsługi klientów poprzez nową aplikację mobilną, z której może korzystać nawet 1 000 000 użytkowników jednocześnie.\nJednym z rozwiązań jest udostępnienie prognozy wartości domu w czasie rzeczywistym.\nGdy użytkownik prosi o wycenę nieruchomości, aplikacja wysyła zapytanie do serwera, który przetwarza je za pomocą modelu Machine Learning (ML) i zwraca oszacowaną wartość.\n\nCzym jest serwowanie modelu ML?\nTrening dobrego modelu ML to dopiero pierwszy krok całego procesu.\nAby model był użyteczny, musisz go udostępnić użytkownikom końcowym, np. w formie API.\n\n\nJak to zrobić?\n\nPotrzebujesz:\n\n\nwytrenowanego modelu ML,\ninterpreter modelu (np. TensorFlow, Scikit-Learn, PyTorch),\ndanych wejściowych dla modelu.\n\n\nKluczowe metryki jakości serwowania modelu:\nCzas odpowiedzi (latency),\nKoszt uruchomienia modelu (infrastruktura serwerowa),\nLiczba zapytań na sekundę (QPS – Queries Per Second).\n\n\nUdostępnianie danych między systemami zawsze było kluczowym wyzwaniem w tworzeniu oprogramowania.\nW obszarze tradycyjnego DevOps koncentrujemy się na infrastrukturze, a w MLOps – na wdrażaniu i utrzymaniu modeli uczenia maszynowego.\n\nZbudowanie systemu przygotowanego do środowiska produkcyjnego jest bardziej skomplikowane niż wytrenowanie samego modelu:\n\nczyszczenie i załadowanie odpowiednich i zwalidowanych danych\nObliczenie zmiennych i ich serwowanie na właściwym środowisku\nSerwowanie modelu w sposób najbardziej efektywny ze względu na koszty\nWersjonowanie, śledzenie i udostępnianie danych i modeli oraz innych artefaktów\nMonitorowanie infrastruktury i modelu\nWdrożenie modelu na skalowalnej infrastrukturze\nAutomatyzacja procesu wdrażania i treningu"
  },
  {
    "objectID": "wyklad4.html#jak-działa-api",
    "href": "wyklad4.html#jak-działa-api",
    "title": "Wykład 4",
    "section": "Jak działa API?",
    "text": "Jak działa API?\nKiedy wywołasz interfejs API, serwer otrzymuje Twoje żądanie, przetwarza je i generuje odpowiedź.\nJeśli wszystko działa poprawnie, otrzymasz wynik w formacie JSON lub XML.\nJeśli wystąpi błąd, API zwróci kod błędu HTTP (np. 400 – nieprawidłowe żądanie, 500 – błąd serwera).\n\nKluczowe zasady REST API:\n\nKlient-Serwer → Klient (np. aplikacja mobilna) wysyła żądanie HTTP do API hostowanego na serwerze, który zwraca odpowiedź.\nDziała to identycznie jak przeglądarka internetowa, która wysyła żądanie do serwera WWW i otrzymuje stronę HTML.\nBezstanowość → Każde żądanie klienta musi zawierać wszystkie niezbędne informacje do przetworzenia odpowiedzi,\nAPI nie powinno przechowywać informacji o wcześniejszych żądaniach użytkownika.\n\n\n\nPrzykład: API serwujące model ML\nPoniżej znajduje się przykładowy serwis API, który udostępnia model ML do prognozowania ceny nieruchomości,\nz wykorzystaniem FastAPI oraz Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wcześniej wytrenowany model ML (np. regresję liniową)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Prognoza ceny nieruchomości na podstawie cech:\n    - area (powierzchnia w m²),\n    - bedrooms (liczba sypialni),\n    - age (wiek budynku w latach).\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n- domenę, \n- port, \n- dodatkowe ścieżki, \n- zapytanie\nMetody HTTP:\n- GET, \n- POST\nNagłówki HTTP zawierają:\n- informacje o autoryzacji, \n- cookies metadata\n\nCała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens 4. Ciało zapytania\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidłowe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API."
  },
  {
    "objectID": "wyklad4.html#publikujsubskrybuj",
    "href": "wyklad4.html#publikujsubskrybuj",
    "title": "Wykład 4",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyłania wiadomości „Publikuj/Subskrybuj” ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujący się tym, że nadawca (publikujący) fragmentu danych (wiadomości) nie kieruje go wprost do odbiorcy. pub/sub to systemy, które często posiadają brokera czyli centralny punkt, w którym znajdują się wiadomości."
  },
  {
    "objectID": "wyklad4.html#apache-kafka",
    "href": "wyklad4.html#apache-kafka",
    "title": "Wykład 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicję:\n\nRozproszona platforma streamingowa\nCo to jest „platforma rozproszonego przesyłania strumieniowego”?\nNajpierw chcę przypomnieć, czym jest „strumień”. Strumienie to po prostu nieograniczone dane, dane, które nigdy się nie kończą. Ciągle ich przybywa i możesz przetwarzać je w czasie rzeczywistym.\nA „rozproszone”? Rozproszony oznacza, że ​​Kafka działa w klastrze, a każdy węzeł w grupie nazywa się Brokerem. Ci brokerzy to po prostu serwery wykonujące kopię Apache Kafka.\nTak więc Kafka to zestaw współpracujących ze sobą maszyn, aby móc obsługiwać i przetwarzać nieograniczone dane w czasie rzeczywistym.\nBrokerzy sprawiają, że jest niezawodny, skalowalny i odporny na błędy. Ale dlaczego panuje błędne przekonanie, że Kafka to kolejny „kolejkowy system przesyłania wiadomości”?\nAby odpowiedzieć na tę odpowiedź, musimy najpierw wyjaśnić, jak działa kolejkowe przesyłanie wiadomości.\n\n\nKolejkowy system przesyłania wiadomości\nPrzesyłanie wiadomości, to po prostu czynność wysyłania wiadomości z jednego miejsca do drugiego. Ma trzech głównych “aktorów”:\n\nProducent: Który tworzy i wysyła komunikaty do jednej lub więcej kolejek;\nKolejka: struktura danych bufora, która odbiera (od producentów) i dostarcza komunikaty (do konsumentów) w sposób FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jedną lub więcej kolejek i otrzymuje ich wiadomości po opublikowaniu.\n\nI to jest to; tak działa przesyłanie wiadomości. Jak widać, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Apache Kafka\nWięcej informacji na temat Kafki znajdziesz w tym linku.\nTeraz, gdy rozumiemy podstawy przesyłania wiadomości, zagłębmy się w świat Apache Kafka.\nW Kafce mamy dwa kluczowe pojęcia: Producentów (Producers) i Konsumentów (Consumers),\nktórzy działają w podobny sposób jak w klasycznych systemach kolejkowych, produkując i konsumując wiadomości.\n\n\nJak widać, Kafka przypomina klasyczny system przesyłania wiadomości, jednak w odróżnieniu od tradycyjnych kolejek,\nzamiast pojęcia kolejki (queue) mamy Tematy (Topics).\n\n\nTematy (Topics) i Partycje (Partitions)\nTemat (Topic) to podstawowy kanał przesyłania danych w Kafce.\nMożna go porównać do folderu, w którym przechowywane są wiadomości.\nKażdy temat posiada jedną lub więcej partycji (Partitions) – podział ten wpływa na skalowalność i równoważenie obciążenia.\nPodczas tworzenia tematu określamy liczbę partycji.\n\nKluczowe cechy tematów i partycji:\n\nTemat to logiczna jednostka, do której producenci wysyłają wiadomości, a konsumenci je odczytują.\nPartycja to fizyczny podział tematu. Można ją porównać do plików w folderze.\nOffset – każda wiadomość w partycji otrzymuje unikalny identyfikator (offset),\nktóry pozwala konsumentom śledzić, które wiadomości zostały już przetworzone.\nKafka przechowuje wiadomości na dysku, dzięki czemu może je ponownie odczytać (w przeciwieństwie do klasycznych kolejek, gdzie wiadomość jest usuwana po przetworzeniu).\nKonsumenci odczytują wiadomości sekwencyjnie, od najstarszej do najnowszej.\nW przypadku awarii konsument może wznowić przetwarzanie od ostatniego zapisanego offsetu.\n\n\n\n\n\n\nBrokerzy (Brokers) i Klaster Kafka\nKafka działa w sposób rozproszony – oznacza to, że może składać się z wielu brokerów (Brokers),\nktóre współpracują jako jeden klaster.\n\n\n\nKluczowe informacje o brokerach\n\nBroker to pojedynczy serwer w klastrze Kafki, odpowiedzialny za przechowywanie partycji tematów.\nKażdy broker w klastrze ma unikalny identyfikator.\nAby zwiększyć dostępność i niezawodność, Kafka wykorzystuje replikację danych.\nWspółczynnik replikacji określa, ile kopii danej partycji ma być przechowywane na różnych brokerach.\nJeśli temat ma trzy partycje i współczynnik replikacji równy trzy,\noznacza to, że każda partycja zostanie powielona na trzech różnych brokerach.\n\nLiczba partycji powinna być dobrana w taki sposób, aby każdy broker miał co najmniej jedną partycję do obsługi.\n\n\n\nProducenci (Producers)\nW Kafka producenci to aplikacje lub usługi, które tworzą i wysyłają wiadomości do tematów.\nDziała to podobnie do systemów kolejkowych, z tą różnicą, że Kafka zapisuje wiadomości w partycjach.\n\nJak Kafka przypisuje wiadomości do partycji?\n\nWiadomości są rozsyłane okrężnie (round-robin) do dostępnych partycji.\nMożemy określić klucz wiadomości, a Kafka wyliczy jego hash,\naby określić, do której partycji trafi wiadomość.\nKlucz wiadomości determinuje przypisanie do partycji – jeśli temat został już utworzony,\nliczba partycji nie może być zmieniona bez zakłócenia tego mechanizmu.\n\nPrzykład przypisania wiadomości do partycji: - Wiadomość 01 trafia do partycji 0 tematu Topic_1. - Wiadomość 02 trafia do partycji 1 tego samego tematu. - Kolejna wiadomość może ponownie trafić do partycji 0, jeśli stosujemy przypisanie round-robin.\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# Wysyłamy wiadomość do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"Nowe mieszkanie na sprzedaż\"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"Wiadomość wysłana do tematu '{topic}'\")\n\n\n\nKonsumenci (Consumers)\nKonsumenci w Kafce odczytują i przetwarzają wiadomości z tematów. Każdy konsument może należeć do grupy konsumentów (Consumer Group), co pozwala na równoległe przetwarzanie wiadomości. - Jeśli wielu konsumentów należy do tej samej grupy, Kafka równoważy obciążenie między nimi. - Jeśli jeden konsument przestanie działać, Kafka automatycznie przypisze jego partycje do innego aktywnego konsumenta.\nPrzykład konsumenta w Pythonie:\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, który nasłuchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Oczekiwanie na wiadomości...\")\n\nfor message in consumer:\n    print(f\"Otrzymano wiadomość: {message.value.decode()}\")\nInnym ważnym pojęciem Kafki są „Grupy konsumentów”. Jest to bardzo ważne, gdy musimy skalować odczytywanie wiadomości. Staje się to bardzo kosztowne, gdy pojedynczy konsument musi czytać z wielu partycji, więc musimy zrównoważyć obciążenie między naszymi konsumentami, wtedy wchodzą grupy konsumentów.\nDane z jednego tematu będą równoważone obciążeniem między konsumentami, dzięki czemu możemy zagwarantować, że nasi konsumenci będą w stanie obsługiwać i przetwarzać dane. Ideałem jest posiadanie takiej samej liczby konsumentów w grupie, jaką mamy jako partycje w temacie, w ten sposób każdy konsument czyta tylko z jednego. Podczas dodawania konsumentów do grupy należy uważać, jeśli liczba konsumentów jest większa niż liczba partycji, niektórzy konsumenci nie będą czytać z żadnego tematu i pozostaną bezczynni."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiały znajdziesz na liście książek.\nMateriały z wykładu i laboratoriów nie są wspierane przez Google. Obecność na wykładach i ćwiczeniach nie zmniejszy Twoich 5 dolarów."
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli VI bud G\n\n\n18-02-2025 (wtorek) 13:30-15:10 - Wykład 1\n\n\n25-02-2025 (wtorek) 13:30-15:10 - Wykład 2\n\n\n04-03-2025 (wtorek) 13:30-15:10 - Wykład 3 online\n\n11-03-2025 (wtorek) 13:30-15:10 - Wykład 4\n18-03-2025 (wtorek) 13:30-15:10 - Wykład 5\n\nWykład 5 kończy się TESTEM: 20 pytań - 30 minut. Test przeprowadzany jest za pośrednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziałek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykłady zakończą się testem (podczas ostatnich zajęć).\nAby zaliczyć test, należy zdobyć więcej niż 13 punktów – jest to warunek konieczny do uczestnictwa w ćwiczeniach.\nLaboratoria\nPodczas laboratoriów będą zadawane prace domowe, które należy przesyłać za pośrednictwem MS Teams. Każdy brak pracy domowej obniża końcową ocenę o 0,5 stopnia.\n\nProjekt\nProjekty należy realizować w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiązywać realny problem biznesowy, który można opracować przy użyciu danych przetwarzanych w trybie online. (Nie wyklucza to użycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny być przesyłane do Apache Kafka, skąd będą poddawane dalszemu przetwarzaniu i analizie.\nMożna używać dowolnego języka programowania w każdym komponencie projektu.\nMożna wykorzystać narzędzia BI.\nŹródłem danych może być dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "plan_wyklady.html",
    "href": "plan_wyklady.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#plan-wykładu",
    "href": "plan_wyklady.html#plan-wykładu",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "🧑‍🏫 Wykłady (teoria + case study z biznesu)\n1️⃣ Wprowadzenie: Ewolucja analizy danych Dane strukturyzowane (SQL, Pandas) vs. nieustrukturyzowane (teksty, obrazy, grafy). Przetwarzanie wsadowe (batch processing) vs. strumieniowe (stream processing). Case study: Jak firmy przechodzą od tabel do analizy strumieniowej? 2️⃣ Systemy przetwarzania danych w czasie rzeczywistym Modele danych: relacyjne (PostgreSQL), grafowe (NetworkX), strumieniowe (Kafka). Lambda i Kappa Architecture – różnice i zastosowania. Case study: Rekomendacje produktowe w e-commerce. 3️⃣ Modele ML/DL dla danych w czasie rzeczywistym Uczenie wsadowe (batch) vs. przyrostowe (online learning). Stochastic Gradient Descent (SGD) – podstawa ML na strumieniach. Case study: Klasyfikacja oszustw w czasie rzeczywistym. 4️⃣ Obiektowe programowanie w Pythonie w kontekście ML Struktury klasowe dla modeli ML. Tworzenie pipeline’ów ML w Pythonie. Case study: Klasyfikacja wiadomości jako SPAM/NON-SPAM w strumieniu tekstów. 5️⃣ Tworzenie API z regułami decyzyjnymi i ML Budowa API w FastAPI dla modelu ML. Integracja modelu klasyfikacji z systemem decyzyjnym. Case study: System wykrywania anomalii w logach serwerowych. 🛠 Laboratoria (praktyka + implementacja w Pythonie)\n🔹 Lab 1: Struktury danych w Pythonie – Pandas, SQL (PostgreSQL, SQLite). 🔹 Lab 2: Dane grafowe w analizie relacji – NetworkX i algorytmy grafowe. 🔹 Lab 3: Analiza tekstów i NLP – przetwarzanie danych tekstowych (spaCy, TF-IDF). 🔹 Lab 4: Strumieniowanie danych w Apache Kafka – pierwsza aplikacja Python + Kafka. 🔹 Lab 5: Uczenie maszynowe na strumieniu – klasyfikacja w czasie rzeczywistym (SGDClassifier). 🔹 Lab 6: Przygotowanie API w FastAPI – serwowanie modelu ML. 🔹 Lab 7: Zastosowanie modelu ML w regułach decyzyjnych – integracja API z logiką biznesową. 🔹 Lab 8: Przetwarzanie obrazów w czasie rzeczywistym – OpenCV + klasyfikacja wideo. 🔹 Lab 9: Wykrywanie oszustw w transakcjach finansowych – online learning na Kafka. 🔹 Lab 10: Projekt końcowy – budowa mikroserwisu do analizy danych w czasie rzeczywistym."
  },
  {
    "objectID": "plan_wyklady.html#moje",
    "href": "plan_wyklady.html#moje",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Moje",
    "text": "Moje\n\nwprowadzenie\nBatch processing\n\n\ntypy danych\nBig data\nETL\nMAP Reduce\nSparkowe przetwarzanie klastrowe\nBazy SQL - OLTP, OLAP\n\n\nAPI online\n\n\nwystawienie serwisu LLM\nbatching\n\n\nNear Real-Time i Real Time\n\n\nStrumienie danych, definicje, biznes,\nLambda/Kappa\nPub Sub, Kafka"
  }
]