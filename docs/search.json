[
  {
    "objectID": "kafka_codes/kafka1.html",
    "href": "kafka_codes/kafka1.html",
    "title": "Wprowadzenie",
    "section": "",
    "text": "Apache Kafka to system przetwarzania strumieniowego (event streaming), ktÃ³ry dziaÅ‚a jako rozproszony broker wiadomoÅ›ci. Pozwala na przesyÅ‚anie i przetwarzanie danych w czasie rzeczywistym.\nDomyÅ›lnym adresem naszego brokera jest broker:9092.\nW Apache Kafka dane sÄ… przechowywane w strukturach zwanych topicami, ktÃ³re peÅ‚niÄ… funkcjÄ™ kolejek komunikacyjnych.\nZarzÄ…dzanie KafkÄ… odbywa siÄ™ za pomocÄ… skryptÃ³w. W naszym przypadku bÄ™dÄ… to skrypty .sh.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#sprawdÅº-listÄ™-topicÃ³w",
    "href": "kafka_codes/kafka1.html#sprawdÅº-listÄ™-topicÃ³w",
    "title": "Wprowadzenie",
    "section": "1ï¸âƒ£ SprawdÅº listÄ™ topicÃ³w",
    "text": "1ï¸âƒ£ SprawdÅº listÄ™ topicÃ³w\nPamiÄ™taj, aby przejÅ›Ä‡ do katalogu domowego:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#utwÃ³rz-nowy-topic-o-nazwie-mytopic",
    "href": "kafka_codes/kafka1.html#utwÃ³rz-nowy-topic-o-nazwie-mytopic",
    "title": "Wprowadzenie",
    "section": "2ï¸âƒ£ UtwÃ³rz nowy topic o nazwie mytopic",
    "text": "2ï¸âƒ£ UtwÃ³rz nowy topic o nazwie mytopic\nkafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#utwÃ³rz-producenta-w-terminalu",
    "href": "kafka_codes/kafka1.html#utwÃ³rz-producenta-w-terminalu",
    "title": "Wprowadzenie",
    "section": "3ï¸âƒ£ UtwÃ³rz producenta w terminalu",
    "text": "3ï¸âƒ£ UtwÃ³rz producenta w terminalu\nTen skrypt pozwoli Ci wprowadzaÄ‡ eventy rÄ™cznie przez terminal. Opcje --property sÄ… dodatkowe i sÅ‚uÅ¼Ä… do analizy w tym przykÅ‚adzie.\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#consumer-w-sparku",
    "href": "kafka_codes/kafka1.html#consumer-w-sparku",
    "title": "Wprowadzenie",
    "section": "4ï¸âƒ£ Consumer w Sparku",
    "text": "4ï¸âƒ£ Consumer w Sparku\nOtwÃ³rz nowy terminal w miejscu, gdzie znajduje siÄ™ plik test_key_value.py, i uruchom program Consumera w Sparku.\n\n%%file test_key_value.py\nfrom pyspark.sql import SparkSession\n\nKAFKA_BROKER = 'broker:9092'\nKAFKA_TOPIC = 'mytopic'\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\ndf = (spark.readStream.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n      .option(\"subscribe\", KAFKA_TOPIC)\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n     )\n\n# Konwersja danych binarnych na stringi\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n .writeStream \\\n .format(\"console\") \\\n .outputMode(\"append\") \\\n .start() \\\n .awaitTermination()\n\nPamiÄ™taj, Å¼e Apache Spark nie posiada domyÅ›lnego konektora do Kafki, dlatego uruchom proces za pomocÄ… spark-submit i pobierz odpowiedni pakiet w Scali:\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#przetestuj-przesyÅ‚anie-danych",
    "href": "kafka_codes/kafka1.html#przetestuj-przesyÅ‚anie-danych",
    "title": "Wprowadzenie",
    "section": "5ï¸âƒ£ Przetestuj przesyÅ‚anie danych",
    "text": "5ï¸âƒ£ Przetestuj przesyÅ‚anie danych\nW terminalu z uruchomionym producentem wpisz tekst w postaci:\njan:45\nalicja:20\nSprawdÅº, co pojawia siÄ™ w oknie aplikacji Consumera.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#zakoÅ„czenie-procesu",
    "href": "kafka_codes/kafka1.html#zakoÅ„czenie-procesu",
    "title": "Wprowadzenie",
    "section": "6ï¸âƒ£ ZakoÅ„czenie procesu",
    "text": "6ï¸âƒ£ ZakoÅ„czenie procesu\nPo zakoÅ„czeniu pokazu uÅ¼yj Ctrl+C, aby zamknÄ…Ä‡ zarÃ³wno okno producenta, jak i aplikacjÄ™ Spark.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html",
    "href": "kafka_codes/kafka2.html",
    "title": "Producent w Å›rodowisku python",
    "section": "",
    "text": "TÄ™ wersjÄ™ Ä‡wiczeÅ„ moÅ¼na przejÅ›Ä‡, posiadajÄ…c nowy obraz Dockerowy i uruchomiony Docker Desktop na wÅ‚asnym komputerze. Jak rÃ³wnieÅ¼ na Å›rodowisku SGH.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-Å›rodowiska",
    "href": "kafka_codes/kafka2.html#uruchomienie-Å›rodowiska",
    "title": "Producent w Å›rodowisku python",
    "section": "1ï¸âƒ£ Uruchomienie Å›rodowiska",
    "text": "1ï¸âƒ£ Uruchomienie Å›rodowiska\nPrzejdÅº do przeglÄ…darki i otwÃ³rz stronÄ™ ze Å›rodowiskiem (w przypadku Dockera otwÃ³rz localhost:8888).\nUruchom Jupyter Lab, a nastÄ™pnie otwÃ³rz nowy terminal (za pomocÄ… ikony terminala).",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#sprawdzenie-katalogÃ³w-i-dostÄ™pnoÅ›ci-kafki",
    "href": "kafka_codes/kafka2.html#sprawdzenie-katalogÃ³w-i-dostÄ™pnoÅ›ci-kafki",
    "title": "Producent w Å›rodowisku python",
    "section": "2ï¸âƒ£ Sprawdzenie katalogÃ³w i dostÄ™pnoÅ›ci Kafki",
    "text": "2ï¸âƒ£ Sprawdzenie katalogÃ³w i dostÄ™pnoÅ›ci Kafki\nPrzejdÅº do katalogu gÅ‚Ã³wnego i wypisz listÄ™ wszystkich elementÃ³w. SprawdÅº, czy na liÅ›cie znajduje siÄ™ katalog kafka.\ncd ~\nls -la",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#sprawdzenie-listy-topicÃ³w",
    "href": "kafka_codes/kafka2.html#sprawdzenie-listy-topicÃ³w",
    "title": "Producent w Å›rodowisku python",
    "section": "3ï¸âƒ£ Sprawdzenie listy topicÃ³w",
    "text": "3ï¸âƒ£ Sprawdzenie listy topicÃ³w\nUruchom polecenie sprawdzajÄ…ce listÄ™ topicÃ³w serwera Kafki:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#dodanie-nowego-topicu",
    "href": "kafka_codes/kafka2.html#dodanie-nowego-topicu",
    "title": "Producent w Å›rodowisku python",
    "section": "4ï¸âƒ£ Dodanie nowego topicu",
    "text": "4ï¸âƒ£ Dodanie nowego topicu\nDodaj topic o nazwie streaming:\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nSprawdÅº ponownie listÄ™ topicÃ³w, upewniajÄ…c siÄ™, Å¼e streaming zostaÅ‚ dodany:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep streaming",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-producenta-w-pythonie",
    "href": "kafka_codes/kafka2.html#uruchomienie-producenta-w-pythonie",
    "title": "Producent w Å›rodowisku python",
    "section": "5ï¸âƒ£ Uruchomienie producenta w Pythonie",
    "text": "5ï¸âƒ£ Uruchomienie producenta w Pythonie\nW nowym terminalu utwÃ³rz plik stream.py i wklej poniÅ¼szy kod:\n\n%%file stream.py\nimport json\nimport random\nimport sys\nfrom datetime import datetime\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nSERVER = \"broker:9092\"\nTOPIC = \"streaming\"\n\nif __name__ == \"__main__\":\n    \n    \n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\")\n    )\n    \n    try:\n        while True:\n            \n            message = {\n                \"time\": str(datetime.now()),\n                \"id\": random.choice(['a','b','c','d']),\n                \"value\": random.randint(0,100)\n            }\n            producer.send(TOPIC, value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()\n\nWriting stream.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#uruchomienie-konsumenta-w-konsoli",
    "href": "kafka_codes/kafka2.html#uruchomienie-konsumenta-w-konsoli",
    "title": "Producent w Å›rodowisku python",
    "section": "6ï¸âƒ£ Uruchomienie konsumenta w konsoli",
    "text": "6ï¸âƒ£ Uruchomienie konsumenta w konsoli\nAby sprawdziÄ‡, czy wysyÅ‚anie wiadomoÅ›ci dziaÅ‚a, otwÃ³rz kolejne okno terminala i uruchom konsumenta:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\nTeraz wszystkie wiadomoÅ›ci wysÅ‚ane przez producenta powinny pojawiÄ‡ siÄ™ w konsoli konsumenta.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#zakoÅ„czenie-pracy",
    "href": "kafka_codes/kafka2.html#zakoÅ„czenie-pracy",
    "title": "Producent w Å›rodowisku python",
    "section": "7ï¸âƒ£ ZakoÅ„czenie pracy",
    "text": "7ï¸âƒ£ ZakoÅ„czenie pracy\nPamiÄ™taj, aby uruchamiaÄ‡ komendy z odpowiedniego katalogu. Po zakoÅ„czeniu Ä‡wiczeÅ„ uÅ¼yj Ctrl+C, aby zatrzymaÄ‡ zarÃ³wno producenta, jak i konsumenta.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Producent w Å›rodowisku python"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html",
    "href": "lectures/wyklad4.html",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#historia-podejÅ›cia-do-architektury",
    "href": "lectures/wyklad4.html#historia-podejÅ›cia-do-architektury",
    "title": "WykÅ‚ad 4",
    "section": "",
    "text": "RozwÃ³j technologii, szczegÃ³lnie przejÅ›cie od monolitÃ³w do mikroserwisÃ³w, miaÅ‚ ogromny wpÅ‚yw na wspÃ³Å‚czesne systemy informatyczne. Monolityczne aplikacje, ktÃ³re byÅ‚y dominujÄ…cym podejÅ›ciem w przeszÅ‚oÅ›ci, stanowiÅ‚y jednÄ…, duÅ¼Ä… jednostkÄ™ kodu. Takie podejÅ›cie miaÅ‚o swoje zalety, takie jak prostota na poczÄ…tkowych etapach rozwoju systemu, ale takÅ¼e istotne wady, w tym trudnoÅ›ci w skalowaniu, ograniczonÄ… elastycznoÅ›Ä‡ i skomplikowanÄ… konserwacjÄ™.\nW miarÄ™ jak technologia ewoluowaÅ‚a, pojawiÅ‚y siÄ™ mikroserwisy â€“ podejÅ›cie, ktÃ³re polega na dzieleniu aplikacji na mniejsze, niezaleÅ¼ne usÅ‚ugi, z ktÃ³rych kaÅ¼da odpowiada za okreÅ›lonÄ… funkcjonalnoÅ›Ä‡. PrzejÅ›cie na mikroserwisy umoÅ¼liwiÅ‚o wiÄ™kszÄ… elastycznoÅ›Ä‡, Å‚atwiejsze skalowanie systemÃ³w oraz szybkie wdraÅ¼anie nowych funkcji. Ponadto kaÅ¼da usÅ‚uga moÅ¼e byÄ‡ rozwijana, testowana i wdraÅ¼ana niezaleÅ¼nie, co upraszcza zarzÄ…dzanie kodem i zmniejsza ryzyko bÅ‚Ä™dÃ³w.\nDziÄ™ki mikroserwisom organizacje mogÄ… lepiej dostosowaÄ‡ siÄ™ do zmieniajÄ…cych siÄ™ potrzeb biznesowych, poprawiÄ‡ dostÄ™pnoÅ›Ä‡ systemÃ³w (poprzez izolowanie awarii do pojedynczych usÅ‚ug) oraz szybciej wprowadzaÄ‡ innowacje. Dodatkowo, mikroserwisy sprzyjajÄ… stosowaniu nowoczesnych metod takich jak konteneryzacja i chmurowe rozwiÄ…zania, co dodatkowo uÅ‚atwia zarzÄ…dzanie infrastrukturÄ… i pozwala na lepsze wykorzystanie zasobÃ³w.\nJednakÅ¼e, mimo wielu korzyÅ›ci, przejÅ›cie do mikroserwisÃ³w wiÄ…Å¼e siÄ™ rÃ³wnieÅ¼ z wyzwaniami, takimi jak:\n\nzÅ‚oÅ¼onoÅ›Ä‡ zarzÄ…dzania komunikacjÄ… miÄ™dzy usÅ‚ugami,\nkoniecznoÅ›Ä‡ monitorowania i utrzymania wiÄ™kszej liczby komponentÃ³w\nzarzÄ…dzanie transakcjami rozproszonymi.\n\nWymaga to nowych narzÄ™dzi i podejÅ›Ä‡ do zarzÄ…dzania oraz wdroÅ¼enia kultury DevOps.\nWraz z rozwojem mikroserwisÃ³w, pojawiÅ‚y siÄ™ takÅ¼e nowe technologie, takie jak serverless i konteneryzacja, ktÃ³re stanowiÄ… naturalne rozszerzenie elastycznoÅ›ci systemÃ³w. Te technologie jeszcze bardziej zwiÄ™kszajÄ… efektywnoÅ›Ä‡ zarzÄ…dzania i skalowania nowoczesnych aplikacji, stajÄ…c siÄ™ kluczowymi elementami w ekosystemie chmurowym.\n\n\nServerless to model, w ktÃ³rym deweloperzy nie muszÄ… zarzÄ…dzaÄ‡ serwerami ani infrastrukturÄ…. Zamiast tego, dostawcy chmurowi zajmujÄ… siÄ™ caÅ‚Ä… infrastrukturÄ…, a programiÅ›ci koncentrujÄ… siÄ™ jedynie na kodzie aplikacji. Kluczowym atutem tego podejÅ›cia jest jego skalowalnoÅ›Ä‡ â€“ aplikacje automatycznie skalujÄ… siÄ™ w zaleÅ¼noÅ›ci od zapotrzebowania na zasoby. Systemy serverless pozwalajÄ… na dynamiczne uruchamianie i zatrzymywanie funkcji w odpowiedzi na konkretne zdarzenia, co prowadzi do optymalizacji kosztÃ³w (pÅ‚acisz tylko za faktyczne wykorzystanie zasobÃ³w). To podejÅ›cie uÅ‚atwia zarzÄ…dzanie aplikacjami o zmiennym lub trudnym do przewidzenia ruchu.\nServerless jest takÅ¼e doskonaÅ‚ym uzupeÅ‚nieniem dla mikroserwisÃ³w, pozwalajÄ…c na uruchamianie niezaleÅ¼nych funkcji w odpowiedzi na rÃ³Å¼ne zdarzenia, co daje jeszcze wiÄ™kszÄ… elastycznoÅ›Ä‡. MoÅ¼e byÄ‡ wykorzystywane w takich zastosowaniach jak przetwarzanie danych w czasie rzeczywistym, obsÅ‚uga API czy automatyzacja zadaÅ„.\n\n\n\nKonteneryzacja (np. przy uÅ¼yciu Docker) to kolejny krok w kierunku zwiÄ™kszenia elastycznoÅ›ci. DziÄ™ki kontenerom, aplikacje oraz ich zaleÅ¼noÅ›ci sÄ… zapakowane w izolowane jednostki, ktÃ³re moÅ¼na uruchamiaÄ‡ w rÃ³Å¼nych Å›rodowiskach w sposÃ³b spÃ³jny i przewidywalny. Kontenery sÄ… lekkie, szybkie do uruchomienia i oferujÄ… Å‚atwoÅ›Ä‡ w przenoszeniu aplikacji miÄ™dzy rÃ³Å¼nymi platformami, co jest kluczowe w architekturach mikroserwisowych.\nKonteneryzacja zyskuje na znaczeniu, szczegÃ³lnie w poÅ‚Ä…czeniu z narzÄ™dziami do zarzÄ…dzania kontenerami, takimi jak Kubernetes, ktÃ³re automatycznie skalujÄ… aplikacje, monitorujÄ… ich stan, zapewniajÄ… wysokÄ… dostÄ™pnoÅ›Ä‡ oraz zarzÄ…dzajÄ… ich cyklem Å¼ycia. To podejÅ›cie idealnie wspiera zarÃ³wno mikroserwisy, jak i serverless, umoÅ¼liwiajÄ…c Å‚atwe wdraÅ¼anie, skalowanie i monitorowanie aplikacji.\n\n\n\nZarÃ³wno serverless, jak i konteneryzacja, stanowiÄ… dalszy krok w kierunku elastycznoÅ›ci, oferujÄ…c moÅ¼liwoÅ›Ä‡ szybkiej reakcji na zmieniajÄ…ce siÄ™ warunki i zapotrzebowanie. WspÃ³lnie z mikroserwisami tworzÄ… nowoczesne podejÅ›cie do architektury aplikacji, ktÃ³re pozwala na rozdzielenie odpowiedzialnoÅ›ci, Å‚atwiejsze skalowanie, dynamiczne dostosowywanie zasobÃ³w i lepsze wykorzystanie infrastruktury chmurowej.\nKombinacja tych technologii umoÅ¼liwia firmom szybkie wdraÅ¼anie nowych funkcji, reagowanie na zmieniajÄ…ce siÄ™ potrzeby uÅ¼ytkownikÃ³w oraz minimalizowanie kosztÃ³w poprzez optymalne wykorzystanie zasobÃ³w, co jest szczegÃ³lnie istotne w dzisiejszym, dynamicznie zmieniajÄ…cym siÄ™ Å›rodowisku technologicznym.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "href": "lectures/wyklad4.html#wpÅ‚yw-technologii-na-systemy-informatyczne",
    "title": "WykÅ‚ad 4",
    "section": "WpÅ‚yw technologii na systemy informatyczne",
    "text": "WpÅ‚yw technologii na systemy informatyczne\nKomunikacja sieciowa, relacyjne bazy danych, rozwiÄ…zania chmurowe oraz Big Data znaczÄ…co zmieniÅ‚y sposÃ³b budowania systemÃ³w informatycznych i wykonywania w nich pracy.\nPodobnie, narzÄ™dzia do przekazu informacji â€“ takie jak gazeta, radio, telewizja, internet, komunikatory i media spoÅ‚ecznoÅ›ciowe â€“ wpÅ‚ynÄ™Å‚y na interakcje miÄ™dzyludzkie oraz struktury spoÅ‚eczne.\nKaÅ¼de nowe medium technologiczne ksztaÅ‚tuje sposÃ³b, w jaki ludzie korzystajÄ… z informatyki i postrzegajÄ… jej rolÄ™ w codziennym Å¼yciu.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "href": "lectures/wyklad4.html#mikrousÅ‚ugi-mikroserwisy-w-nowoczesnej-architekturze-it",
    "title": "WykÅ‚ad 4",
    "section": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT",
    "text": "MikrousÅ‚ugi (Mikroserwisy) w nowoczesnej architekturze IT\nJednym z najpopularniejszych podejÅ›Ä‡ do budowy systemÃ³w informatycznych jest koncepcja mikrousÅ‚ug (microservices).\nJest ona szeroko stosowana zarÃ³wno w tworzeniu oprogramowania, jak i w prowadzeniu firm opartych na analizie danych (Data-Driven).\n\nGÅ‚Ã³wne zalety mikroserwisÃ³w:\n\nWydajnoÅ›Ä‡ â€“ kaÅ¼da usÅ‚uga realizuje jedno, dobrze okreÅ›lone zadanie (â€œrÃ³b jednÄ… rzecz, ale dobrzeâ€).\nElastycznoÅ›Ä‡ â€“ umoÅ¼liwiajÄ… Å‚atwe modyfikacje i skalowanie systemu.\nPrzejrzystoÅ›Ä‡ architektury â€“ system skÅ‚ada siÄ™ z niewielkich, niezaleÅ¼nych moduÅ‚Ã³w.\n\nMikroserwisy moÅ¼na porÃ³wnaÄ‡ do czystych funkcji w programowaniu funkcyjnym â€“ kaÅ¼da usÅ‚uga dziaÅ‚a niezaleÅ¼nie i posiada jasno okreÅ›lone wejÅ›cia oraz wyjÅ›cia.\nAby umoÅ¼liwiÄ‡ komunikacjÄ™ miÄ™dzy mikroserwisami, czÄ™sto wykorzystuje siÄ™ Application Programming Interfaces (API), ktÃ³re pozwalajÄ… na wymianÄ™ danych i integracjÄ™ rÃ³Å¼nych usÅ‚ug.\n\n\nPrzykÅ‚ad API w mikroserwisach â€“ Python & FastAPI\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy mikroserwis REST API w Pythonie z uÅ¼yciem FastAPI, ktÃ³ry zwraca informacje o uÅ¼ytkownikach:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# PrzykÅ‚adowe dane uÅ¼ytkownikÃ³w\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane uÅ¼ytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nJak to dziaÅ‚a?\n\nUruchamiamy serwer FastAPI.\nMoÅ¼emy uzyskaÄ‡ dane uÅ¼ytkownika, wysyÅ‚ajÄ…c Å¼Ä…danie GET do http://127.0.0.1:8000/users/1.\nAPI zwrÃ³ci dane w formacie JSON, np.:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousÅ‚ug jest wykorzystanie interfejsÃ³w API (Application Programming Interface).\nAPI umoÅ¼liwia komunikacjÄ™ i integracjÄ™ miÄ™dzy rÃ³Å¼nymi mikroserwisami, podobnie jak strona internetowa komunikuje siÄ™ z przeglÄ…darkÄ….\nGdy odwiedzasz stronÄ™ internetowÄ…, serwer wysyÅ‚a kod, ktÃ³ry Twoja przeglÄ…darka interpretuje i wyÅ›wietla jako stronÄ™ internetowÄ….\nPodobnie dziaÅ‚a API â€“ wysyÅ‚a odpowiedzi na zapytania klienta.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "href": "lectures/wyklad4.html#przypadek-biznesowy-model-ml-jako-usÅ‚uga",
    "title": "WykÅ‚ad 4",
    "section": "Przypadek biznesowy: Model ML jako usÅ‚uga",
    "text": "Przypadek biznesowy: Model ML jako usÅ‚uga\nZaÅ‚Ã³Å¼my, Å¼e pracujesz w firmie zajmujÄ…cej siÄ™ sprzedaÅ¼Ä… nieruchomoÅ›ci w Bostonie.\nChcesz zwiÄ™kszyÄ‡ sprzedaÅ¼ i poprawiÄ‡ jakoÅ›Ä‡ obsÅ‚ugi klientÃ³w poprzez nowÄ… aplikacjÄ™ mobilnÄ…, z ktÃ³rej moÅ¼e korzystaÄ‡ nawet 1 000 000 uÅ¼ytkownikÃ³w jednoczeÅ›nie.\nJednym z rozwiÄ…zaÅ„ jest udostÄ™pnienie prognozy wartoÅ›ci domu w czasie rzeczywistym.\nGdy uÅ¼ytkownik prosi o wycenÄ™ nieruchomoÅ›ci, aplikacja wysyÅ‚a zapytanie do serwera, ktÃ³ry przetwarza je za pomocÄ… modelu Machine Learning (ML) i zwraca oszacowanÄ… wartoÅ›Ä‡.\n\nCzym jest serwowanie modelu ML?\nTrening dobrego modelu ML to dopiero pierwszy krok caÅ‚ego procesu.\nAby model byÅ‚ uÅ¼yteczny, musisz go udostÄ™pniÄ‡ uÅ¼ytkownikom koÅ„cowym, np. w formie API.\n\n\nJak to zrobiÄ‡?\n\nPotrzebujesz:\n\n\nwytrenowanego modelu ML,\ninterpreter modelu (np. TensorFlow, Scikit-Learn, PyTorch),\ndanych wejÅ›ciowych dla modelu.\n\n\nKluczowe metryki jakoÅ›ci serwowania modelu:\nCzas odpowiedzi (latency),\nKoszt uruchomienia modelu (infrastruktura serwerowa),\nLiczba zapytaÅ„ na sekundÄ™ (QPS â€“ Queries Per Second).\n\n\nUdostÄ™pnianie danych miÄ™dzy systemami zawsze byÅ‚o kluczowym wyzwaniem w tworzeniu oprogramowania.\nW obszarze tradycyjnego DevOps koncentrujemy siÄ™ na infrastrukturze, a w MLOps â€“ na wdraÅ¼aniu i utrzymaniu modeli uczenia maszynowego.\n\nZbudowanie systemu przygotowanego do Å›rodowiska produkcyjnego jest bardziej skomplikowane niÅ¼ wytrenowanie samego modelu:\n\nczyszczenie i zaÅ‚adowanie odpowiednich i zwalidowanych danych\nObliczenie zmiennych i ich serwowanie na wÅ‚aÅ›ciwym Å›rodowisku\nSerwowanie modelu w sposÃ³b najbardziej efektywny ze wzglÄ™du na koszty\nWersjonowanie, Å›ledzenie i udostÄ™pnianie danych i modeli oraz innych artefaktÃ³w\nMonitorowanie infrastruktury i modelu\nWdroÅ¼enie modelu na skalowalnej infrastrukturze\nAutomatyzacja procesu wdraÅ¼ania i treningu",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#jak-dziaÅ‚a-api",
    "href": "lectures/wyklad4.html#jak-dziaÅ‚a-api",
    "title": "WykÅ‚ad 4",
    "section": "Jak dziaÅ‚a API?",
    "text": "Jak dziaÅ‚a API?\nKiedy wywoÅ‚asz interfejs API, serwer otrzymuje Twoje Å¼Ä…danie, przetwarza je i generuje odpowiedÅº.\nJeÅ›li wszystko dziaÅ‚a poprawnie, otrzymasz wynik w formacie JSON lub XML.\nJeÅ›li wystÄ…pi bÅ‚Ä…d, API zwrÃ³ci kod bÅ‚Ä™du HTTP (np. 400 â€“ nieprawidÅ‚owe Å¼Ä…danie, 500 â€“ bÅ‚Ä…d serwera).\n\nKluczowe zasady REST API:\n\nKlient-Serwer â†’ Klient (np. aplikacja mobilna) wysyÅ‚a Å¼Ä…danie HTTP do API hostowanego na serwerze, ktÃ³ry zwraca odpowiedÅº.\nDziaÅ‚a to identycznie jak przeglÄ…darka internetowa, ktÃ³ra wysyÅ‚a Å¼Ä…danie do serwera WWW i otrzymuje stronÄ™ HTML.\nBezstanowoÅ›Ä‡ â†’ KaÅ¼de Å¼Ä…danie klienta musi zawieraÄ‡ wszystkie niezbÄ™dne informacje do przetworzenia odpowiedzi,\nAPI nie powinno przechowywaÄ‡ informacji o wczeÅ›niejszych Å¼Ä…daniach uÅ¼ytkownika.\n\n\n\nPrzykÅ‚ad: API serwujÄ…ce model ML\nPoniÅ¼ej znajduje siÄ™ przykÅ‚adowy serwis API, ktÃ³ry udostÄ™pnia model ML do prognozowania ceny nieruchomoÅ›ci,\nz wykorzystaniem FastAPI oraz Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wczeÅ›niej wytrenowany model ML (np. regresjÄ™ liniowÄ…)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Prognoza ceny nieruchomoÅ›ci na podstawie cech:\n    - area (powierzchnia w mÂ²),\n    - bedrooms (liczba sypialni),\n    - age (wiek budynku w latach).\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n\n\ndomenÄ™,\nport,\ndodatkowe Å›cieÅ¼ki,\nzapytanie\n\n\nMetody HTTP:\n\n\nGET,\nPOST\n\n\nNagÅ‚Ã³wki HTTP zawierajÄ…:\n\n\ninformacje o autoryzacji,\ncookies metadata\n\nCaÅ‚a informacja zawarta jest w Content-Type: application/json, text â€¦ Accept: application/json, Authorization: Basic abase64string, Tokens 4. CiaÅ‚o zapytania\nNajczÄ™Å›ciej wybieranym formatem dla wymiany informacji miÄ™dzy serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt sÅ‚ownika - â€œkluczâ€: â€œwartoÅ›Ä‡â€.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedÅº - Response\n\nTreÅ›Ä‡ odpowiedzi przekazywana jest razem z nagÅ‚Ã³wkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: â€œContent-Typeâ€ =&gt; â€application/json; charset=utf-8â€, â€Serverâ€ =&gt; â€Genie/Julia/1.8.5â€\nTreÅ›Ä‡ (ciaÅ‚o) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidÅ‚owe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#publikujsubskrybuj",
    "href": "lectures/wyklad4.html#publikujsubskrybuj",
    "title": "WykÅ‚ad 4",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyÅ‚ania wiadomoÅ›ci â€Publikuj/Subskrybujâ€ ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujÄ…cy siÄ™ tym, Å¼e nadawca (publikujÄ…cy) fragmentu danych (wiadomoÅ›ci) nie kieruje go wprost do odbiorcy. pub/sub to systemy, ktÃ³re czÄ™sto posiadajÄ… brokera czyli centralny punkt, w ktÃ³rym znajdujÄ… siÄ™ wiadomoÅ›ci.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#apache-kafka",
    "href": "lectures/wyklad4.html#apache-kafka",
    "title": "WykÅ‚ad 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicjÄ™:\n\nRozproszona platforma streamingowa\nCo to jest â€platforma rozproszonego przesyÅ‚ania strumieniowegoâ€?\nNajpierw chcÄ™ przypomnieÄ‡, czym jest â€strumieÅ„â€. Strumienie to po prostu nieograniczone dane, dane, ktÃ³re nigdy siÄ™ nie koÅ„czÄ…. CiÄ…gle ich przybywa i moÅ¼esz przetwarzaÄ‡ je w czasie rzeczywistym.\nA â€rozproszoneâ€? Rozproszony oznacza, Å¼e â€‹â€‹Kafka dziaÅ‚a w klastrze, a kaÅ¼dy wÄ™zeÅ‚ w grupie nazywa siÄ™ Brokerem. Ci brokerzy to po prostu serwery wykonujÄ…ce kopiÄ™ Apache Kafka.\nTak wiÄ™c Kafka to zestaw wspÃ³Å‚pracujÄ…cych ze sobÄ… maszyn, aby mÃ³c obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ nieograniczone dane w czasie rzeczywistym.\nBrokerzy sprawiajÄ…, Å¼e jest niezawodny, skalowalny i odporny na bÅ‚Ä™dy. Ale dlaczego panuje bÅ‚Ä™dne przekonanie, Å¼e Kafka to kolejny â€kolejkowy system przesyÅ‚ania wiadomoÅ›ciâ€?\nAby odpowiedzieÄ‡ na tÄ™ odpowiedÅº, musimy najpierw wyjaÅ›niÄ‡, jak dziaÅ‚a kolejkowe przesyÅ‚anie wiadomoÅ›ci.\n\n\nKolejkowy system przesyÅ‚ania wiadomoÅ›ci\nPrzesyÅ‚anie wiadomoÅ›ci, to po prostu czynnoÅ›Ä‡ wysyÅ‚ania wiadomoÅ›ci z jednego miejsca do drugiego. Ma trzech gÅ‚Ã³wnych â€œaktorÃ³wâ€:\n\nProducent: KtÃ³ry tworzy i wysyÅ‚a komunikaty do jednej lub wiÄ™cej kolejek;\nKolejka: struktura danych bufora, ktÃ³ra odbiera (od producentÃ³w) i dostarcza komunikaty (do konsumentÃ³w) w sposÃ³b FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jednÄ… lub wiÄ™cej kolejek i otrzymuje ich wiadomoÅ›ci po opublikowaniu.\n\nI to jest to; tak dziaÅ‚a przesyÅ‚anie wiadomoÅ›ci. Jak widaÄ‡, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Apache Kafka\nWiÄ™cej informacji na temat Kafki znajdziesz w tym linku.\nTeraz, gdy rozumiemy podstawy przesyÅ‚ania wiadomoÅ›ci, zagÅ‚Ä™bmy siÄ™ w Å›wiat Apache Kafka.\nW Kafce mamy dwa kluczowe pojÄ™cia: ProducentÃ³w (Producers) i KonsumentÃ³w (Consumers),\nktÃ³rzy dziaÅ‚ajÄ… w podobny sposÃ³b jak w klasycznych systemach kolejkowych, produkujÄ…c i konsumujÄ…c wiadomoÅ›ci.\n\n\nJak widaÄ‡, Kafka przypomina klasyczny system przesyÅ‚ania wiadomoÅ›ci, jednak w odrÃ³Å¼nieniu od tradycyjnych kolejek,\nzamiast pojÄ™cia kolejki (queue) mamy Tematy (Topics).\n\n\nTematy (Topics) i Partycje (Partitions)\nTemat (Topic) to podstawowy kanaÅ‚ przesyÅ‚ania danych w Kafce.\nMoÅ¼na go porÃ³wnaÄ‡ do folderu, w ktÃ³rym przechowywane sÄ… wiadomoÅ›ci.\nKaÅ¼dy temat posiada jednÄ… lub wiÄ™cej partycji (Partitions) â€“ podziaÅ‚ ten wpÅ‚ywa na skalowalnoÅ›Ä‡ i rÃ³wnowaÅ¼enie obciÄ…Å¼enia.\nPodczas tworzenia tematu okreÅ›lamy liczbÄ™ partycji.\n\nKluczowe cechy tematÃ³w i partycji:\n\nTemat to logiczna jednostka, do ktÃ³rej producenci wysyÅ‚ajÄ… wiadomoÅ›ci, a konsumenci je odczytujÄ….\nPartycja to fizyczny podziaÅ‚ tematu. MoÅ¼na jÄ… porÃ³wnaÄ‡ do plikÃ³w w folderze.\nOffset â€“ kaÅ¼da wiadomoÅ›Ä‡ w partycji otrzymuje unikalny identyfikator (offset),\nktÃ³ry pozwala konsumentom Å›ledziÄ‡, ktÃ³re wiadomoÅ›ci zostaÅ‚y juÅ¼ przetworzone.\nKafka przechowuje wiadomoÅ›ci na dysku, dziÄ™ki czemu moÅ¼e je ponownie odczytaÄ‡ (w przeciwieÅ„stwie do klasycznych kolejek, gdzie wiadomoÅ›Ä‡ jest usuwana po przetworzeniu).\nKonsumenci odczytujÄ… wiadomoÅ›ci sekwencyjnie, od najstarszej do najnowszej.\nW przypadku awarii konsument moÅ¼e wznowiÄ‡ przetwarzanie od ostatniego zapisanego offsetu.\n\n\n\n\n\n\nBrokerzy (Brokers) i Klaster Kafka\nKafka dziaÅ‚a w sposÃ³b rozproszony â€“ oznacza to, Å¼e moÅ¼e skÅ‚adaÄ‡ siÄ™ z wielu brokerÃ³w (Brokers),\nktÃ³re wspÃ³Å‚pracujÄ… jako jeden klaster.\n\n\n\nKluczowe informacje o brokerach\n\nBroker to pojedynczy serwer w klastrze Kafki, odpowiedzialny za przechowywanie partycji tematÃ³w.\nKaÅ¼dy broker w klastrze ma unikalny identyfikator.\nAby zwiÄ™kszyÄ‡ dostÄ™pnoÅ›Ä‡ i niezawodnoÅ›Ä‡, Kafka wykorzystuje replikacjÄ™ danych.\nWspÃ³Å‚czynnik replikacji okreÅ›la, ile kopii danej partycji ma byÄ‡ przechowywane na rÃ³Å¼nych brokerach.\nJeÅ›li temat ma trzy partycje i wspÃ³Å‚czynnik replikacji rÃ³wny trzy,\noznacza to, Å¼e kaÅ¼da partycja zostanie powielona na trzech rÃ³Å¼nych brokerach.\n\nLiczba partycji powinna byÄ‡ dobrana w taki sposÃ³b, aby kaÅ¼dy broker miaÅ‚ co najmniej jednÄ… partycjÄ™ do obsÅ‚ugi.\n\n\n\nProducenci (Producers)\nW Kafka producenci to aplikacje lub usÅ‚ugi, ktÃ³re tworzÄ… i wysyÅ‚ajÄ… wiadomoÅ›ci do tematÃ³w.\nDziaÅ‚a to podobnie do systemÃ³w kolejkowych, z tÄ… rÃ³Å¼nicÄ…, Å¼e Kafka zapisuje wiadomoÅ›ci w partycjach.\n\nJak Kafka przypisuje wiadomoÅ›ci do partycji?\n\nWiadomoÅ›ci sÄ… rozsyÅ‚ane okrÄ™Å¼nie (round-robin) do dostÄ™pnych partycji.\nMoÅ¼emy okreÅ›liÄ‡ klucz wiadomoÅ›ci, a Kafka wyliczy jego hash,\naby okreÅ›liÄ‡, do ktÃ³rej partycji trafi wiadomoÅ›Ä‡.\nKlucz wiadomoÅ›ci determinuje przypisanie do partycji â€“ jeÅ›li temat zostaÅ‚ juÅ¼ utworzony,\nliczba partycji nie moÅ¼e byÄ‡ zmieniona bez zakÅ‚Ã³cenia tego mechanizmu.\n\nPrzykÅ‚ad przypisania wiadomoÅ›ci do partycji:\n\nWiadomoÅ›Ä‡ 01 trafia do partycji 0 tematu Topic_1.\nWiadomoÅ›Ä‡ 02 trafia do partycji 1 tego samego tematu.\nKolejna wiadomoÅ›Ä‡ moÅ¼e ponownie trafiÄ‡ do partycji 0, jeÅ›li stosujemy przypisanie round-robin.\n\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# WysyÅ‚amy wiadomoÅ›Ä‡ do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"Nowe mieszkanie na sprzedaÅ¼\"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"WiadomoÅ›Ä‡ wysÅ‚ana do tematu '{topic}'\")\n\n\n\nKonsumenci (Consumers)\nKonsumenci w Kafce odczytujÄ… i przetwarzajÄ… wiadomoÅ›ci z tematÃ³w. KaÅ¼dy konsument moÅ¼e naleÅ¼eÄ‡ do grupy konsumentÃ³w (Consumer Group), co pozwala na rÃ³wnolegÅ‚e przetwarzanie wiadomoÅ›ci. - JeÅ›li wielu konsumentÃ³w naleÅ¼y do tej samej grupy, Kafka rÃ³wnowaÅ¼y obciÄ…Å¼enie miÄ™dzy nimi. - JeÅ›li jeden konsument przestanie dziaÅ‚aÄ‡, Kafka automatycznie przypisze jego partycje do innego aktywnego konsumenta.\nPrzykÅ‚ad konsumenta w Pythonie:\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, ktÃ³ry nasÅ‚uchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Oczekiwanie na wiadomoÅ›ci...\")\n\nfor message in consumer:\n    print(f\"Otrzymano wiadomoÅ›Ä‡: {message.value.decode()}\")\nInnym waÅ¼nym pojÄ™ciem Kafki sÄ… â€Grupy konsumentÃ³wâ€. Jest to bardzo waÅ¼ne, gdy musimy skalowaÄ‡ odczytywanie wiadomoÅ›ci. Staje siÄ™ to bardzo kosztowne, gdy pojedynczy konsument musi czytaÄ‡ z wielu partycji, wiÄ™c musimy zrÃ³wnowaÅ¼yÄ‡ obciÄ…Å¼enie miÄ™dzy naszymi konsumentami, wtedy wchodzÄ… grupy konsumentÃ³w.\nDane z jednego tematu bÄ™dÄ… rÃ³wnowaÅ¼one obciÄ…Å¼eniem miÄ™dzy konsumentami, dziÄ™ki czemu moÅ¼emy zagwarantowaÄ‡, Å¼e nasi konsumenci bÄ™dÄ… w stanie obsÅ‚ugiwaÄ‡ i przetwarzaÄ‡ dane. IdeaÅ‚em jest posiadanie takiej samej liczby konsumentÃ³w w grupie, jakÄ… mamy jako partycje w temacie, w ten sposÃ³b kaÅ¼dy konsument czyta tylko z jednego. Podczas dodawania konsumentÃ³w do grupy naleÅ¼y uwaÅ¼aÄ‡, jeÅ›li liczba konsumentÃ³w jest wiÄ™ksza niÅ¼ liczba partycji, niektÃ³rzy konsumenci nie bÄ™dÄ… czytaÄ‡ z Å¼adnego tematu i pozostanÄ… bezczynni.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 4"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html",
    "href": "lectures/wyklad3.html",
    "title": "WykÅ‚ad 3",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, podstawowych sposobÃ³w przetwarzania i analizowania danych strumieniowych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#definicje",
    "href": "lectures/wyklad3.html#definicje",
    "title": "WykÅ‚ad 3",
    "section": "Definicje",
    "text": "Definicje\n\nZapoznaj siÄ™ z tematem danych strumieniowych\n\nDefinicja 1 â€“ Zdarzenie to wszystko, co moÅ¼na zaobserwowaÄ‡ w danym momencie czasu. Jest generowane jako bezpoÅ›redni skutek dziaÅ‚ania.\nDefinicja 2 â€“ W kontekÅ›cie danych zdarzenie to niezmienialny rekord w strumieniu danych, zakodowany jako JSON, XML, CSV lub w formacie binarnym.\nDefinicja 3 â€“ CiÄ…gÅ‚y strumieÅ„ zdarzeÅ„ to nieskoÅ„czony zbiÃ³r pojedynczych zdarzeÅ„ uporzÄ…dkowanych w czasie, np. logi z urzÄ…dzeÅ„.\nDefinicja 4 â€“ StrumieÅ„ danych to dane tworzone przyrostowo w czasie, generowane ze ÅºrÃ³deÅ‚ statycznych (baza danych, odczyt linii z pliku) lub dynamicznych (logi, sensory, funkcje).\nPrzedsiÄ™biorstwo to organizacja, ktÃ³ra generuje i odpowiada na ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„.\n\n\n\n\nAnalityka strumieniowa\nAnalityka strumieniowa (ang. stream analytics) nazywana jest rÃ³wnieÅ¼ przetwarzaniem strumieniowym zdarzeÅ„ (ang. event stream processing) â€“ czyli przetwarzaniem duÅ¼ych iloÅ›ci danych juÅ¼ na etapie ich generowania.\nNiezaleÅ¼nie od zastosowanej technologii, wszystkie dane powstajÄ… jako ciÄ…gÅ‚y strumieÅ„ zdarzeÅ„ â€“ obejmuje to m.in.:\n\ndziaÅ‚ania uÅ¼ytkownikÃ³w na stronach internetowych,\n\nlogi systemowe,\n\npomiary z sensorÃ³w.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad3.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 3",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego analizujemy dane historyczne, a czas uruchomienia procesu nie ma Å¼adnego zwiÄ…zku z momentem wystÄ…pienia analizowanych zdarzeÅ„.\nNatomiast w przetwarzaniu strumieniowym wyrÃ³Å¼niamy dwie koncepcje czasu:\n\nCzas zdarzenia (event time) â€“ moment, w ktÃ³rym zdarzenie faktycznie miaÅ‚o miejsce.\nCzas przetwarzania (processing time) â€“ moment, w ktÃ³rym system przetwarza zdarzenie.\n\nIdealne przetwarzanie danych\nW idealnej sytuacji przetwarzanie nastÄ™puje natychmiast po wystÄ…pieniu zdarzenia:\n\nRzeczywiste przetwarzanie danych\nW praktyce przetwarzanie danych zawsze odbywa siÄ™ z pewnym opÃ³Åºnieniem, co jest widoczne jako punkty poniÅ¼ej linii idealnego przetwarzania (poniÅ¼ej przekÄ…tnej na wykresie).\n\nW aplikacjach przetwarzania strumieniowego istotna jest rÃ³Å¼nica miÄ™dzy czasem powstania zdarzenia a czasem jego przetwarzania. Do najczÄ™stszych przyczyn opÃ³ÅºnieÅ„ naleÅ¼Ä…:\n\nprzesyÅ‚anie danych przez sieÄ‡,\nbrak komunikacji miÄ™dzy urzÄ…dzeniem a sieciÄ….\n\nPrzykÅ‚adem jest Å›ledzenie poÅ‚oÅ¼enia samochodu przez aplikacjÄ™ GPS â€“ przejazd przez tunel moÅ¼e spowodowaÄ‡ chwilowÄ… utratÄ™ danych.\nObsÅ‚uga opÃ³ÅºnieÅ„ w przetwarzaniu strumieniowym\nOpÃ³Åºnienia w przetwarzaniu zdarzeÅ„ moÅ¼na obsÅ‚uÅ¼yÄ‡ na dwa sposoby:\n\nMonitorowanie liczby pominiÄ™tych zdarzeÅ„ i wyzwalanie alarmu w przypadku zbyt duÅ¼ej liczby odrzuceÅ„.\nZastosowanie korekty za pomocÄ… watermarkingu, czyli dodatkowego mechanizmu uwzglÄ™dniajÄ…cego opÃ³Åºnione zdarzenia.\n\nProces przetwarzania zdarzeÅ„ w czasie rzeczywistym moÅ¼na przedstawiÄ‡ jako funkcjÄ™ schodkowÄ…:\n\nNie wszystkie zdarzenia wnoszÄ… wkÅ‚ad do analizy â€“ niektÃ³re mogÄ… zostaÄ‡ odrzucone ze wzglÄ™du na zbyt duÅ¼e opÃ³Åºnienie.\nWykorzystanie watermarkingu pozwala na uwzglÄ™dnienie dodatkowego czasu na pojawienie siÄ™ opÃ³Åºnionych zdarzeÅ„. Proces ten obejmuje wszystkie zdarzenia powyÅ¼ej przerywanej linii. Mimo to nadal mogÄ… zdarzyÄ‡ siÄ™ przypadki, w ktÃ³rych niektÃ³re punkty zostanÄ… pominiÄ™te.\n\nPrzedstawione na wykresach sytuacje jawnie wskazujÄ… dlaczego pojÄ™cie czasu jest istotnym czynnikiem i wymaga Å›cisÅ‚ego okreÅ›lenia juÅ¼ na poziomie definiowania potrzeb biznesowych. Przypisywanie znacznikÃ³w czasu do danych (zdarzeÅ„) to trudne zadanie.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "href": "lectures/wyklad3.html#okna-czasowe-w-analizie-strumieniowej",
    "title": "WykÅ‚ad 3",
    "section": "Okna czasowe w analizie strumieniowej",
    "text": "Okna czasowe w analizie strumieniowej\nW przetwarzaniu strumieniowym okna czasowe pozwalajÄ… na grupowanie danych w ograniczone czasowo segmenty, co umoÅ¼liwia analizÄ™ zdarzeÅ„ w okreÅ›lonych przedziaÅ‚ach czasowych. W zaleÅ¼noÅ›ci od zastosowania stosuje siÄ™ rÃ³Å¼ne typy okien, dostosowane do charakterystyki danych i wymagaÅ„ analitycznych.\n\n\nOkno rozÅ‚Ä…czne (Tumbling Window)\nJest to okno o staÅ‚ej dÅ‚ugoÅ›ci, ktÃ³re nie nakÅ‚ada siÄ™ na siebie â€“ kaÅ¼de zdarzenie naleÅ¼y tylko do jednego okna.\nâœ… Charakterystyka:\n\nStaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n\nBrak nakÅ‚adania siÄ™ na siebie\n\nIdealne do podziaÅ‚u danych na rÃ³wne segmenty czasowe\n\nğŸ“Œ PrzykÅ‚ad: Analiza liczby zamÃ³wieÅ„ w sklepie internetowym co 5 minut.\n\n\n\n\nOkno przesuwne (Sliding Window)\nObejmuje wszystkie zdarzenia nastÄ™pujÄ…ce w okreÅ›lonym przedziale czasu, gdzie okno przesuwa siÄ™ w sposÃ³b ciÄ…gÅ‚y.\nâœ… Charakterystyka:\n\nKaÅ¼de zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien\n\nOkno przesuwa siÄ™ o zadany interwaÅ‚\n\nPrzydatne do wykrywania trendÃ³w i anomalii\n\nğŸ“Œ PrzykÅ‚ad: Åšledzenie Å›redniej temperatury w ciÄ…gu ostatnich 10 minut, aktualizowane co 2 minuty.\n\n\n\n\nOkno skokowe (Hopping Window)\nJest podobne do okna rozÅ‚Ä…cznego, ale pozwala na nakÅ‚adanie siÄ™ okien na siebie, dziÄ™ki czemu jedno zdarzenie moÅ¼e naleÅ¼eÄ‡ do kilku okien. Jest stosowane do wygÅ‚adzania danych.\nâœ… Charakterystyka:\n\nStaÅ‚a dÅ‚ugoÅ›Ä‡ okna\n\nMoÅ¼liwoÅ›Ä‡ nakÅ‚adania siÄ™ na siebie\n\nPrzydatne do redukcji szumÃ³w w danych\n\nğŸ“Œ PrzykÅ‚ad: Analiza liczby odwiedzajÄ…cych stronÄ™ co 10 minut, ale aktualizowana co 5 minut, aby lepiej wychwytywaÄ‡ trendy.\n\n\n\n\nOkno sesyjne (Session Window)\nOkno sesyjne grupuje zdarzenia na podstawie okresÃ³w aktywnoÅ›ci i zamyka siÄ™ po okreÅ›lonym czasie braku aktywnoÅ›ci.\nâœ… Charakterystyka:\n\nDynamiczna dÅ‚ugoÅ›Ä‡ okna\n\nDefiniowane przez aktywnoÅ›Ä‡ uÅ¼ytkownika\n\nStosowane w analizie sesji uÅ¼ytkownikÃ³w\n\nğŸ“Œ PrzykÅ‚ad: Analiza sesji uÅ¼ytkownikÃ³w na stronie internetowej â€“ sesja trwa tak dÅ‚ugo, jak dÅ‚ugo uÅ¼ytkownik wykonuje akcje, ale koÅ„czy siÄ™ po 15 minutach braku aktywnoÅ›ci.\n\n\n\nPodsumowanie\nRÃ³Å¼ne rodzaje okien czasowych sÄ… stosowane w zaleÅ¼noÅ›ci od specyfiki danych i celÃ³w analizy. WybÃ³r odpowiedniego okna wpÅ‚ywa na dokÅ‚adnoÅ›Ä‡ wynikÃ³w i efektywnoÅ›Ä‡ systemu analitycznego.\n\n\n\n\n\n\n\n\nTyp okna\nCharakterystyka\nZastosowanie\n\n\n\n\nRozÅ‚Ä…czne (Tumbling)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, brak nakÅ‚adania\nRaporty okresowe\n\n\nPrzesuwne (Sliding)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, nakÅ‚adajÄ…ce siÄ™ okna\nTrendy, wykrywanie anomalii\n\n\nSkokowe (Hopping)\nStaÅ‚a dÅ‚ugoÅ›Ä‡, czÄ™Å›ciowe nakÅ‚adanie\nWygÅ‚adzanie danych\n\n\nSesyjne (Session)\nDynamiczna dÅ‚ugoÅ›Ä‡, zaleÅ¼na od aktywnoÅ›ci\nAnaliza sesji uÅ¼ytkownikÃ³w\n\n\n\nKaÅ¼dy typ okna ma swoje unikalne zastosowania i pomaga w lepszej interpretacji danych strumieniowych. WybÃ³r wÅ‚aÅ›ciwej metody zaleÅ¼y od potrzeb biznesowych i charakterystyki analizowanych danych.\nW analizie danych strumieniowych interpretacja czasu jest zÅ‚oÅ¼onym zagadnieniem, poniewaÅ¼:\n\nRÃ³Å¼ne systemy majÄ… rÃ³Å¼ne zegary, co moÅ¼e prowadziÄ‡ do niespÃ³jnoÅ›ci,\nDane mogÄ… docieraÄ‡ z opÃ³Åºnieniem, co wymaga technik watermarkingu i okien czasowych,\nRÃ³Å¼ne podejÅ›cia do analizy czasu zdarzenia i czasu przetwarzania wpÅ‚ywajÄ… na dokÅ‚adnoÅ›Ä‡ wynikÃ³w.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 3"
    ]
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr zimowy 2024/2025, SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nSzczegÃ³Å‚owy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykÅ‚adÃ³w i Ä‡wiczeÅ„ oraz proponowanÄ… literaturÄ™.\nInne ksiÄ…Å¼ki zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli III bud G\n\n22-02-2025 (sobota) 08:00-09:30 - WykÅ‚ad 1\n08-03-2025 (sobota) 08:00-09:30 - WykÅ‚ad 2\n\n\n\nLaboratoria\n\nLab1\n22-03-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n23-03-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab2\n05-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n06-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab3\n26-04-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n27-04-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab4\n17-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n18-05-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\nLab5\n31-05-2025 (sobota) 08:00-13:20 - G-210 grupy 11, 17, 18\n01-06-2025 (niedziela) 13:30-17:00 - G-116 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡) 20 pytaÅ„.\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.X Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3.11 -m venv &lt;name of env&gt;\n# dla linux i mac os\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n# dla windows \n# &lt;name of env&gt;\\Scripts\\activate\n(venv)$ \nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "href": "info.html#zacznij-korzystaÄ‡-z-serwisu-github",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z serwisu GitHub",
    "text": "Zacznij korzystaÄ‡ z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystaÄ‡ z serwisu github\nPracujÄ…c nad projektem np. praca magisterska, (samodzielnie lub w zespole) czÄ™sto potrzebujesz sprawdziÄ‡ jakie zmiany, kiedy i przez kogo zostaÅ‚y wprowadzone do projektu. W zadaniu tym Å›wietnie sprawdza siÄ™ system kontroli wersji czyli GIT.\nGit moÅ¼esz pobraÄ‡ i zainstalowaÄ‡ jak zwykÅ‚y program na dowolnym komputerze. Jednak najczÄ™Å›ciej (maÅ‚e projekty) korzysta siÄ™ z serwisÃ³w z jakimÅ› systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dziÄ™ki ktÃ³remu moÅ¼esz korzystaÄ‡ z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki moÅ¼esz przechowywaÄ‡ w publicznych (dostÄ™p majÄ… wszyscy) repozytoriach.\nSkupimy siÄ™ wyÅ‚Ä…cznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyÅ¼szym poziomie znajdujÄ… siÄ™ konta indywidualne np http://github.com/sebkaz, bÄ…dÅº zakÅ‚adane przez organizacje. UÅ¼ytkownicy indywidualni mogÄ… tworzyÄ‡ repozytoria publiczne (public ) bÄ…dÅº prywatne (private).\nJeden plik nie powinien przekraczaÄ‡ 100 MB.\nRepo (skrÃ³t do repozytorium) tworzymy za pomocÄ… Create a new repository. KaÅ¼de repo powinno mieÄ‡ swojÄ… indywidualnÄ… nazwÄ™.\n\n\nBranche\nGÅ‚Ã³wna (tworzona domyÅ›lnie) gaÅ‚Ä…Åº rapozytorium ma nazwÄ™ master.\n\n\nNajwaÅ¼niejsze polecnia do zapamiÄ™tania\n\nÅ›ciÄ…ganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba moÅ¼esz pobraÄ‡ repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejÅ›cie do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawiÄ‡ siÄ™ ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPoÅ‚Ä…cz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsÅ‚uga w 3 krokach\n\n# sprawdÅº zmiany jakie zostaÅ‚y dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzÄ…cy stan wraz z informacjÄ… co zrobiÅ‚eÅ›\ngit commit -m \" opis \"\n# 3. potem juÅ¼ zostaje tylko\ngit push origin master\nWarto obejrzeÄ‡ Youtube course.\nCiekawe i proste wprowadzenie mozna znaleÅºÄ‡ tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "href": "info.html#zacznij-korzystaÄ‡-z-dockera",
    "title": "NarzÄ™dzia",
    "section": "Zacznij korzystaÄ‡ z Dockera",
    "text": "Zacznij korzystaÄ‡ z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swÃ³j system przejdÅº do strony.\nJeÅ¼li wszystko zainstalowaÅ‚o siÄ™ prawidÅ‚owo wykonaj nastÄ™pujÄ…ce polecenia:\n\nSprawdÅº zainstalowanÄ… wersjÄ™\n\ndocker --version\n\nÅšciÄ…gnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzeglÄ…d Å›ciÄ…gnietych obrazÃ³w:\n\ndocker image ls\n\ndocker images\n\nPrzeglÄ…d uruchomionych kontenerÃ³w:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsuniÄ™cie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam rÃ³wnieÅ¼ krÃ³tkie intro"
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#ksiÄ…Å¼ki",
    "href": "ksiazki.html#ksiÄ…Å¼ki",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\nA. Bellemare MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™ Zobacz opis lub Kup\n\n\n\n\n\n\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocÄ… pakietÃ³w Pandas i NumPy oraz Å›rodowiska IPython. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieÄ‡. Algorytmy przyszÅ‚oÅ›ci. Wydanie II (ebook) Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nA. Geron Uczenie maszynowe z uÅ¼yciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nR. Schutt, C. Oâ€™Neil Badanie danych. Raport z pierwszej lini dziaÅ‚aÅ„. Zobacz opis lub Kup ksiÄ…Å¼kÄ™.\nT. Segaran Nowe usÅ‚ugi 2.0. Przewodnik po analizie zbiorÃ³w danych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jÄ™zykiem Python i bibliotekÄ… Keras. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie gÅ‚Ä™bokie z jÄ™zykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Weidman Uczenie gÅ‚Ä™bokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyÄ‡ komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistÃ³w. Budowanie aplikacji AI za pomocÄ… fastai i PyTorch Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. BÅ‚yskawiczna analiza danych Zobacz opis lub Kup\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind ZrozumieÄ‡ programowanie Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nA. Allain C++. Przewodnik dla poczÄ…tkujÄ…cych Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdraÅ¼anie aplikacji Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadaÅ„ z pythonem. Zobacz opis lub Kup ksiÄ…Å¼kÄ™, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzÄ™dzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatnikÃ³w\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI donâ€™t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "",
    "text": "Na poprzednich zajÄ™ciach omawialiÅ›my wykorzystanie modelu regresji liniowej dla danych ustrukturyzowanych. W najprostszym przypadku dla jednej zmiennej \\(X\\) i jednej zmiennej celu moglibyÅ›my np. przypisaÄ‡ model w postaci:\nsatysfakcja_z_zycia = \\(\\alpha_0\\) + \\(\\alpha_1\\) PKB_per_capita\n\\(\\alpha_0\\) nazywamy punktem odciÄ™cia (intercept) albo punktem obciÄ…Å¼enia (bias)\nimport numpy as np\n\nnp.random.seed(42) \nm = 100\nX = 2*np.random.rand(m,1) \na_0, a_1 = 4, 3 \ny = a_0 + a_1 * X + np.random.randn(m,1)\nimport matplotlib.pyplot as plt\n\nplt.scatter(X, y)\nplt.show()\nW ogÃ³lnoÅ›ci model liniowy: \\(\\hat{y} = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_n x_n\\) gdzie \\(\\hat{y}\\) to predykcja naszego modelu (wartoÅ›Ä‡ prognozowana), dla \\(n\\) cech przy wartoÅ›ciach cechy \\(x_i\\).\nW postaci zwektoryzowanej moÅ¼emy napisaÄ‡: \\(\\hat{y} = \\vec{\\alpha}^{T} \\vec{x}\\)\nW tej postaci widaÄ‡ dlaczego w tym modelu dokÅ‚ada siÄ™ kolumnÄ™ jedynek - wynikajÄ… one z wartoÅ›ci \\(x_0\\) dla \\(\\alpha_0\\).\n# dodajmy jedynkÄ™ do naszej tabeli \nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)\nPowiedzieliÅ›my, Å¼e moÅ¼emy w tym modelu znaleÅºÄ‡ funkcjÄ™ kosztu\n\\(MSE(\\vec{x}, \\hat{y}) = \\sum_{i=1}^{m} \\left( \\vec{\\alpha}^{T} \\vec{x}^{(i)} - y^{(i)} \\right)^{2}\\)\nTak naprawdÄ™ moÅ¼emy \\(MSE(\\vec{x}, \\hat{y}) = MSE(\\vec{\\alpha})\\)\nRozwiÄ…zanie analityczne: \\(\\vec{\\alpha} = (X^{T}X)^{-1} X^T y\\)\n# rozwiÄ…zanie analityczne \nalpha_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\nalpha_best, np.array([4,3])\n\n(array([[4.21509616],\n        [2.77011339]]),\n array([4, 3]))\nX_new = np.array([[0],[2]])\nX_new_b = add_dummy_feature(X_new)\ny_predict = X_new_b @ alpha_best\n\nimport matplotlib.pyplot as plt\n\nplt.plot(X_new, y_predict, \"r-\", label=\"prediction\")\nplt.plot(X,y, \"b.\")\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X,y) \n\nprint(f\"a_0={lin_reg.intercept_[0]}, a_1 = {lin_reg.coef_[0][0]}\")\n\nprint(\"predykcja\", lin_reg.predict(X_new))\n\na_0=4.215096157546746, a_1 = 2.770113386438484\npredykcja [[4.21509616]\n [9.75532293]]\n# Logistic Regression w scikit learn oparta jest o metodÄ™ lstsq \nalpha_best_svd, _, _, _ = np.linalg.lstsq(X_b, y, rcond=1e-6)\nalpha_best_svd\n\narray([[4.21509616],\n       [2.77011339]])",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Optymalizacja modeli w Pythonie"
    ]
  },
  {
    "objectID": "labs/lab5.html#gradient-prosty",
    "href": "labs/lab5.html#gradient-prosty",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Gradient prosty",
    "text": "Gradient prosty\nPamiÄ™taj o standaryzacji zmiennych (aby byÅ‚y one reprezentowane w tej samej skali).\n\nWsadowy gradient prosty\nW celu implementacji musimy policzyÄ‡ pochodne czÄ…stkowe dla funkcji kosztu wobec kaÅ¼dego parametru \\(\\alpha_i\\).\n\\(\\frac{\\partial}{\\partial \\alpha_j}MSE(\\vec{x}, \\hat{y}) = 2 \\sum_{i=1}^{m} \\left( \\vec{\\alpha}^{T} \\vec{x}^{(i)} - y^{(i)} \\right) x_j^{(i)}\\)\nKomputery posiadajÄ… wÅ‚asnoÅ›Ä‡ mnoÅ¼enia macierzy co pozwala obliczyÄ‡ nam wszystkie pochodne w jednym obliczeniu. WzÃ³r i algorytm liczÄ…cy wszystkie pochodne â€œna razâ€ wykorzystuje caÅ‚y zbiÃ³r X dlatego teÅ¼ nazywamy go wsadowym.\nPo obliczeniu gradientu po prostu idziemy â€œw przeciwnÄ… stronÄ™â€\n$ {next} = - {} MSE()$\n\nfrom IPython.display import Image\nImage(filename='../img/02_10.png', width=500) \n\n\n\n\n\n\n\n\n\neta = 0.1\nn_epochs = 1000\nm = len(X_b)\nnp.random.seed(42) \nalpha = np.random.randn(2,1) # losowo wybieramy rozwiÄ…zanie\n\nfor epoch in range(n_epochs):\n    gradients = 2/m* X_b.T @ (X_b @ alpha - y)\n    #print(alpha)\n    alpha = alpha - eta*gradients\n\n\nalpha\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nsprawdz jak wygladajÄ… wyniki dla rÃ³Å¼nych eta dla 0.02, 0.1, 0.5",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Optymalizacja modeli w Pythonie"
    ]
  },
  {
    "objectID": "labs/lab5.html#stochastic-gradient-descent",
    "href": "labs/lab5.html#stochastic-gradient-descent",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nJednym z powaÅ¼niejszych problemÃ³w wsadowego gradientu jest jego zaleÅ¼noÅ›Ä‡ od wykorzystania (w kaÅ¼dym kroku) caÅ‚ej macierzy danych. KorzystajÄ…c z wÅ‚asnoÅ›ci statystycznych moÅ¼emy zobaczyÄ‡ jak bÄ™dzie realizowaÅ‚a siÄ™ zbieÅ¼noÅ›Ä‡ rozwiÄ…zania jeÅ›li za kaÅ¼dym razem wylosujemy prÃ³bkÄ™ danych i na niej okreÅ›limy gradient. Ze wzglÄ™du, iÅ¼ w pamiÄ™ci przechowujemy tylko pewnÄ… porcjÄ™ danych algorytm ten moÅ¼e byÄ‡ uÅ¼ywany dla bardzo duÅ¼ych zbiorÃ³w danych. Warto jednak mieÄ‡ Å›wiadomoÅ›Ä‡, Å¼e tak otrzymane wyniki majÄ… charakter chaotyczny, co oznacza, Å¼e funkcja kosztu nie zbiega siÄ™ w kierunku minimum lecz przeskakuje dÄ…Å¼Ä…c do minimun w sensie Å›redniej.\n\nn_epochs = 50\nm = len(X_b)\n\n\ndef learning_schedule(t, t0=5, t1=50):\n    return t0/(t+t1)\n\nnp.random.seed(42)\nalpha = np.random.randn(2,1)\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1] \n        gradients = 2 * xi.T @ (xi @ alpha - yi)\n        eta = learning_schedule(epoch * m + iteration) \n        alpha = alpha - eta * gradients\n        \n\n\nalpha\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, \n                       penalty=None, eta0=0.01, \n                       n_iter_no_change=100, random_state=42)\n\nsgd_reg.fit(X, y.ravel())\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressor?Documentation for SGDRegressoriFittedSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05) \n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Optymalizacja modeli w Pythonie"
    ]
  },
  {
    "objectID": "labs/lab5.html#perceptron-i-oop",
    "href": "labs/lab5.html#perceptron-i-oop",
    "title": "Optymalizacja modeli w Pythonie",
    "section": "Perceptron i OOP",
    "text": "Perceptron i OOP\n\nfrom random import randint\nrandint(1,6)\n\n3\n\n\n\nfrom random import randint\n\nclass Kosc():\n    \"\"\"opis\"\"\"\n    def __init__(self, sciany=6):\n        \"\"\"\n        To jest metoda uruchamiana podczas inicjalizacji obiektu\n\n        params:\n        sciany (int) \n        \"\"\"\n        # zdefiniuj zmiennÄ… sciany i przypisz do niej domyÅ›lnÄ… wartoÅ›Ä‡ 6\n        self.sciany = sciany\n        \n        \n    def roll(self):\n        \"\"\"opis metody\n        metoda realizujÄ…ca rzut koÅ›ciÄ… - zwraca liczbÄ™ losowÄ… w zakresie 1 do liczby scian\n        \"\"\"\n        return randint(1, self.sciany)\n\n\na = Kosc()\n# rzuÄ‡ 10 razy koÅ›ciÄ… i wyniki zapisz do listy\n[a.roll() for _ in range(10)]\n\n[1, 1, 4, 2, 4, 2, 4, 3, 2, 6]\n\n\n\nfrom random import choice\nchoice([0,1,2,3,4])\n\n1\n\n\n\nfrom random import choice\n\nclass RandomWalk():\n    def __init__(self, num_points=5000):\n        self.num_points = num_points\n        self.x_values = [0]\n        self.y_values = [0]\n    \n    def fill_walk(self):\n        while len(self.x_values) &lt; self.num_points:\n            # ruch prawo-lewo \n            # wylosuj kierunek dodatni lub ujemy oraz odlegÅ‚oÅ›Ä‡ 0-5 i przypisz do zmiennych\n            \n            x_direction = choice([-1,1])\n            x_distance = choice([0,1,2,3,4])\n            x_step = x_direction*x_distance\n            \n            y_direction = choice([-1,1])\n            y_distance = choice([0,1,2,3,4])\n            y_step = y_direction*y_distance\n            \n            # napisz warunek pomijajÄ…cy krok gdy x i y step = 0 (uÅ¼yj continue)\n            if x_step == 0 and y_step == 0:\n                continue\n            \n            next_x = self.x_values[-1] + x_step\n            next_y = self.y_values[-1] + y_step\n            \n            self.x_values.append(next_x)\n            self.y_values.append(next_y)\n\n\nrw = RandomWalk()\nrw.fill_walk()\n\nrw.x_values[:5]\n\n[0, 0, -1, 2, 5]\n\n\n\nimport matplotlib.pyplot as  plt\npoint_number = list(range(rw.num_points))\nplt.scatter(rw.x_values, rw.y_values, c=point_number, cmap=plt.cm.Blues,\n           edgecolor='none', s=15)\nplt.scatter(0,0,c='green', edgecolor='none', s=100)\nplt.scatter(rw.x_values[-1], rw.y_values[-1],c='red', edgecolor='none', s=100)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSztuczne neurony - rys historyczny\nW 1943 roku W. McCulloch i W. Pitts zaprezentowali pierwszÄ… koncepcjÄ™ uproszczonego modelu komÃ³rki nerwowej tzw. Nuronu McCulloch-Pittsa (MCP). W.S. McCulloch, W. Pitts, A logical Calculus of the Ideas Immanent in Nervous Activity. â€œThe Bulletin of Mathematical Biophysicsâ€ 1943 nr 5(4)\nNeuronami nazywamy wzajemnie poÅ‚Ä…czone komÃ³rki nerwowe w mÃ³zgu, ktÃ³re sÄ… odpowiedzialne za przetwarzanie i przesyÅ‚anie sygnaÅ‚Ã³w chemicznych i elektrycznych. KomÃ³rka taka opisana jest jako bramka logiczna zawierajÄ…ca binarne wyjÅ›cia. Do dendrytÃ³w dociera duÅ¼a liczba sygnaÅ‚Ã³w, ktÃ³re sÄ… integrowane w ciele komÃ³rki i (jeÅ¼eli energia przekracza okreÅ›lonÄ… wartoÅ›Ä‡ progowÄ…) zostaje wygenerowany sygnaÅ‚ wyjÅ›ciowy przepuszczany przez akson.\n\nImage(filename='../img/02_01.png', width=800) \n\n\n\n\n\n\n\n\nPo kilku latach Frank Rosenblatt (na podstawie MCP) zaproponowaÅ‚ pierwszÄ… koncepcjÄ™ reguÅ‚y uczenia perceprtonu. F. Rosenblatt, The Perceptron, a Perceiving and Recognizing Automaton, Cornell Aeronautical Laboratory, 1957\n\nImage(filename='../img/02_04.png', width=800) \n\n\n\n\n\n\n\n\n\nImage(filename='../img/02_02.png', width=800) \n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\ny = np.where(y == 0, -1, 1)\n\nimport matplotlib.pyplot as plt\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\ndziecko = Perceptron()\ndziecko.fit()\n\n# dziecko musi mieÄ‡ parametr uczenia\ndziecko.eta\n\n# moÅ¼emy sprawdziÄ‡ jak szybko siÄ™ uczy == ile bÅ‚Ä™dÃ³w robi\n\ndziecko.errors_ \n\n# rozwiÄ…zania znajdÄ… siÄ™ w wagach\ndziecko.w_\n# w naszym przypadku dziecko uczy siÄ™ dwÃ³ch wag !\n\n\nclass Perceptron():\n    def __init__(self, n_iter=10, eta=0.01):\n        self.n_iter = n_iter\n        self.eta = eta\n        \n    def fit(self, X, y):\n        self.w_ = np.zeros(1+X.shape[1])\n        self.errors_ = []\n        for _ in range(self.n_iter):\n            pass\n        return self\n\n\nimport random\n\nclass Perceptron():\n    \n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n    \n    def fit(self, X, y):\n        \n        #self.w_ = np.zeros(1+X.shape[1])\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] \n        self.errors_ = []\n        \n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X,y):\n                #print(xi, target)\n                update = self.eta*(target-self.predict(xi))\n                #print(update)\n                self.w_[1:] += update*xi\n                self.w_[0] += update\n                #print(self.w_)\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:])+self.w_[0]\n    \n    def predict(self, X):\n        return np.where(self.net_input(X)&gt;=0.0, 1, -1)\n\n\n# uzycie jak wszsytkie klasy sklearn\nppn = Perceptron()\nppn.fit(X,y)\n\n&lt;__main__.Perceptron at 0x30454a810&gt;\n\n\n\nprint(ppn.errors_)\nprint(ppn.w_)\n\n[7, 3, 2, 3, 2, 1, 0, 0, 0, 0]\n[-0.7593420638708699, 0.06978770951926482, 0.13667189800765753]\n\n\n\nppn.predict(np.array([-3, 5]))\n\narray(-1)\n\n\n\n# dodatkowa funkcja\n\nfrom matplotlib.colors import ListedColormap\n\ndef plot_decision_regions(X,y,classifier, resolution=0.02):\n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max()+1\n    x2_min, x2_max = X[:,1].min() -1, X[:,1].max()+1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(),xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl,0], y=X[y==cl,1], alpha=0.8, color=cmap(idx), marker=markers[idx], label=cl)\n\n# dla kwiatkÃ³w\n\n\nplot_decision_regions(X,y,classifier=ppn)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nImage(filename='../img/02_09.png', width=600) \n\n\n\n\n\n\n\n\n\n# ZADANIE - Opisz czym rÃ³Å¼ni siÄ™ poniÅ¼szy algorytm od Perceprtona ? \nclass Adaline():\n    '''Klasyfikator  - ADAptacyjny LIniowy NEuron'''\n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X,y):\n        #self.w_ = np.zeros(1+X.shape[1])\n        import random\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])]\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(X)\n            errors = (y-output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        return self.net_input(X)\n\n    def predict(self, X):\n        return np.where(self.activation(X) &gt;= 0.0, 1, -1) \n\n\nad = Adaline(n_iter=20, eta=0.01)\n\nad.fit(X,y)\n\nprint(ad.w_)\n\nplot_decision_regions(X,y,classifier=ad)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n[-7.007375104548615e+30, -3.913586524697914e+31, -2.1895923576993013e+31]\n\n\n\n\n\n\n\n\n\n\nad.cost_[:10]\n\n[589.6216559520215,\n 894654.4940675091,\n 1395529303.6601284,\n 2176827739791.7388,\n 3395542469963400.5,\n 5.29656456254259e+18,\n 8.261889348562429e+21,\n 1.2887375354700108e+25,\n 2.010247735426649e+28,\n 3.1357012941461006e+31]\n\n\n\nad2 = Adaline(n_iter=100, eta=0.0001)\n\nad2.fit(X,y)\n\nplot_decision_regions(X,y,classifier=ad2)\nplt.xlabel(\"dlugosc dzialki [cm]\")\nplt.ylabel(\"dlugosc platka [cm]\")\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(ad2.w_)\n\nad2.cost_[-10:]\n\n[0.03259855193831394, -0.2832913340615083, 0.5509821059793044]\n\n\n[9.092674275002349,\n 8.949907500366704,\n 8.810273785299684,\n 8.673704364120878,\n 8.540131980448582,\n 8.409490854073036,\n 8.28171664855683,\n 8.15674643954629,\n 8.034518683778513,\n 7.91497318876858]\n\n\n\n%%file app.py\n\nimport pickle\nimport numpy as np\nfrom flask import Flask, request, jsonify\n\nclass Perceptron():\n    \n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n    \n    def fit(self, X, y):\n        self.w_ = [random.uniform(-1.0, 1.0) for _ in range(1+X.shape[1])] \n        self.errors_ = []\n        \n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X,y):\n                update = self.eta*(target-self.predict(xi))\n                self.w_[1:] += update*xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:])+self.w_[0]\n    \n    def predict(self, X):\n        return np.where(self.net_input(X)&gt;=0.0, 1, -1)\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/predict_get', methods=['GET'])\ndef get_prediction():\n    # sepal length\n    sepal_length = float(request.args.get('sl'))\n    petal_length = float(request.args.get('pl'))\n    \n    features = [sepal_length, petal_length]\n\n    # Load pickled model file\n    with open('model.pkl',\"rb\") as picklefile:\n        model = pickle.load(picklefile)\n        \n    # Predict the class using the model\n    predicted_class = int(model.predict(features))\n    \n    # Return a json object containing the features and prediction\n    return jsonify(features=features, predicted_class=predicted_class)\n\n@app.route('/predict_post', methods=['POST'])\ndef post_predict():\n    data = request.get_json(force=True)\n    # sepal length\n    sepal_length = float(data.get('sl'))\n    petal_length = float(data.get('pl'))\n    \n    features = [sepal_length, petal_length]\n\n    # Load pickled model file\n    with open('model.pkl',\"rb\") as picklefile:\n        model = pickle.load(picklefile)\n        \n    # Predict the class using the model\n    predicted_class = int(model.predict(features))\n    output = dict(features=features, predicted_class=predicted_class)\n    # Return a json object containing the features and prediction\n    return jsonify(output)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n\nWriting app.py\n\n\n\nimport requests\nresponse = requests.get(\"http://127.0.0.1:5000/predict_get?sl=6.3&pl=2.6\")\nprint(response.content)\n\n\nimport requests\njson = {\"sl\":2.4, \"pl\":2.6}\nresponse = requests.post(\"http://127.0.0.1:5000/predict_post\", json=json)\nprint(response.content)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Optymalizacja modeli w Pythonie"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Dane ustrukturyzowane",
    "section": "",
    "text": "Dane klienta to pewne wartoÅ›ci, ktÃ³re moÅ¼esz przypisaÄ‡ do zmiennych: np wiek: 42, wzrost: 178, pozyczka: 1000, zarobki: 5000, imiÄ™: Jan\nZdefiniuj zmienne customer_1_{cecha} i przypisz im wartoÅ›ci z przykÅ‚adu powyÅ¼ej\ncustomer_1_wiek = 42\ncustomer_1_wzrost = 178\ncustomer_1_pozyczka = 1000\ncustomer_1_zarobki = 5000\ncustomer_1_imie = 'Jan'\ncustomer_1 = [42,178,1000,5000,'Jan']\nWeÅºmy dwie listy numeryczne\na = [1,2,3]\nb = [4,5,6]\n# dodawanie list\nprint(f\"a+b: {a+b}\")\n# moÅ¼na teÅ¼ uÅ¼yÄ‡ metody format\nprint(\"a+b: {}\".format(a+b))\n\na+b: [1, 2, 3, 4, 5, 6]\na+b: [1, 2, 3, 4, 5, 6]\n# mnoÅ¼enie list\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\nno-defined operation\nKaÅ¼dy obiekt pythonowy moÅ¼na rozszerzyÄ‡ o nowe metody i atrybuty.\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n[1 2 3] [4 5 6]\nprint(f\"aa+bb: {aa+bb}\")\n# dodawanie dziaÅ‚a\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - czy to poprawne mnoÅ¼enie?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - a czy otrzymany wynik teÅ¼ realizuje poprawne mnoÅ¼enie?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# mnoÅ¼enie rÃ³wnieÅ¼ dziaÅ‚a\n\naa+bb: [5 7 9]\n==================================================\n[ 4 10 18]\naa*bb - czy to poprawne mnoÅ¼enie?\n32\nnp.dot - a czy otrzymany wynik teÅ¼ realizuje poprawne mnoÅ¼enie?\nCo dziaÅ‚a szybciej?\ndef iloczyn_skalarny_lista(x: list, y: list) -&gt; float: \n    iloczyn = 0.\n    for i in range(len(x)):\n        iloczyn += x[i] * y[i]\n    return iloczyn\n\na = list(range(1000))\nb = list(range(1000))\n\n%timeit iloczyn_skalarny_lista(a, b)\n\n27.9 Î¼s Â± 1.22 Î¼s per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\nimport numpy as np\ndef iloczyn_skalarny_numpy(x, w):\n    return x.dot(w)\n    \na = np.arange(1000)\nb = np.arange(1000)\n\n%timeit iloczyn_skalarny_numpy(a, b)\n\n545 ns Â± 3.29 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)\n# wÅ‚asnoÅ›ci tablic\nx = np.array(range(4))\nprint(x)\nx.shape\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\n[0 1 2 3]\nscalar object dim: 0\nvector object dim: 1\nmatrix object dim: 2\nObliczenia wykonywane na danych mieszczÄ…cych siÄ™ w pamiÄ™ci. &gt; czy moÅ¼na jeszcze przyÅ›pieszyÄ‡ obliczenia?\nKurs Numpy ze strony Sebastiana Raschki",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#pytorch",
    "href": "labs/lab3.html#pytorch",
    "title": "Dane ustrukturyzowane",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source Python-based deep learning library. PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n\nPyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\nPyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\nPyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n\n\nimport torch\n\nprint(torch.__version__)\n\nprint(torch.cuda.is_available())\nprint(torch.backends.mps.is_available())\n\ntensor0d = torch.tensor(1) \ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n2.6.0\nFalse\nTrue\n\n\n\n# typy\nprint(tensor1d.dtype)\n\nprint(torch.tensor([1.0, 2.0, 3.0]).dtype)\n\ntorch.int64\ntorch.float32\n\n\n\ntensor2d.shape\nprint(tensor2d.reshape(3, 2))\nprint(tensor2d.view(3, 2))\n\ntensor([[1, 2],\n        [2, 3],\n        [4, 5]])\ntensor([[1, 2],\n        [2, 3],\n        [4, 5]])\n\n\n\nprint(tensor2d.T)\n\ntensor([[1, 3],\n        [2, 4],\n        [2, 5]])\n\n\n\nprint(tensor2d.matmul(tensor2d.T))\n\nprint(tensor2d @ tensor2d.T)\n\ntensor([[ 9, 21],\n        [21, 50]])\ntensor([[ 9, 21],\n        [21, 50]])\n\n\nszczegÃ³Å‚owe info znajdziesz w dokumentacji",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#modelowanie-danych-ustrukturyzowanych",
    "href": "labs/lab3.html#modelowanie-danych-ustrukturyzowanych",
    "title": "Dane ustrukturyzowane",
    "section": "Modelowanie danych ustrukturyzowanych",
    "text": "Modelowanie danych ustrukturyzowanych\nRozwaÅ¼my jednÄ… zmiennÄ… (xs) od ktÃ³rej zaleÅ¼y nasza zmienna wynikowa (ys - target).\nxs = np.array([-1,0,1,2,3,4])\nys = np.array([-3,-1,1,3,5,7])\nModelem ktÃ³ry moÅ¼emy zastosowaÄ‡ jest regresja liniowa.\n\n# Regresja liniowa \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nxs = np.array([-1,0,1,2,3,4])\n# a raczej \nxs = xs.reshape(-1, 1)\n\nys = np.array([-3, -1, 1, 3, 5, 7])\n\nreg = LinearRegression()\nmodel = reg.fit(xs,ys)\n\nprint(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n\nmodel.predict(np.array([[1],[5]]))\n\nsolution: x1=2.0, x0=-1.0\n\n\narray([1., 9.])\n\n\nProsty kod realizuje w peÅ‚ni nasze zadanie znalezienia modelu regresji liniowej.\nDo czego moÅ¼e nam posÅ‚uÅ¼yc tak wygenerowany model?\nAby z niego skorzystac potrzebujemy wyeksportowaÄ‡ go do pliku.\nWykorzystaj bibliotekÄ™ pickle w celu zapisu obiektu modelu\n\n# save model\nimport pickle\nwith open('model.pkl', \"wb\") as picklefile:\n    pickle.dump(model, picklefile)\n\nTeraz moÅ¼emy go zaimportowaÄ‡ (np na Github) i wykorzystaÄ‡ w innych projektach.\n\n# load model\nwith open('model.pkl',\"rb\") as picklefile:\n    mreg = pickle.load(picklefile)\n\n\nmreg.predict(xs)\n\narray([-3., -1.,  1.,  3.,  5.,  7.])\n\n\n\nfrom torch.nn import Linear\nx_t = torch.tensor([1,2,3,4,5]).view(-1,1)\nx_t=x_t.to(torch.float32)\nm = Linear(1,1)\nm(torch.tensor([1.]))\n\ntensor([-0.5447], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nm(x_t)\n\ntensor([[-0.5447],\n        [-0.8818],\n        [-1.2190],\n        [-1.5562],\n        [-1.8934]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n# forward \nimport torch.nn.functional as F\n\ny = torch.tensor([1.0])\nx1 = torch.tensor([1.1])\nw1 = torch.tensor([2.2], requires_grad=True)\nb  = torch.tensor([0.2], requires_grad=True)\nz = x1 * w1 + b\na = torch.sigmoid(z)\nloss = F.binary_cross_entropy(a,y)\n\n# automatic diff \nfrom torch.autograd import grad\n\ngrad_L_w1 = grad(loss, w1, retain_graph= True)\ngrad_L_b = grad(loss, b, retain_graph= True)\n\nloss.backward()\nprint(w1.grad)\n\ntensor([-0.0746])\n\n\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self, n_input:int, n_output: int):\n        super(LinearRegression, self).__init__()\n\n        self.layers = torch.nn.Sequential(\n                torch.nn.Linear(n_input, n_output)\n        )\n    def forward(self, x):\n        return self.layers(x)\n\n\nx = np.array(xs, dtype=np.float32)\ny = np.array(ys, dtype=np.float32)\n\nX_train = torch.from_numpy(x).view(-1,1)\ny_train = torch.from_numpy(y).view(-1,1)\n\n\nlr_model = LinearRegression(1,1)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)\n\n\nnum_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\nprint(f\"liczba trenowalnych parametrÃ³w: {num_params}\")\n\nliczba trenowalnych parametrÃ³w: 2\n\n\n\nfor layer in lr_model.layers:\n    if isinstance(layer, torch.nn.Linear):\n        print(f\"weight: {layer.state_dict()['weight']}\")\n        print(f\"bias: {layer.state_dict()['bias']}\")\n\nweight: tensor([[0.0460]])\nbias: tensor([0.4732])\n\n\n\nepochs = 10\n# petla uczaca \nfor epoch in range(epochs):\n    lr_model.train() # etap trenowania \n\n    y_pred = lr_model(X_train)\n    loss = criterion(y_pred, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.2f}')\n \n    lr_model.eval() # etap ewaluacji modelu\n\n# po treningu jeszcze raz generujemy predykcje\nlr_model.eval()\nwith torch.no_grad():\n    predicted = lr_model(X_train)\n\n\nlr_model.layers[0].weight, lr_model.layers[0].weight.shape\n\n(Parameter containing:\n tensor([[1.0423]], requires_grad=True),\n torch.Size([1, 1]))",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab3.html#inne-sposoby-pozyskiwania-danych",
    "href": "labs/lab3.html#inne-sposoby-pozyskiwania-danych",
    "title": "Dane ustrukturyzowane",
    "section": "Inne sposoby pozyskiwania danych",
    "text": "Inne sposoby pozyskiwania danych\n\nGotowe ÅºrÃ³dÅ‚a w bibliotekach pythonowych\nDane z plikÃ³w zewnÄ™trznych (np. csv, json, txt) z lokalnego dysku lub z internetu\nDane z bazy danych (np. MySQL, PostgreSQL, MongoDB)\nDane generowane w sposÃ³b sztuczny pod wybrany problem modelowy.\nStrumienie danych\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\n# find all keys\nprint(iris.keys())\n\n# print description\nprint(iris.DESCR)\n\n\nimport pandas as pd\nimport numpy as np\n\n# create DataFrame\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n    - class:\n            - Iris-Setosa\n            - Iris-Versicolour\n            - Iris-Virginica\n\n:Summary Statistics:\n\n============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD   Class Correlation\n============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n============== ==== ==== ======= ===== ====================\n\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n:Creator: R.A. Fisher\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n:Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. dropdown:: References\n\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n    Mathematical Statistics\" (John Wiley, NY, 1950).\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n    Structure and Classification Rule for Recognition in Partially Exposed\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n    on Information Theory, May 1972, 431-433.\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n    conceptual clustering system finds 3 classes in the data.\n  - Many, many more ...\n\n\n\n\n# show last\ndf.tail(10)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n140\n6.7\n3.1\n5.6\n2.4\n2.0\n\n\n141\n6.9\n3.1\n5.1\n2.3\n2.0\n\n\n142\n5.8\n2.7\n5.1\n1.9\n2.0\n\n\n143\n6.8\n3.2\n5.9\n2.3\n2.0\n\n\n144\n6.7\n3.3\n5.7\n2.5\n2.0\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2.0\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2.0\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2.0\n\n\n\n\n\n\n\n\n# show info about NaN values and a type of each column.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\n 4   target             150 non-null    float64\ndtypes: float64(5)\nmemory usage: 6.0 KB\n\n\n\n# statistics\ndf.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n1.000000\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n0.819232\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n0.000000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n0.000000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n1.000000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n2.000000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n2.000000\n\n\n\n\n\n\n\n\n# new features\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\n# remove features (columns) \ndf = df.drop(columns=['target'])\n# filtering first 100 rows and 4'th column\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\niris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\nf, ax = plt.subplots(1, figsize=(15,9))\nsns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)\n\n\n\n\n\n\n\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\n\ny = np.where(y == 'setosa',-1,1)\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Perceptron\n\nper_clf = Perceptron()\nper_clf.fit(X,y)\n\ny_pred = per_clf.predict([[2, 0.5],[4,5.5]])\ny_pred\n\narray([-1,  1])\n\n\nÅºrÃ³dÅ‚a zewnÄ™trzne\n\nIRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndf = pd.read_csv(IRIS_PATH, names=col_names)\n\n\n# save to sqlite\nimport sqlite3\n# generate database\nconn = sqlite3.connect(\"iris.db\")\n# pandas to_sql\n\ntry:\n    df.to_sql(\"iris\", conn, index=False)\nexcept:\n    print(\"tabela juÅ¼ istnieje\")\n\ntabela juÅ¼ istnieje\n\n\n\nresult = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length &gt; 5\", conn)\n\nSztuczne dane\n\n# Dane sztucznie generowane\nfrom sklearn import datasets\nX, y = datasets.make_classification(n_samples=10**4,\nn_features=20, n_informative=2, n_redundant=2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# podziaÅ‚ na zbiÃ³r treningowy i testowy\ntrain_samples = 7000 # 70% danych treningowych\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\nrfc.predict(X_train[0].reshape(1, -1))\n\narray([0])\n\n\nAnaliza Laboratorium 3 - Chatgbt\n\nPraca z danymi ustrukturyzowanymi:\n\nÄ†wiczenia rozpoczynajÄ… siÄ™ od reprezentacji danych klienta za pomocÄ… zmiennych, list i struktur NumPy. DziÄ™ki temu studenci uczÄ… siÄ™, dlaczego listy nie sÄ… optymalne do przechowywania danych oraz jakie korzyÅ›ci niesie ze sobÄ… uÅ¼ycie tablic NumPy, takie jak efektywnoÅ›Ä‡ obliczeniowa i moÅ¼liwoÅ›Ä‡ wykonywania operacji wektorowych.\n\nWprowadzenie do PyTorch:\n\nLaboratorium wprowadza podstawy biblioteki PyTorch, prezentujÄ…c jej moÅ¼liwoÅ›ci w zakresie obliczeÅ„ tensorowych oraz automatycznego rÃ³Å¼niczkowania. To przygotowuje studentÃ³w do bardziej zaawansowanych zastosowaÅ„, takich jak budowa i trenowanie modeli uczenia maszynowego. ï¿¼\nPropozycje rozszerzeÅ„\nPropozycje do pracy dla studentÃ³w jako rozszerzenia:\n\nIntegracja z Pandas: Wprowadzenie biblioteki Pandas do pracy z danymi tabelarycznymi pozwoliÅ‚oby studentom lepiej zrozumieÄ‡ manipulacjÄ™ danymi i ich przygotowanie do analizy.\n\n\nPandas DataFrames zostaÅ‚ omÃ³wiony jako podobieÅ„stwo obiektÃ³w pythona do tabel SQL.\n\n\nWizualizacja danych: Dodanie Ä‡wiczeÅ„ z wykorzystaniem bibliotek takich jak Matplotlib czy Seaborn umoÅ¼liwiÅ‚oby studentom lepsze zrozumienie danych poprzez ich graficznÄ… reprezentacjÄ™.\n\n\nCzÄ™Å›ciowe przykÅ‚ady pokazujÄ… jak wykorzystaÄ‡ analizÄ™ danych za pomocÄ… bibliotek takich jak Matplotlib czy Seaborn, oraz jak interpretowaÄ‡ otrzymywane wyniki.\n\n\nPraktyczne zastosowania: Zaproponowanie mini-projektu, w ktÃ³rym studenci analizujÄ… rzeczywiste dane (np. dane finansowe czy dane z mediÃ³w spoÅ‚ecznoÅ›ciowych) przy uÅ¼yciu poznanych narzÄ™dzi, zwiÄ™kszyÅ‚oby zaangaÅ¼owanie i pokazaÅ‚o praktyczne zastosowanie zdobytej wiedzy.\n\n\nProjekt analizy danych tabelarycznych realizowany jest jako jedno z zadaÅ„ domowych.\n\n\nPorÃ³wnanie z innymi bibliotekami: Przedstawienie rÃ³Å¼nic miÄ™dzy NumPy, PyTorch i TensorFlow w kontekÅ›cie analizy danych mogÅ‚oby pomÃ³c studentom w wyborze odpowiednich narzÄ™dzi do konkretnych zadaÅ„.\n\nPodsumowanie\nLaboratorium 3 stanowi solidne wprowadzenie do pracy z danymi ustrukturyzowanymi i wykorzystania bibliotek NumPy oraz PyTorch. Dodanie powyÅ¼szych rozszerzeÅ„ mogÅ‚oby jeszcze bardziej zwiÄ™kszyÄ‡ wartoÅ›Ä‡ edukacyjnÄ… zajÄ™Ä‡, przygotowujÄ…c studentÃ³w do realnych wyzwaÅ„ w analizie danych w czasie rzeczywistym.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Wprowadzenie do Apache Spark",
    "section": "",
    "text": "Apache Spark to ogÃ³lnego przeznaczenia, przetwarzajÄ…cy dane w pamiÄ™ci silnik obliczeniowy. Spark moÅ¼e byÄ‡ uÅ¼ywany wraz z Hadoopem, Yarnem i innymi komponentami Big Data, aby w peÅ‚ni wykorzystaÄ‡ jego moÅ¼liwoÅ›ci oraz poprawiÄ‡ wydajnoÅ›Ä‡ aplikacji. Oferuje wysokopoziomowe interfejsy API w jÄ™zykach Scala, Java, Python, R i SQL.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Wprowadzenie do Apache Spark"
    ]
  },
  {
    "objectID": "labs/lab6.html#architektura-spark",
    "href": "labs/lab6.html#architektura-spark",
    "title": "Wprowadzenie do Apache Spark",
    "section": "Architektura Spark",
    "text": "Architektura Spark\nApache Spark dziaÅ‚a w architekturze mistrz-podwÅ‚adny (master-slave), gdzie gÅ‚Ã³wny wÄ™zeÅ‚ nazywany jest â€Driverâ€, a wÄ™zÅ‚y podrzÄ™dne to â€Workersâ€. Punktem startowym aplikacji Spark jest sc, czyli instancja klasy SparkContext, ktÃ³ra dziaÅ‚a wewnÄ…trz Drivera.\n\n\n\nArchitektura Spark 1\n\n\n\n\n\nArchitektura Spark 2",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Wprowadzenie do Apache Spark"
    ]
  },
  {
    "objectID": "labs/lab6.html#gÅ‚Ã³wne-komponenty-apache-spark",
    "href": "labs/lab6.html#gÅ‚Ã³wne-komponenty-apache-spark",
    "title": "Wprowadzenie do Apache Spark",
    "section": "GÅ‚Ã³wne komponenty Apache Spark",
    "text": "GÅ‚Ã³wne komponenty Apache Spark\n\nSpark Core\nCzÄ™sto nazywany rÃ³wnieÅ¼ samym â€Sparkâ€. Podstawowym elementem Sparka jest RDD (Resilient Distributed Dataset) â€” odporna na bÅ‚Ä™dy, rozproszona kolekcja danych, przetwarzana rÃ³wnolegle na wielu wÄ™zÅ‚ach klastra.\n\n\nSpark SQL\nW tej bibliotece dane reprezentowane sÄ… jako DataFrame, czyli struktura danych podobna do tabeli relacyjnej. Spark SQL umoÅ¼liwia analizÄ™ danych z uÅ¼yciem skÅ‚adni podobnej do SQL oraz funkcji przetwarzajÄ…cych dane.\n\n\nSpark Streaming\nBiblioteka ta wprowadza pojÄ™cie D-Stream (Discretized Stream) â€” strumienia danych dzielonego na maÅ‚e porcje (mikropartie), ktÃ³re moÅ¼na przetwarzaÄ‡ niemal w czasie rzeczywistym.\n\n\nMLlib\nBiblioteka do uczenia maszynowego, zawierajÄ…ca popularne algorytmy, takie jak filtrowanie kolaboracyjne, klasyfikacja, klasteryzacja czy regresja.\n\n\nGraphX\nBiblioteka sÅ‚uÅ¼Ä…ca do przetwarzania grafÃ³w. UmoÅ¼liwia rozwiÄ…zywanie problemÃ³w z zakresu teorii grafÃ³w, takich jak PageRank, komponenty spÃ³jne i inne.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Wprowadzenie do Apache Spark"
    ]
  },
  {
    "objectID": "labs/lab6.html#uruchomienie-apache-spark",
    "href": "labs/lab6.html#uruchomienie-apache-spark",
    "title": "Wprowadzenie do Apache Spark",
    "section": "Uruchomienie Apache Spark",
    "text": "Uruchomienie Apache Spark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Lab6\").getOrCreate()\nspark\n\nSparkSession - in-memory\n\nSparkContext\n\n...\n\nPrzykÅ‚ad 1 - dane ralizujÄ…ce szereg czasowy\nczujnik_temperatury = ((12.5, \"2019-01-02 12:00:00\"),\n(17.6, \"2019-01-02 12:00:20\"),\n(14.6,  \"2019-01-02 12:00:30\"),\n(22.9,  \"2019-01-02 12:01:15\"),\n(17.4,  \"2019-01-02 12:01:30\"),\n(25.8,  \"2019-01-02 12:03:25\"),\n(27.1,  \"2019-01-02 12:02:40\"),\n)\nDane realizujÄ…ce pomiar temperatury w czasie.\nAby wygenerowaÄ‡ DataFrame naleÅ¼y uÅ¼yÄ‡ metod createDataFrame. Jednak naleÅ¼y pamiÄ™taÄ‡ aby zdefiniowaÄ‡ typy danych.\nW nastÄ™pnych laboratoriach szerzej opiszemy typy danych w Sparku.\nZdefiuniujmy schemat naszych danych.\nfrom pyspark.sql.functions import to_timestamp\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\nschema = StructType([\n    StructField(\"temperatura\", DoubleType(), True),\n    StructField(\"czas\", StringType(), True),\n])\nJak widaÄ‡ praktycznie wszystkie elementy poza nazwÄ… kolumny oraz parametrem True sÄ… przedstawione jako obiekty.\ndf = (spark.createDataFrame(czujnik_temperatury, schema=schema)\n      .withColumn(\"czas\", to_timestamp(\"czas\")))\nSprawdÅºmy jak wyglÄ…da schemat utworzonej tabeli.\ndf.printSchema()\n\nroot\n |-- temperatura: double (nullable = true)\n |-- czas: timestamp (nullable = true)\n\nNastÄ™pnie moÅ¼emy sprawdziÄ‡ jak przedstawia siÄ™ sama tabela.\ndf.show()\n+-----------+-------------------+\n|temperatura|               czas|\n+-----------+-------------------+\n|       12.5|2019-01-02 12:00:00|\n|       17.6|2019-01-02 12:00:20|\n|       14.6|2019-01-02 12:00:30|\n|       22.9|2019-01-02 12:01:15|\n|       17.4|2019-01-02 12:01:30|\n|       25.8|2019-01-02 12:03:25|\n|       27.1|2019-01-02 12:02:40|\n+-----------+-------------------+\n\n\nSpark jako SQL\nRamki danych w sparku pozwalajÄ… wykorzystaÄ‡ jÄ™zyk sql:\ndf.createOrReplaceTempView(\"czujnik_temperatury\")\nspark.sql(\"SELECT * FROM czujnik_temperatury where temperatura &gt; 21\").show()\n+-------------------+-----------+\n|               czas|temperatura|\n+-------------------+-----------+\n|2019-01-02 12:01:15|       22.9|\n|2019-01-02 12:03:25|       25.8|\n|2019-01-02 12:02:40|       27.1|\n+-------------------+-----------+\n\n\nGrupowanie danych\nStandardowy grupowanie danych w sparku po zmiennej â€œczasâ€ wygeneruje nam liczbÄ™ wierszy w kaÅ¼dym grupie. Ze wzglÄ™du, iÅ¼ zmienne czasowe majÄ… rÃ³Å¼ne wartoÅ›ci, iloÅ›Ä‡ otrzymanych grup bÄ™dzie rÃ³wna iloÅ›ci wierszy w tabeli.\ndf2 = df.groupBy(\"czas\").count()\ndf2.show()\nWykorzystujÄ…c funkcjÄ™ window moÅ¼emy wygenerowaÄ‡ grupy czasowe w zaleÅ¼noÅ›ci od wybranego okna czasowego.\n# Thumbling window\n\nimport pyspark.sql.functions as F\n\ndf2 = df.groupBy(F.window(\"czas\",\"30 seconds\")).count()\ndf2.show(truncate=False)\n\n+------------------------------------------+-----+\n|window                                    |count|\n+------------------------------------------+-----+\n|{2019-01-02 12:00:00, 2019-01-02 12:00:30}|2    |\n|{2019-01-02 12:00:30, 2019-01-02 12:01:00}|1    |\n|{2019-01-02 12:01:00, 2019-01-02 12:01:30}|1    |\n|{2019-01-02 12:01:30, 2019-01-02 12:02:00}|1    |\n|{2019-01-02 12:03:00, 2019-01-02 12:03:30}|1    |\n|{2019-01-02 12:02:30, 2019-01-02 12:03:00}|1    |\n+------------------------------------------+-----+\nSprawdÅºmy schemat\ndf2.printSchema()\nroot\n |-- window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- count: long (nullable = false)\nPodstawowa rÃ³Å¼nica miÄ™dzy pandasowymi ramkami danych i sparkowymi jest taka, Å¼e w komÃ³rce danych sparkowych moÅ¼na uÅ¼ywaÄ‡ typÃ³w zÅ‚oÅ¼onych - np struct.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Wprowadzenie do Apache Spark"
    ]
  },
  {
    "objectID": "labs/lab8.html",
    "href": "labs/lab8.html",
    "title": "Apache Spark z plikami RDD",
    "section": "",
    "text": "from pyspark import SparkContext\n\nsc = SparkContext(appName=\"myAppName\")",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#rdd",
    "href": "labs/lab8.html#rdd",
    "title": "Apache Spark z plikami RDD",
    "section": "RDD",
    "text": "RDD\n\nResilient Distributed Dataset\nPodstawowa abstrakcja oraz rdzeÅ„ Sparka\nObsÅ‚ugiwane przez dwa rodzaje operacji:\n\nAkcje:\n\noperacje uruchamiajÄ…ce egzekucjÄ™ transformacji na RDD\nprzyjmujÄ… RDD jako input i zwracajÄ… wynik NIE bÄ™dÄ…cy RDD\n\nTransformacje:\n\nleniwe operacje\nprzyjmujÄ… RDD i zwracajÄ… RDD\n\n\nIn-Memory - dane RDD przechowywane w pamiÄ™ci\nImmutable\nLazy evaluated\nParallel - przetwarzane rÃ³wnolegle\nPartitioned - rozproszone",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#waÅ¼ne-informacje",
    "href": "labs/lab8.html#waÅ¼ne-informacje",
    "title": "Apache Spark z plikami RDD",
    "section": "WAÅ»NE informacje !",
    "text": "WAÅ»NE informacje !\nWaÅ¼ne do zrozumienia dziaÅ‚ania SPARKA:\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRDD\nResilient Distributed Dataset\n\n\nTransformation\nSpark operation that produces an RDD\n\n\nAction\nSpark operation that produces a local object\n\n\nSpark Job\nSequence of transformations on data with a final action\n\n\n\nDwie podstawowe metody tworzenia RDD:\n\n\n\n\n\n\n\nMethod\nResult\n\n\n\n\nsc.parallelize(array)\nCreate RDD of elements of array (or list)\n\n\nsc.textFile(path/to/file)\nCreate RDD of lines from file\n\n\n\nPodstawowe transformacje\n\n\n\n\n\n\n\nTransformation Example\nResult\n\n\n\n\nfilter(lambda x: x % 2 == 0)\nDiscard non-even elements\n\n\nmap(lambda x: x * 2)\nMultiply each RDD element by 2\n\n\nmap(lambda x: x.split())\nSplit each string into words\n\n\nflatMap(lambda x: x.split())\nSplit each string into words and flatten sequence\n\n\nsample(withReplacement=True,0.25)\nCreate sample of 25% of elements with replacement\n\n\nunion(rdd)\nAppend rdd to existing RDD\n\n\ndistinct()\nRemove duplicates in RDD\n\n\nsortBy(lambda x: x, ascending=False)\nSort elements in descending order\n\n\n\nPodstawowe akcje\n\n\n\n\n\n\n\nAction\nResult\n\n\n\n\ncollect()\nConvert RDD to in-memory list\n\n\ntake(3)\nFirst 3 elements of RDD\n\n\ntop(3)\nTop 3 elements of RDD\n\n\ntakeSample(withReplacement=True,3)\nCreate sample of 3 elements with replacement\n\n\nsum()\nFind element sum (assumes numeric elements)\n\n\nmean()\nFind element mean (assumes numeric elements)\n\n\nstdev()\nFind element deviation (assumes numeric elements)\n\n\n\n\nkeywords = ['Books', 'DVD', 'CD', 'PenDrive'] # nasze dane \n\nkey_rdd = sc.parallelize(keywords) # metoda parallelize - \"wczyta dane\" \n\nkey_rdd.collect() # akcja wyÅ›wietlania \n\n\nkey_small = key_rdd.map(lambda x: x.lower()) # transformacja\n\nkey_small.collect() # akcja \n\n\nsc.stop()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#map-reduce",
    "href": "labs/lab8.html#map-reduce",
    "title": "Apache Spark z plikami RDD",
    "section": "Map reduce",
    "text": "Map reduce\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"new\").getOrCreate()\n\n# otrzymanie obiektu SparkContext\nsc = spark.sparkContext\n\n\ntekst = sc.textFile(\"MobyDick.txt\")\ntekst.take(5)\n\n\nimport re\n# Word Count on RDD \nsc.textFile(\"MobyDick.txt\")\\\n.map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n.flatMap(lambda x: [(y, 1) for y in x]).reduceByKey(lambda x,y: x + y)\\\n.take(5)\n\n\nsc.stop()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#spark-streaming",
    "href": "labs/lab8.html#spark-streaming",
    "title": "Apache Spark z plikami RDD",
    "section": "SPARK STREAMING",
    "text": "SPARK STREAMING\nCzÄ™Å›Ä‡ Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym.\n\nDane mogÄ… pochodziÄ‡ z rÃ³Å¼nych ÅºrÃ³deÅ‚ np. sokety TCP, Kafka, etc. KorzystajÄ…c z poznanych juÅ¼ metod map, reduce, join, oraz window moÅ¼na w Å‚atwy sposÃ³b generowaÄ‡ przetwarzanie strumienia tak jaby byÅ‚ to nieskoÅ„czony ciÄ…g RDD. Ponadto nie ma problemu aby wywoÅ‚aÄ‡ na strumieniu operacje ML czy wykresy.\nCaÅ‚a procedura przedstawia siÄ™ nastÄ™pujÄ…co:\n\nSPARK STREAMING w tej wersji wprowadza abstrakcje zwanÄ… discretized stream DStream (reprezentuje sekwencjÄ™ RDD).\nOperacje na DStream moÅ¼na wykonywaÄ‡ w API JAVA, SCALA, Python, R (nie wszystkie moÅ¼liwoÅ›ci sÄ… dostÄ™pne dla Pythona).\nSpark Streaming potrzebuje minium 2 rdzenie.\n\n\nStreamingContext(sparkContext, batchDuration) - reprezentuje poÅ‚Ä…czenie z klastrem i sÅ‚uÅ¼y do tworzenia DStreamÃ³w, batchDuration wskazuje na granularnoÅ›Ä‡ batchâ€™y (w sekundach)\nsocketTextStream(hostname, port) - tworzy DStream na podstawie danych napÅ‚ywajÄ…cych ze wskazanego ÅºrÃ³dÅ‚a TCP\nflatMap(f), map(f), reduceByKey(f) - dziaÅ‚ajÄ… analogicznie jak w przypadku RDD z tym Å¼e tworzÄ… nowe DStreamâ€™y\npprint(n) - printuje pierwsze n (domyÅ›lnie 10) elementÃ³w z kaÅ¼dego RDD wygenerowanego w DStreamâ€™ie\nStreamingContext.start() - rozpoczyna dziaÅ‚ania na strumieniach\nStreamingContext.awaitTermination(timeout) - oczekuje na zakoÅ„czenie dziaÅ‚aÅ„ na strumieniach\nStreamingContext.stop(stopSparkContext, stopGraceFully) - koÅ„czy dziaÅ‚ania na strumieniach\n\nObiekt StreamingContext moÅ¼na wygenerowaÄ‡ za pomocÄ… obiektu SparkContext.\n\n\n\nimport re\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\n# Create a local StreamingContext with two working thread\n# and batch interval of 1 second\n\nsc = SparkContext(\"local[2]\", \"NetworkWordCount2\")\nssc = StreamingContext(sc, 2)\n\n# DStream\nlines = ssc.socketTextStream(\"localhost\", 9998)\n\n# podziel kaÅ¼dÄ… liniÄ™ na wyrazy\n# DStream jest mapowany na kolejny DStream\n# words = lines.flatMap(lambda line: line.split(\" \"))\n\nwords = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n\n# zliczmy kaÅ¼dy wyraz w kaÅ¼dym batchu\n# DStream jest mapowany na kolejny DStream\n# pairs = words.map(lambda word: (word, 1))\n\n# DStream jest mapowany na kolejny DStream                  \n# wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n\nwordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n# wydrukuj pierwszy element\nwordCounts.pprint()\n\n\n# w konsoli linuxowej netcat Nmap for windows\n!nc -lk 9998\n\n\n# before starting, run a stream data\nssc.start()             # Start the computation\nssc.awaitTermination()\nssc.stop()\nsc.stop()\n\n\nprzesylanie strumienia przez socket\nnc -lk 9998\n\n%%file start_stream.py\n\nfrom socket import *\nimport time\n\nrdd = list()\nwith open(\"MobyDick.txt\", 'r') as ad:\n    for line in ad:\n        rdd.append(line)\n\nHOST = 'localhost'\nPORT = 9998\nADDR = (HOST, PORT)\ntcpSock = socket(AF_INET, SOCK_STREAM)\ntcpSock.bind(ADDR)\ntcpSock.listen(5)\n\n\nwhile True:\n    c, addr = tcpSock.accept()\n    print('got connection')\n    for line in rdd:\n        try:\n            c.send(line.encode())\n            time.sleep(1)\n        except:\n            break\n    c.close()\n    print('disconnected')\n\n\n%%file streamWordCount.py\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split\n\nif __name__ == \"__main__\":\n    spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n    spark.sparkContext.setLogLevel(\"ERROR\")\n    \n    lines = (spark\n         .readStream\n         .format(\"socket\")\n         .option(\"host\", \"localhost\")\n         .option(\"port\", 9998)\n         .load())\n\n    words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n    word_counts = words.groupBy(\"word\").count()\n\n    streamingQuery = (word_counts\n         .writeStream\n         .format(\"console\")\n         .outputMode(\"complete\")\n         .trigger(processingTime=\"5 second\")\n         .start())\n\n    streamingQuery.awaitTermination()\n         \n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split\n\nbatch_counter = {\"count\": 0}\n\ndef process_batch(df, batch_id):\n    batch_counter[\"count\"] += 1\n    print(f\"Batch ID: {batch_id}\")\n    df.show(truncate=False)\n\n\nspark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n    \nlines = (spark\n         .readStream\n         .format(\"socket\")\n         .option(\"host\", \"localhost\")\n         .option(\"port\", 9998)\n         .load())\n\nwords = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\nword_counts = words.groupBy(\"word\").count()\n\nstreamingQuery = (word_counts.writeStream\n         .format(\"console\")\n         .outputMode(\"complete\")\n         .foreachBatch(process_batch) \n         .trigger(processingTime=\"5 second\")\n         .start())",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#uruchomienie-kodu-wysyÅ‚ajÄ…cego-strumieÅ„",
    "href": "labs/lab8.html#uruchomienie-kodu-wysyÅ‚ajÄ…cego-strumieÅ„",
    "title": "Apache Spark z plikami RDD",
    "section": "Uruchomienie kodu wysyÅ‚ajÄ…cego strumieÅ„",
    "text": "Uruchomienie kodu wysyÅ‚ajÄ…cego strumieÅ„\nUzupeÅ‚nij skrypt tak by generowaÅ‚ nastÄ™pujÄ…ce dane:\n\nutwÃ³rz zmiennÄ… message ktÃ³ra bÄ™dzie sÅ‚ownikiem zawierajÄ…cym informacje pojedynczego eventu (klucz: wartoÅ›Ä‡):\n\nâ€œtimeâ€ : aktualny czas w postaci stringu datetime.now()\nâ€œidâ€ : wybierane losowo z listy [â€œaâ€, â€œbâ€, â€œcâ€, â€œdâ€, â€œeâ€]\nâ€œvalue: losowa wartoÅ›Ä‡ z zakresu 0 do 100\n\n\n\n%%file stream.py\n\nimport json\nimport random\nimport sys\nfrom datetime import datetime\nfrom time import sleep\nfrom kafka import KafkaProducer\n\n\nKAFKA_SERVER = \"broker:9092\"\nTOPIC = 'stream'\nLAG = 2\n\ndef create_producer(server):\n    return KafkaProducer(\n        bootstrap_servers=[server],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n        api_version=(3, 7, 0),\n    )\n\nif __name__ == \"__main__\":\n    \n    producer = create_producer(KAFKA_SERVER)\n    try:\n        while True:\n\n            message = {\n                \"time\" : str(datetime.now() )  ,\n                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"])     ,\n                \"temperatura\" : random.randint(-100,100)  ,\n                \"cisnienie\" :  random.randint(0,50)   ,\n            }\n  \n            producer.send(TOPIC, value=message)\n            sleep(LAG)\n    except KeyboardInterrupt:\n        producer.close()\n\nOverwriting stream.py\n\n\n\nw terminalu jupyterlab uruchom plik stream.py\n\npython stream.py\nsprawdz w oknie consumenta czy wysyÅ‚ane wiadomoÅ›ci przychodzÄ… do Kafki.\nZa uruchomienie importu kafka odpowiedzialna jest biblioteka kafka-python ktÃ³rÄ… moÅ¼esz zainstalowaÄ‡ poleceniem pip install kafka-python",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab8.html#apache-spark",
    "href": "labs/lab8.html#apache-spark",
    "title": "Apache Spark z plikami RDD",
    "section": "APACHE SPARK",
    "text": "APACHE SPARK\nPrzygotuj kod skryptu ktÃ³ry pobierze informacje z przesyÅ‚anego strumienia danych.\n\n%%file app.py\n\n# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 app.py\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\nSERVER = \"broker:9092\"\nTOPIC = 'stream'\n\nschema = StructType(\n        [\n            StructField(\"time\", TimestampType()),\n            StructField(\"id\", StringType()),\n            StructField(\"temperatura\", IntegerType()),\n            StructField(\"cisnienie\", IntegerType()),\n        ]\n    )\n\nSCHEMA = \"\"\"time Timestamp, id String, temperatura Int, cisnienie Int \"\"\" # DDL string\n\n\nif __name__ == \"__main__\":\n    \n    spark = SparkSession.builder.getOrCreate()\n    spark.sparkContext.setLogLevel(\"WARN\")\n     \n    raw = (\n        spark.readStream\n        .format(\"kafka\")\n        .option(\"kafka.bootstrap.servers\", SERVER)\n        .option(\"subscribe\", TOPIC)\n        .load()\n    )\n    # query =  (\n    #     raw.writeStream\n    #     .outputMode(\"append\")\n    #     .format(\"console\")\n    #     .option(\"truncate\", False)\n    #     .start()\n    # )\n    parsed = (raw.select(\"timestamp\", from_json(decode(col(\"value\"), \"utf-8\"), SCHEMA).alias(\"moje_dane\"))\n                .select(\"timestamp\", \"moje_dane.*\")\n             )\n    # query =  (\n    #     parsed.writeStream\n    #     .outputMode(\"append\")\n    #     .format(\"console\")\n    #     .option(\"truncate\", False)\n    #     .start()\n    # )\n    # gr = parsed.agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n    # query =  (gr.writeStream\n    #     .outputMode(\"update\")\n    #     .format(\"console\")\n    #     .option(\"truncate\", False)\n    #     .start()\n    # )\n    gr = (parsed.withWatermark(\"timestamp\", \"5 seconds\")\n    .groupBy(window(\"timestamp\", \"10 seconds\", \"7 seconds\"))\n    .agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n         )\n    query =  (gr.writeStream\n        .outputMode(\"complete\")\n        .format(\"console\")\n        .option(\"truncate\", False)\n        .start()\n    )\n    query.awaitTermination()\n\nOverwriting app.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Apache Spark z plikami RDD"
    ]
  },
  {
    "objectID": "labs/lab9.html",
    "href": "labs/lab9.html",
    "title": "Przygotowanie danych",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nspark\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.4.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n# create dataframe \nprev = spark.read.csv(\"data/block*.csv\")\nprev\nprev.show(2)\nprev.show()\n# dodatkowe opcje z header i wartoÅ›ci null \nparsed = spark.read.option(\"header\", \"true\")\\\n.option(\"nullValue\", \"?\")\\\n.option(\"inferSchema\", \"true\")\\\n.csv(\"data/block*.csv\")\nparsed.show(5)\nparsed.printSchema()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#inne-formaty",
    "href": "labs/lab9.html#inne-formaty",
    "title": "Przygotowanie danych",
    "section": "inne formaty",
    "text": "inne formaty\n\nparquet\norc\njson\njdbc\navro\nyrxy\nimage\nlibsvm\nbinary\nxml\n\n\nparsed.write.format(\"parquet\").save(\"data/block2.parquet\")\n\n\nt = spark.read.format(\"parquet\").load(\"data/block.parquet\")\n\n\nt.show(2)\n\n\nspark.stop()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#schematy-danych",
    "href": "labs/lab9.html#schematy-danych",
    "title": "Przygotowanie danych",
    "section": "schematy danych",
    "text": "schematy danych\n\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n\nschema = StructType([\n  StructField(\"Date\", StringType(), True),\n  StructField(\"Open\", DoubleType(), True),\n  StructField(\"High\", DoubleType(), True),\n  StructField(\"Low\", DoubleType(), True),\n  StructField(\"Close\", DoubleType(), True),\n  StructField(\"Volume\", IntegerType(), True),\n  StructField(\"Name\", StringType(), True)\n])\n\n\nddlSchemaStr = \"\"\"Date STRING, Open FLOAT, High FLOAT, \nLow FLOAT, Close FLOAT, Voulme INT, Name String \n\"\"\"\n\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.read.option(\"header\", True)\\\n.csv(\"data/stocks/AAPL_2006-01-01_to_2018-01-01.csv\", schema=ddlSchemaStr)\n\ndf.show(5)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#dane-niustrukturyzowane",
    "href": "labs/lab9.html#dane-niustrukturyzowane",
    "title": "Przygotowanie danych",
    "section": "dane niustrukturyzowane",
    "text": "dane niustrukturyzowane\n\n%%file test.json\n\n{\n \"id\": \"0001\",\n \"type\": \"donut\",\n \"name\": \"Cake\",\n \"ppu\": 0.55,\n \"batters\":\n  {\n   \"batter\":\n    [\n     { \"id\": \"1001\", \"type\": \"Regular\" },\n     { \"id\": \"1002\", \"type\": \"Chocolate\" },\n     { \"id\": \"1003\", \"type\": \"Blueberry\" }\n    ]\n  },\n \"topping\":\n  [\n   { \"id\": \"5001\", \"type\": \"None\" },\n   { \"id\": \"5002\", \"type\": \"Glazed\" },\n   { \"id\": \"5005\", \"type\": \"Sugar\" },\n   { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n   { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n   { \"id\": \"5003\", \"type\": \"Chocolate\" },\n   { \"id\": \"5004\", \"type\": \"Maple\" }\n  ]\n}\n\nOverwriting test.json\n\n\n\nrawDFjson = spark.read.json(\"test.json\", multiLine = \"true\")\n\n\nrawDFjson.printSchema()\n\nroot\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n\nsampleDF = rawDFjson.withColumnRenamed(\"id\", \"key\")\n\n\nsampleDF.printSchema()\n\nroot\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- key: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n\nbatDF = sampleDF.select(\"key\", \"batters.batter\")\nbatDF.printSchema()\n\nroot\n |-- key: string (nullable = true)\n |-- batter: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n\n\n\n\nbatDF.show(1, False)\n\n+----+-------------------------------------------------------+\n|key |batter                                                 |\n+----+-------------------------------------------------------+\n|0001|[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}]|\n+----+-------------------------------------------------------+\n\n\n\n\nfrom pyspark.sql.functions import explode\nbat2DF = batDF.select(\"key\", explode(\"batter\").alias(\"new_batter\"))\nbat2DF.show()\n\n+----+-----------------+\n| key|       new_batter|\n+----+-----------------+\n|0001|  {1001, Regular}|\n|0001|{1002, Chocolate}|\n|0001|{1003, Blueberry}|\n+----+-----------------+\n\n\n\n\nbat2DF.printSchema()\n\nroot\n |-- key: string (nullable = true)\n |-- new_batter: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- type: string (nullable = true)\n\n\n\n\nbat2DF.select(\"key\", \"new_batter.*\").show()\n\n+----+----+---------+\n| key|  id|     type|\n+----+----+---------+\n|0001|1001|  Regular|\n|0001|1002|Chocolate|\n|0001|1003|Blueberry|\n+----+----+---------+\n\n\n\n\nfinalBatDF = (sampleDF\n        .select(\"key\",  \nexplode(\"batters.batter\").alias(\"new_batter\"))\n        .select(\"key\", \"new_batter.*\")\n        .withColumnRenamed(\"id\", \"bat_id\")\n        .withColumnRenamed(\"type\", \"bat_type\"))\nfinalBatDF.show()\n\n\ntopDF = (sampleDF\n        .select(\"key\", explode(\"topping\").alias(\"new_topping\"))\n        .select(\"key\", \"new_topping.*\")\n        .withColumnRenamed(\"id\", \"top_id\")\n        .withColumnRenamed(\"type\", \"top_type\")\n        )\ntopDF.show(10, False)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#eksploracyjna-analiza-danych",
    "href": "labs/lab9.html#eksploracyjna-analiza-danych",
    "title": "Przygotowanie danych",
    "section": "Eksploracyjna Analiza Danych",
    "text": "Eksploracyjna Analiza Danych\n\n# zweryfikuj schemat danych\nparsed.printSchema()\n\n\n# sprawdz wartosci dla pierwszego rzedu\nparsed.first()\n\n\n# ile przypadkow \nparsed.count()\n\n\n# zapisz do pamieci na klastrze (1 maszyna) \nparsed.cache()\n\n\n# target \"is_match\" liczba zgodnych i niezgodnych rekordow\nfrom pyspark.sql.functions import col\n\nparsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n\n\n# inne agregaty agg\nfrom pyspark.sql.functions import avg, stddev, stddev_pop\n\nparsed.agg(avg(\"cmp_sex\"), stddev(\"cmp_sex\"), stddev_pop(\"cmp_sex\")).show()\n\n\n# polecenia sql - przypisanie nazwy dla silnika sql - tabela przejsciowa\nparsed.createOrReplaceTempView(\"dane\")\n\n\nspark.sql(\"\"\" SELECT is_match, COUNT(*) cnt FROM dane group by is_match order by cnt DESC\"\"\").show()\n\n\n# zbiorcze statystyki \nsummary = parsed.describe()\nsummary.show()\n\n\nsummary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n\n\nktÃ³ra zmienna lepiej opisze dane c1 czy c2\n\n\n# statystyki dla poszczegolnych klas\n\n# filtrowanie sql\nmatches = parsed.where(\"is_match = true\")\n# filtrowanie pyspark\nmisses = parsed.filter(col(\"is_match\") == False)\n\nmatch_summary = matches.describe()\nmiss_summary = misses.describe()\n\n\nmatch_summary.show()\n\n\nmiss_summary.show()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#tabele-przestawne-spark",
    "href": "labs/lab9.html#tabele-przestawne-spark",
    "title": "Przygotowanie danych",
    "section": "Tabele przestawne spark",
    "text": "Tabele przestawne spark\n\nsummary_p = summary.toPandas()\n\n\nsummary_p.head()\n\n\nsummary_p.shape\n\n\nsummary_p = summary_p.set_index('summary').transpose().reset_index()\nsummary_p = summary_p.rename(columns={'index':'field'})\nsummary_p = summary_p.rename_axis(None, axis=1)\n\n\nsummaryT = spark.createDataFrame(summary_p)\nsummaryT.show()\n\n\nsummaryT.printSchema() # czy dobre typy danych ?? \n\n\nfrom pyspark.sql.types import DoubleType\n\nfor c in summaryT.columns:\n    if c == 'field':\n        continue\n    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n\n\nsummaryT.printSchema() # teraz lepiej\n\n\n# wykonaj to samo dla tabel match i miss\n\n\ndef pivot_summary(desc):\n    desc_p = desc.toPandas()\n    desc_p = desc_p.set_index('summary').transpose().reset_index()\n    desc_p = desc_p.rename(columns={'index':'field'})\n    desc_p = desc_p.rename_axis(None, axis=1)\n    descT = spark.createDataFrame(desc_p)\n    for c in descT.columns:\n        if c == 'field':\n            continue\n        else:\n            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n    return descT\n\n\nmatch_summaryT = pivot_summary(match_summary)\nmiss_summaryT = pivot_summary(miss_summary)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab9.html#zÅ‚Ä…czenia",
    "href": "labs/lab9.html#zÅ‚Ä…czenia",
    "title": "Przygotowanie danych",
    "section": "zÅ‚Ä…czenia",
    "text": "zÅ‚Ä…czenia\n\nmatch_summaryT.createOrReplaceTempView(\"match_s\")\nmiss_summaryT.createOrReplaceTempView(\"miss_s\")\n\n\nspark.sql(\"\"\"\nSelect a.field, a.count + b.count total, a.mean - b.mean delta\nfrom match_s a inner join miss_s b on a.field = b.field \nwhere a.field not in (\"id_1\", \"id_2\")\norder by delta DESC, total DESC\n\"\"\").show()\n\n\ndo modelu : cmp_plz, cmp_by, cmp_bd, cmp_lname_c1, cmp_bm\n\n\n## score = suma zmiennych\nzmienne = ['cmp_plz','cmp_by','cmp_bd','cmp_lname_c1','cmp_bm']\nsuma = \" + \".join(zmienne)\n\n\nsuma\n\n\nfrom pyspark.sql.functions import expr\n\n\nscored = parsed.fillna(0, subset=zmienne)\\\n.withColumn('score', expr(suma))\\\n.select('score','is_match')\n\n\nscored.show()\n\n\n# ocena wartosci progowej\ndef crossTabs(scored, t):\n    return scored.selectExpr(f\"score &gt;= {t} as above\", \"is_match\")\\\n    .groupBy(\"above\").pivot(\"is_match\",(\"true\",\"false\"))\\\n    .count()\n\n\ncrossTabs(scored, 4.0).show()\n\n\ncrossTabs(scored, 2.0).show()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Przygotowanie danych"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "",
    "text": "W tym Ä‡wiczeniu nauczysz siÄ™, jak stworzyÄ‡ proste API w Flasku, uruchomiÄ‡ je, wysyÅ‚aÄ‡ do niego zapytania oraz wykorzystaÄ‡ model decyzyjny w oparciu o podstawowÄ… reguÅ‚Ä™ logicznÄ….",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-podstawowego-api",
    "href": "labs/lab2.html#tworzenie-podstawowego-api",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "1ï¸âƒ£ Tworzenie podstawowego API",
    "text": "1ï¸âƒ£ Tworzenie podstawowego API\nNajpierw utworzymy podstawowÄ… aplikacjÄ™ Flask.\n\nZapisanie kodu API do pliku\nW Jupyter Notebooku uÅ¼yj magicznej komendy %%file, aby zapisaÄ‡ kod podstawowej aplikacji flask do pliku app.py: Kod znajdziesz na cw1 Jako tekst do wyÅ›wietlenie strony gÅ‚Ã³wnej uÅ¼yj Witaj w moim API!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nWriting app.py\n\n\nTeraz uruchom API w terminalu, wpisujÄ…c:\npython app.py\nFlask uruchomi serwer lokalnie pod adresem http://127.0.0.1:5000/.\n\n\nSprawdzenie dziaÅ‚ania API\nW Jupyter Notebooku wykonaj zapytanie GET do strony gÅ‚Ã³wnej. Na podstawie pola status_code napisz wyraÅ¼enie warunkowe ktÃ³re dla status_code 200 wyÅ›wietli zawartoÅ›Ä‡ odpowiedzi (z pola content).\n\nimport requests\nresponse = pass # TWOJ KOD\n\nJeÅ›li wszystko dziaÅ‚a poprawnie, zobaczysz komunikat Witaj w moim API!.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#dodanie-nowej-podstrony",
    "href": "labs/lab2.html#dodanie-nowej-podstrony",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "2ï¸âƒ£ Dodanie nowej podstrony",
    "text": "2ï¸âƒ£ Dodanie nowej podstrony\nDodajmy nowÄ… podstronÄ™ mojastrona, ktÃ³ra zwrÃ³ci komunikat To jest moja strona!.\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nPonownie uruchom API i wykonaj zapytanie do strony \"http://127.0.0.1:5000/mojastrona\":\n\nresponse = pass # TWOJ KOD\n\nPowinieneÅ› zobaczyÄ‡: To jest moja strona!",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "href": "labs/lab2.html#automatyczne-uruchamianie-serwera-z-jupyter-notebook",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "3ï¸âƒ£ Automatyczne uruchamianie serwera z Jupyter Notebook",
    "text": "3ï¸âƒ£ Automatyczne uruchamianie serwera z Jupyter Notebook\nZamknij wczeÅ›niej uruchomiony serwer (Ctrl+C w terminalu) i uruchom go ponownie bezpoÅ›rednio z Jupyter Notebook, korzystajÄ…c z subprocess.Popen:\n\nimport subprocess\n# TWOJ KOD \nserver = pass\n\nPo testach zamknij serwer wykorzystujÄ…c metodÄ™ kill:\n\n# TWOJ KOD",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#obsÅ‚uga-parametrÃ³w-w-adresie-url",
    "href": "labs/lab2.html#obsÅ‚uga-parametrÃ³w-w-adresie-url",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "4ï¸âƒ£ ObsÅ‚uga parametrÃ³w w adresie URL",
    "text": "4ï¸âƒ£ ObsÅ‚uga parametrÃ³w w adresie URL\nDodajemy nowÄ… podstronÄ™ /hello, ktÃ³ra bÄ™dzie przyjmowaÄ‡ parametr name.\nEdytuj app.py, dodajÄ…c odpowiedni kod\n\n%%file app.py\n###\n# TWOJ KOD API \n###\n\nUruchom serwer i sprawdÅº dziaÅ‚anie API:\nres1 = requests.get(\"http://127.0.0.1:5000/hello\")\nprint(res1.content)  # Powinno zwrÃ³ciÄ‡ \"Hello!\"\n\nres2 = requests.get(\"http://127.0.0.1:5000/hello?name=Sebastian\")\nprint(res2.content)  # Powinno zwrÃ³ciÄ‡ \"Hello Sebastian!\"",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "href": "labs/lab2.html#tworzenie-api-z-prostym-modelem-ml",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "5ï¸âƒ£ Tworzenie API z prostym modelem ML",
    "text": "5ï¸âƒ£ Tworzenie API z prostym modelem ML\nStworzymy nowÄ… podstronÄ™ /api/v1.0/predict, ktÃ³ra przyjmuje dwie liczby i zwraca wynik reguÅ‚y decyzyjnej: - JeÅ›li suma dwÃ³ch liczb jest wiÄ™ksza niÅ¼ 5.8, zwraca 1. - W przeciwnym razie zwraca 0.\nSprawdÅº dziaÅ‚anie API:\nres = requests.get(\"http://127.0.0.1:5000/api/v1.0/predict?num1=3&num2=4\")\nprint(res.json())  # Powinno zwrÃ³ciÄ‡ {\"prediction\": 1, \"features\": {\"num1\": 3.0, \"num2\": 4.0}}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab2.html#podsumowanie",
    "href": "labs/lab2.html#podsumowanie",
    "title": "Tworzenie API we Flasku â€“ Wprowadzenie",
    "section": "Podsumowanie",
    "text": "Podsumowanie\nPo wykonaniu tego Ä‡wiczenia studenci bÄ™dÄ… umieli:\n\nâœ… TworzyÄ‡ podstawowe API w Flasku.\n\nâœ… DodawaÄ‡ podstrony i obsÅ‚ugiwaÄ‡ parametry URL.\n\nâœ… WysyÅ‚aÄ‡ zapytania GET i analizowaÄ‡ odpowiedzi.\n\nâœ… Automatycznie uruchamiaÄ‡ serwer z Jupyter Notebook.\n\nâœ… ImplementowaÄ‡ prosty model decyzyjny w API.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Tworzenie API we Flasku â€“ Wprowadzenie"
    ]
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "dane nieustruktyryzowane",
    "section": "",
    "text": "Dane nieustrukturyzowane to dane, ktÃ³re nie sÄ… w Å¼aden sposÃ³b uporzÄ…dkowane, takie jak:\nNiezaleÅ¼nie od typu, wszystko przetwarzamy w tensorach (macierzach wielowymiarowych). To moÅ¼e prowadziÄ‡ do chÄ™ci wykorzystania modeli ML i sieci neuronowych do analizy danych nieustrukturyzowanych.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\n\n# 2-dim picture 28 x 28 pixel\npicture_2d = np.random.uniform(size=(28,28))\npicture_2d[0:5,0:5]\n\narray([[0.51209426, 0.14663302, 0.22121714, 0.05310908, 0.75023721],\n       [0.32680599, 0.1005447 , 0.55604465, 0.09762113, 0.94599531],\n       [0.03287616, 0.60521745, 0.19558811, 0.7921698 , 0.097081  ],\n       [0.98409001, 0.87207277, 0.67522807, 0.71541535, 0.75485835],\n       [0.02429088, 0.06016476, 0.89526054, 0.87395299, 0.8126181 ]])\nplt.imshow(picture_2d, interpolation='nearest')\nplt.show()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#pytorch---pretrenowane-modele-klasyfikujÄ…ce",
    "href": "labs/lab4.html#pytorch---pretrenowane-modele-klasyfikujÄ…ce",
    "title": "dane nieustruktyryzowane",
    "section": "PyTorch - pretrenowane modele klasyfikujÄ…ce",
    "text": "PyTorch - pretrenowane modele klasyfikujÄ…ce\n\nimport urllib.request\nurl = 'https://pytorch.tips/coffee'\nfpath = 'coffee.jpg'\n# pobierz na dysk\nurllib.request.urlretrieve(url, fpath)\n\n('coffee.jpg', &lt;http.client.HTTPMessage at 0x145c46750&gt;)\n\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image # pillow library\n\n\nimg = Image.open('coffee.jpg')\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nimport torch\nfrom torchvision import transforms\n\nOdrobinÄ™ zmienimy wÅ‚asnoÅ›ci obrazka\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize( \n    mean = [0.485, 0.456, 0.406],\n    std = [0.229, 0.224,0.225])\n])\n\n\nimg_tensor = transform(img)\n\nSprawdzmy rozmiary\n\ntype(img_tensor), img_tensor.shape\n\n(torch.Tensor, torch.Size([3, 224, 224]))\n\n\n\n# utworzenie batch size - dodatkowego wymiaru (na inne obrazki)\nbatch = img_tensor.unsqueeze(0)\nbatch.shape\n\ntorch.Size([1, 3, 224, 224])\n\n\n\nfrom torchvision import models\n\nmodels.list_models()[:5]\n\n['alexnet',\n 'convnext_base',\n 'convnext_large',\n 'convnext_small',\n 'convnext_tiny']",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#model-alexnet",
    "href": "labs/lab4.html#model-alexnet",
    "title": "dane nieustruktyryzowane",
    "section": "Model alexnet",
    "text": "Model alexnet\n\nalexnet = models.alexnet(pretrained=True)\n\n/Users/seba/Documents/GitHub/RTA_2025/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/seba/Documents/GitHub/RTA_2025/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/seba/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233M/233M [00:03&lt;00:00, 67.9MB/s] \n\n\n\nalexnet\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\nalexnet.eval()\npredict = alexnet(batch)\n\n\n_, idx = torch.max(predict,1)\n\n\nprint(idx)\n\ntensor([967])\n\n\n\nurl = 'https://pytorch.tips/imagenet-labels'\nfpath = 'imagenet_class_labels.txt'\nurllib.request.urlretrieve(url, fpath)\n\n('imagenet_class_labels.txt', &lt;http.client.HTTPMessage at 0x30aab0490&gt;)\n\n\n\nwith open('imagenet_class_labels.txt') as f:\n    classes = [line.strip() for line in f.readlines()]\n\n\nclasses[0:5]\n\n[\"{0: 'tench, Tinca tinca',\",\n \"1: 'goldfish, Carassius auratus',\",\n \"2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\",\n \"3: 'tiger shark, Galeocerdo cuvieri',\",\n \"4: 'hammerhead, hammerhead shark',\"]\n\n\n\nprob = torch.nn.functional.softmax(predict, dim=1)[0] *100\nprob[:10]\n\ntensor([2.5403e-09, 1.5528e-07, 1.2023e-08, 1.0434e-09, 2.9924e-07, 3.6093e-08,\n        8.3350e-10, 1.4222e-11, 1.0724e-10, 1.2831e-10],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nclasses[idx.item()], prob[idx.item()].item()\n\n(\"967: 'espresso',\", 87.99551391601562)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#model-resnet",
    "href": "labs/lab4.html#model-resnet",
    "title": "dane nieustruktyryzowane",
    "section": "Model resnet",
    "text": "Model resnet\n\nresnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n\nDownloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /Users/seba/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:02&lt;00:00, 73.7MB/s] \n\n\n\n# resnet\n\n\nresnet.eval()\nout = resnet(batch)\n\n\n_, index = torch.max(out,1)\nprob = torch.nn.functional.softmax(out, dim=1)[0] *100\n\n\nclasses[index.item()], prob[index.item()].item()\n\n(\"967: 'espresso',\", 49.123924255371094)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#wÅ‚asny-model-dla-danych-graficznych",
    "href": "labs/lab4.html#wÅ‚asny-model-dla-danych-graficznych",
    "title": "dane nieustruktyryzowane",
    "section": "wÅ‚asny model dla danych graficznych",
    "text": "wÅ‚asny model dla danych graficznych\nZobaczmy jak sieci neuronowe dziaÅ‚ajÄ… na danych graficznych.\n\n# 60000 obrazow 28x28\n\n# Loading the Fashion-MNIST dataset\nfrom torchvision import datasets, transforms\n# transformacja i normalizacja danych \ntransform = transforms.Compose([transforms.ToTensor(),\n  transforms.Normalize((0.5,), (0.5,))\n])\n\n# Download and load the training data\ntrainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\ntestset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:00&lt;00:00, 48.0MB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00&lt;00:00, 928kB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:00&lt;00:00, 16.5MB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00&lt;00:00, 12.1MB/s]\n\n\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n\nindexes = np.random.randint(0, images.shape[0], size=25)\nimages_rand = images[indexes]\nplt.figure(figsize=(5,5))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    image = images_rand[i]\n    plt.imshow(image[0])\n    plt.axis('off')\n\nplt.show()\nplt.close('all')\n\n\n\n\n\n\n\n\nPrzykÅ‚adowy model sieci nueronowej (bez konwolucji) - czy sÄ…dzisz, Å¼e to dobre rozwiÄ…zanie?\n\n# Define the network architecture\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nmodel = nn.Sequential(nn.Linear(784, 128),\n                      nn.ReLU(),\n                      nn.Linear(128, 10),\n                      nn.LogSoftmax(dim = 1)\n                     )\n\n# Define the loss\ncriterion = nn.NLLLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr = 0.002)\n\n# Define the epochs\nepochs = 30\n\ntrain_losses, test_losses = [], []\n\nfor e in range(epochs):\n  running_loss = 0\n  for images, labels in trainloader:\n    # Flatten Fashion-MNIST images into a 784 long vector\n    images = images.view(images.shape[0], -1)\n    \n    # Training pass\n    optimizer.zero_grad()\n    \n    output = model.forward(images)\n    loss = criterion(output, labels)\n    loss.backward()\n    optimizer.step()\n    \n    running_loss += loss.item()\n  else:\n    test_loss = 0\n    accuracy = 0\n    \n    # Turn off gradients for validation, saves memory and computation\n    with torch.no_grad():\n      # Set the model to evaluation mode\n      model.eval()\n      \n      # Validation pass\n      for images, labels in testloader:\n        images = images.view(images.shape[0], -1)\n        log_ps = model(images)\n        test_loss += criterion(log_ps, labels)\n        \n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim = 1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy += torch.mean(equals.type(torch.FloatTensor))\n    \n    model.train()\n    train_losses.append(running_loss/len(trainloader))\n    test_losses.append(test_loss/len(testloader))\n    \n    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n\nEpoch: 1/30.. Training loss: 0.491.. Test loss: 0.417.. Test Accuracy: 0.849\nEpoch: 2/30.. Training loss: 0.381.. Test loss: 0.464.. Test Accuracy: 0.832\nEpoch: 3/30.. Training loss: 0.347.. Test loss: 0.400.. Test Accuracy: 0.854\nEpoch: 4/30.. Training loss: 0.321.. Test loss: 0.391.. Test Accuracy: 0.857\nEpoch: 5/30.. Training loss: 0.304.. Test loss: 0.409.. Test Accuracy: 0.856\nEpoch: 6/30.. Training loss: 0.293.. Test loss: 0.393.. Test Accuracy: 0.861\nEpoch: 7/30.. Training loss: 0.279.. Test loss: 0.386.. Test Accuracy: 0.865\nEpoch: 8/30.. Training loss: 0.270.. Test loss: 0.362.. Test Accuracy: 0.879\nEpoch: 9/30.. Training loss: 0.260.. Test loss: 0.388.. Test Accuracy: 0.868\nEpoch: 10/30.. Training loss: 0.253.. Test loss: 0.376.. Test Accuracy: 0.877\nEpoch: 11/30.. Training loss: 0.246.. Test loss: 0.367.. Test Accuracy: 0.879\nEpoch: 12/30.. Training loss: 0.237.. Test loss: 0.368.. Test Accuracy: 0.880\nEpoch: 13/30.. Training loss: 0.235.. Test loss: 0.374.. Test Accuracy: 0.877\nEpoch: 14/30.. Training loss: 0.224.. Test loss: 0.367.. Test Accuracy: 0.877\nEpoch: 15/30.. Training loss: 0.218.. Test loss: 0.420.. Test Accuracy: 0.865\nEpoch: 16/30.. Training loss: 0.215.. Test loss: 0.390.. Test Accuracy: 0.874\nEpoch: 17/30.. Training loss: 0.208.. Test loss: 0.395.. Test Accuracy: 0.876\nEpoch: 18/30.. Training loss: 0.204.. Test loss: 0.392.. Test Accuracy: 0.880\nEpoch: 19/30.. Training loss: 0.196.. Test loss: 0.417.. Test Accuracy: 0.878\nEpoch: 20/30.. Training loss: 0.196.. Test loss: 0.409.. Test Accuracy: 0.878\nEpoch: 21/30.. Training loss: 0.192.. Test loss: 0.390.. Test Accuracy: 0.880\nEpoch: 22/30.. Training loss: 0.188.. Test loss: 0.412.. Test Accuracy: 0.885\nEpoch: 23/30.. Training loss: 0.183.. Test loss: 0.406.. Test Accuracy: 0.883\nEpoch: 24/30.. Training loss: 0.178.. Test loss: 0.402.. Test Accuracy: 0.884\nEpoch: 25/30.. Training loss: 0.175.. Test loss: 0.427.. Test Accuracy: 0.878\nEpoch: 26/30.. Training loss: 0.173.. Test loss: 0.450.. Test Accuracy: 0.882\nEpoch: 27/30.. Training loss: 0.165.. Test loss: 0.444.. Test Accuracy: 0.882\nEpoch: 28/30.. Training loss: 0.165.. Test loss: 0.442.. Test Accuracy: 0.881\nEpoch: 29/30.. Training loss: 0.160.. Test loss: 0.459.. Test Accuracy: 0.876\nEpoch: 30/30.. Training loss: 0.163.. Test loss: 0.449.. Test Accuracy: 0.882\n\n\n\nplt.plot(train_losses, label = \"Training loss\")\nplt.plot(test_losses, label = \"Validation loss\")\nplt.legend(frameon = False)\n\n\n\n\n\n\n\n\n\nprint(\"My model: \\n\\n\", model, \"\\n\")\nprint(\"The state dict keys: \\n\\n\", model.state_dict().keys())\n\nMy model: \n\n Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=10, bias=True)\n  (3): LogSoftmax(dim=1)\n) \n\nThe state dict keys: \n\n odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n\n\n\ntorch.save(model.state_dict(), 'checkpoint.pth')\n\nA jakie inne sieci i warstwy moÅ¼emy wykorzystaÄ‡ do analizy danych nieustrukturyzowanych?\n\nZnajdÅº odpowiedÅº na to pytanie w dokumentacji biblioteki PyTorch.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#dane-tekstowe-i-model-worka-sÅ‚Ã³w",
    "href": "labs/lab4.html#dane-tekstowe-i-model-worka-sÅ‚Ã³w",
    "title": "dane nieustruktyryzowane",
    "section": "Dane tekstowe i model Worka sÅ‚Ã³w",
    "text": "Dane tekstowe i model Worka sÅ‚Ã³w\n\nimport pandas as pd\ndf_train = pd.read_csv(\"train.csv\")\ndf_train = df_train.drop(\"index\", axis=1)\nprint(df_train.head())\nprint(np.bincount(df_train[\"label\"]))\n\n                                                text  label\n0  When we started watching this series on cable,...      1\n1  Steve Biko was a black activist who tried to r...      1\n2  My short comment for this flick is go pick it ...      1\n3  As a serious horror fan, I get that certain ma...      0\n4  Robert Cummings, Laraine Day and Jean Muir sta...      1\n[17452 17548]\n\n\n\n# BoW model  - wektoryzator z sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(lowercase=True, max_features=10_000, stop_words=\"english\")\n\ncv.fit(df_train[\"text\"])\n\nCountVectorizer(max_features=10000, stop_words='english')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(max_features=10000, stop_words='english') \n\n\n\n# sÅ‚ownik i nasze zmienne ..\nprint(list(cv.vocabulary_.keys())[:10])\nprint(list(cv.vocabulary_.values())[:10])\n\n['started', 'watching', 'series', 'cable', 'idea', 'hate', 'character', 'hold', 'beautifully', 'developed']\n[8515, 9725, 7957, 1320, 4488, 4191, 1544, 4339, 892, 2574]\n\n\n\nX_train = cv.transform(df_train[\"text\"])\n\n\n# to dense matrix\nfeat_vec = np.array(X_train[0].todense())[0]\nprint(feat_vec.shape)\nnp.bincount(feat_vec)\n\n(10000,)\n\n\narray([9926,   67,    5,    0,    1,    0,    1])",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab4.html#obiektowe-podejÅ›cie-do-modelowania",
    "href": "labs/lab4.html#obiektowe-podejÅ›cie-do-modelowania",
    "title": "dane nieustruktyryzowane",
    "section": "Obiektowe podejÅ›cie do modelowania",
    "text": "Obiektowe podejÅ›cie do modelowania\n\nimport pandas as pd\nimport numpy as np\n \n# przykÅ‚ad danych ustrukturyzowanych\ndf = pd.read_csv(\"students.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nsex\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\ntarget\n\n\n\n\n0\nfemale\ngroup B\nbachelor's degree\nstandard\nnone\n72\n72\n74\n0\n\n\n1\nfemale\ngroup C\nsome college\nstandard\ncompleted\n69\n90\n88\n1\n\n\n2\nfemale\ngroup B\nmaster's degree\nstandard\nnone\n90\n95\n93\n0\n\n\n3\nmale\ngroup A\nassociate's degree\nfree/reduced\nnone\n47\n57\n44\n1\n\n\n4\nmale\ngroup C\nsome college\nstandard\nnone\n76\n78\n75\n0\n\n\n\n\n\n\n\n\nlen(df), list(df.columns)\n\n(99,\n ['sex',\n  'race/ethnicity',\n  'parental level of education',\n  'lunch',\n  'test preparation course',\n  'math score',\n  'reading score',\n  'writing score',\n  'target'])\n\n\n\nX = df.drop(columns=['target'])\ny = df['target']\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# ZAMIAST OD RAZU PRZETWARZAC !!! najpierw przygotuj kroki - pipeline\n\nnumeric_features = ['math score','reading score','writing score']\ncategorical_features = ['sex','race/ethnicity','parental level of education','lunch','test preparation course']\n\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num_trans\", numeric_transformer, numeric_features),\n    (\"cat_trans\", categorical_transformer, categorical_features)\n])\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    (\"preproc\", preprocessor),\n    (\"model\", LogisticRegression())\n])\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\npipeline\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('model', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('model', LogisticRegression())]) preproc: ColumnTransformer?Documentation for preproc: ColumnTransformerColumnTransformer(transformers=[('num_trans',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['math score', 'reading score',\n                                  'writing score']),\n                                ('cat_trans',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex', 'race/ethnicity',\n                                  'parental level of education', 'lunch',\n                                  'test preparation course'])]) num_trans['math score', 'reading score', 'writing score'] SimpleImputer?Documentation for SimpleImputerSimpleImputer() StandardScaler?Documentation for StandardScalerStandardScaler() cat_trans['sex', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\nPAMIETAJ - obiekt pipeline to obiekt pythonowy i tak jak obiekt modelu moÅ¼na go zapisaÄ‡ do pickla.\n\n\nfrom sklearn.model_selection import train_test_split\nX_tr, X_test, y_tr, y_test = train_test_split(X,y,\ntest_size=0.2, random_state=42)\n\npipeline.fit(X_tr, y_tr)\n\nscore = pipeline.score(X_test, y_test)\nprint(score)\n\n0.45\n\n\n\nimport joblib\njoblib.dump(pipeline, 'your_pipeline.pkl')\n\n['your_pipeline.pkl']\n\n\nTU ZACZYNA SIÄ˜ MAGIA OBIEKTOWEGO PYTHONA - nie pisz kodu i nie uruchamiaj kodÃ³w wiele razy dla rÃ³Å¼nych parametrÃ³w - niech Python zrobi to za Ciebie\n\nparam_grid = [\n              {\"preproc__num_trans__imputer__strategy\":\n              [\"mean\",\"median\"],\n               \"model__n_estimators\":[2,5,10,100,500],\n               \"model__min_samples_leaf\": [1, 0.1],\n               \"model\":[RandomForestClassifier()]},\n              {\"preproc__num_trans__imputer__strategy\":\n                [\"mean\",\"median\"],\n               \"model__C\":[0.1,1.0,10.0,100.0,1000],\n                \"model\":[LogisticRegression()]}\n]\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ngrid_search = GridSearchCV(pipeline, param_grid,\ncv=2, verbose=1, n_jobs=-1)\n\n\ngrid_search.fit(X_tr, y_tr)\n\ngrid_search.best_params_\n\nFitting 2 folds for each of 30 candidates, totalling 60 fits\n\n\n{'model': RandomForestClassifier(),\n 'model__min_samples_leaf': 0.1,\n 'model__n_estimators': 500,\n 'preproc__num_trans__imputer__strategy': 'median'}\n\n\n\ngrid_search.score(X_test, y_test), grid_search.score(X_tr, y_tr)\n\n(0.5, 0.7341772151898734)\n\n\nTeraz drobna modyfikacja - wiemy, Å¼e takiej zmiennej nie chcemy do modelu - ma tylko jednÄ… wartoÅ›Ä‡. Ale jak zweryfikowaÄ‡ jakie to zmienne jeÅ›li masz 3 mln kolumn?\n\ndf['bad_feature'] = 1\n\n\nX = df.drop(columns=['target'])\ny = df['target']\nX_tr, X_test, y_tr, y_test = train_test_split(X,y,\ntest_size=0.2, random_state=42)\n\n\nnumeric_features = ['math score','reading score','writing score', 'bad_feature']\n# znajdz sposÃ³b na automatyczny podziaÅ‚ dla zmiennych numerycznych i nienumerycznych\n\n\ngrid_search = GridSearchCV(pipeline, param_grid,\ncv=2, verbose=1, n_jobs=-1)\n\ngrid_search.fit(X_tr, y_tr)\n\ngrid_search.best_params_\n\nFitting 2 folds for each of 30 candidates, totalling 60 fits\n\n\n{'model': RandomForestClassifier(),\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 2,\n 'preproc__num_trans__imputer__strategy': 'median'}\n\n\n\ngrid_search.score(X_tr, y_tr), grid_search.score(X_test, y_test)\n\n(0.8227848101265823, 0.7)\n\n\n\nNAPISZ WÅASNÄ„ KLASÄ˜ KTÃ“RA ZREALIZUJE TRNSFORMACJE ZA CIEBIE\n\n# your own transformator class\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DelOneValueFeature(BaseEstimator, TransformerMixin):\n    \"\"\"Description\"\"\"\n    def __init__(self):\n        self.one_value_features = []\n        \n    def fit(self, X, y=None):\n        for feature in X.columns:\n            unique = X[feature].unique()\n            if len(unique)==1:\n                self.one_value_features.append(feature)\n        return self\n    def transform(self, X, y=None):\n        if not self.one_value_features:\n            return X\n        return X.drop(axis='columns', columns=self.one_value_features)\n\n\n# UTWÃ“RZ NOWY PIPELINE\npipeline2 = Pipeline([\n    (\"moja_transformacja\",DelOneValueFeature()),\n    (\"preprocesser\", preprocessor),\n    (\"classifier\", LogisticRegression())])\n    \npipeline2.fit(X_tr, y_tr)\nscore2 = pipeline2.score(X_test, y_test)\nprint(pipeline2)\n\nPipeline(steps=[('moja_transformacja', DelOneValueFeature()),\n                ('preprocesser',\n                 ColumnTransformer(transformers=[('num_trans',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['math score',\n                                                   'reading score',\n                                                   'writing score']),\n                                                 ('cat_trans',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex', 'race/ethnicity',\n                                                   'parental level of '\n                                                   'education',\n                                                   'lunch',\n                                                   'test preparation '\n                                                   'course'])])),\n                ('classifier', LogisticRegression())])\n\n\nğŸ” Analiza zawartoÅ›ci laboratorium 4\n\nWprowadzenie do danych nieustrukturyzowanych\n\nLaboratorium rozpoczyna siÄ™ od przedstawienia danych nieustrukturyzowanych, takich jak obrazy, teksty, dÅºwiÄ™ki czy wideo. PodkreÅ›lono, Å¼e niezaleÅ¼nie od typu danych, wszystko przetwarzane jest w tensorach (macierzach wielowymiarowych), co umoÅ¼liwia wykorzystanie modeli uczenia maszynowego i sieci neuronowych do ich analizy.\n\nPraca z obrazami\n\nÄ†wiczenia pokazujÄ…, jak za pomocÄ… bibliotek NumPy i Matplotlib generowaÄ‡ i wizualizowaÄ‡ obrazy dwuwymiarowe. NastÄ™pnie wprowadzono bibliotekÄ™ PyTorch i jej moduÅ‚ torchvision do przetwarzania obrazÃ³w, w tym: â€¢ Pobieranie i wczytywanie obrazÃ³w z internetu â€¢ Transformacje obrazÃ³w (zmiana rozmiaru, przycinanie, normalizacja) â€¢ Konwersja obrazÃ³w do tensorÃ³w â€¢ Tworzenie batchy danych\n\nWykorzystanie pretrenowanych modeli\n\nLaboratorium demonstruje, jak zaÅ‚adowaÄ‡ i wykorzystaÄ‡ pretrenowany model AlexNet z biblioteki torchvision.models do klasyfikacji obrazÃ³w. Pokazano rÃ³wnieÅ¼, jak przygotowaÄ‡ dane wejÅ›ciowe i uzyskaÄ‡ predykcje z modelu.\n\nPraca z danymi tekstowymi\n\nW dalszej czÄ™Å›ci laboratorium wprowadzono analizÄ™ danych tekstowych za pomocÄ… modelu worka sÅ‚Ã³w (bag-of-words). Pokazano, jak przeksztaÅ‚ciÄ‡ teksty na reprezentacje numeryczne, ktÃ³re mogÄ… byÄ‡ wykorzystane w modelach uczenia maszynowego.\n\nObiektowe podejÅ›cie do modelowania\n\nNa koniec laboratorium przedstawiono obiektowe podejÅ›cie do tworzenia modeli w PyTorch, co jest istotne przy budowie bardziej zÅ‚oÅ¼onych architektur sieci neuronowych.\nâ¸»\nğŸ’¡ Propozycje rozszerzeÅ„\nAby jeszcze bardziej wzbogaciÄ‡ laboratorium 4, moÅ¼na rozwaÅ¼yÄ‡ dodanie nastÄ™pujÄ…cych elementÃ³w:\n\nWykorzystanie innych pretrenowanych modeli\n\nDodanie przykÅ‚adÃ³w z wykorzystaniem innych pretrenowanych modeli, takich jak ResNet czy VGG, pozwoliÅ‚oby studentom porÃ³wnaÄ‡ rÃ³Å¼ne architektury i ich zastosowania.\n\nFinaÅ‚owy projekt integrujÄ…cy obrazy i tekst\n\nZaproponowanie projektu, w ktÃ³rym studenci Å‚Ä…czÄ… analizÄ™ obrazÃ³w i tekstÃ³w (np. klasyfikacja memÃ³w), umoÅ¼liwiÅ‚oby praktyczne zastosowanie zdobytej wiedzy.\n\nWprowadzenie do transfer learningu\n\nPokazanie, jak dostosowaÄ‡ pretrenowane modele do nowych zadaÅ„ poprzez transfer learning, przygotowaÅ‚oby studentÃ³w do pracy z ograniczonymi zbiorami danych.\n\nAnaliza danych dÅºwiÄ™kowych\n\nDodanie sekcji dotyczÄ…cej analizy danych dÅºwiÄ™kowych (np. rozpoznawanie mowy) rozszerzyÅ‚oby zakres omawianych danych nieustrukturyzowanych.\n\nWykorzystanie bibliotek NLP\n\nWprowadzenie bibliotek takich jak spaCy czy Hugging Face Transformers do analizy tekstu pozwoliÅ‚oby na bardziej zaawansowane przetwarzanie jÄ™zyka naturalnego.\nâ¸»\nâœ… Podsumowanie\nLaboratorium 4 stanowi solidne wprowadzenie do analizy danych nieustrukturyzowanych z wykorzystaniem bibliotek NumPy i PyTorch. Dodanie powyÅ¼szych rozszerzeÅ„ mogÅ‚oby jeszcze bardziej zwiÄ™kszyÄ‡ wartoÅ›Ä‡ edukacyjnÄ… zajÄ™Ä‡, przygotowujÄ…c studentÃ³w do realnych wyzwaÅ„ w pracy z rÃ³Å¼norodnymi danymi w kontekÅ›cie Big Data.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "dane nieustruktyryzowane"
    ]
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentÃ³w z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacjÄ… w Dockerze.\nNauczysz siÄ™:\n\nTworzenia prostego REST API,\nObsÅ‚ugi zapytaÅ„ HTTP i obsÅ‚ugi bÅ‚Ä™dÃ³w w API,\nTestowania API z wykorzystaniem pytest,\nPrzenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#cel",
    "href": "labs/lab1.html#cel",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Celem laboratorium jest zapoznanie studentÃ³w z tworzeniem aplikacji REST API w Pythonie z wykorzystaniem biblioteki Flask oraz jej konteneryzacjÄ… w Dockerze.\nNauczysz siÄ™:\n\nTworzenia prostego REST API,\nObsÅ‚ugi zapytaÅ„ HTTP i obsÅ‚ugi bÅ‚Ä™dÃ³w w API,\nTestowania API z wykorzystaniem pytest,\nPrzenoszenia aplikacji do kontenera Docker.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "href": "labs/lab1.html#tworzenie-aplikacji-rest-api",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "1. Tworzenie aplikacji REST API",
    "text": "1. Tworzenie aplikacji REST API\nNaszym zadaniem jest wystawienie aplikacji w Pythonie, ktÃ³ra na Å¼Ä…danie klienta udzieli odpowiedzi na podstawie predykcji wygenerowanej przez model.\nAplikacjÄ™ napiszemy w Pythonie z wykorzystaniem Flask 3.0.3.\n\nKod minimalnej aplikacji Flask\nNaszÄ… aplikacjÄ™ chcemy uruchomiÄ‡ lokalnie, a nastÄ™pnie w prosty sposÃ³b przenieÅ›Ä‡ i uruchomiÄ‡ na dowolnym komputerze. Dlatego naturalnym rozwiÄ…zaniem jest zapisanie kodu w pliku z rozszerzeniem .py.\nAby automatycznie zapisaÄ‡ kod aplikacji do pliku app.py, wykorzystamy magicznÄ… komendÄ™ %%file plik.py.\n\n%%file app.py\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\n\nif __name__ == '__main__':\n    app.run()\n\nWriting app.py\n\n\n\nUwaga! W dokumentacji Flask w kodzie podstawowej aplikacji nie wystÄ™pujÄ… dwie ostatnie linie odpowiedzialne za uruchomienie serwera.\n\nif __name__ == '__main__':\n    app.run()\nWyjaÅ›nijmy co zawiera przykÅ‚adowy kod.\n\nfrom flask import Flask ZaÅ‚adowanie biblioteki\napp = Flask(__name__) utworzenie interfejsu serwera API\nkod podstrony z wykorzystaniem dekoratora\n\n@app.route('/')\ndef home():\n    return jsonify({\"message\": \"Hello, World!\"})\nDekoratory w Pythonie pozwalajÄ… modyfikowaÄ‡ zachowanie funkcji bez zmiany jej kodu. Flask wykorzystuje dekoratory do tworzenia tras (@app.route), ale moÅ¼na je takÅ¼e stosowaÄ‡ w analizie danych â€“ np. do logowania czasu wykonania funkcji lub obsÅ‚ugi bÅ‚Ä™dÃ³w.\n\nPrzykÅ‚ad: Normalizacja wartoÅ›ci w danych\nZaÅ‚Ã³Å¼my, Å¼e mamy funkcjÄ™, ktÃ³ra pobiera dane z pliku CSV i zwraca listÄ™ wartoÅ›ci. Dodamy dekorator, ktÃ³ry automatycznie przeskaluje dane do zakresu 0-1, co czÄ™sto jest wymagane przed analizÄ… statystycznÄ… lub trenowaniem modeli ML.\n\nimport numpy as np\n\n# Dekorator do normalizacji danych\ndef normalize_data(func):\n    def wrapper(*args, **kwargs):\n        data = func(*args, **kwargs)  # Pobranie oryginalnych danych\n        min_val, max_val = min(data), max(data)\n        normalized = [(x - min_val) / (max_val - min_val) for x in data]\n        print(\"Dane po normalizacji:\", normalized)\n        return normalized\n    return wrapper\n\n\n@normalize_data\ndef get_data():\n    return [10, 15, 20, 30, 50]\n\nget_data()\n\nDane po normalizacji: [0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n[0.0, 0.125, 0.25, 0.5, 1.0]\n\n\n\nÄ†wiczenie: â€Napisz dekorator, ktÃ³ry zaokrÄ…gla wartoÅ›ci do 2 miejsc po przecinku.â€\n\n\n\n\nObsÅ‚uga bÅ‚Ä™dÃ³w w API\nDodajmy obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w, np. kiedy klient poda niepoprawne dane:\n@app.errorhandler(404)\ndef not_found(error):\n    return jsonify({\"error\": \"Not Found\"}), 404\n\n@app.errorhandler(400)\ndef bad_request(error):\n    return jsonify({\"error\": \"Bad Request\"}), 400",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "href": "labs/lab1.html#uruchomienie-serwera-lokalnie",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "Uruchomienie serwera lokalnie",
    "text": "Uruchomienie serwera lokalnie\nUruchomienie serwera moze odbyÄ‡ siÄ™ na przynajmniej na dwa sposoby.\n\nUruchomienie serwera przez terminal\nOtwÃ³rz termianal w lokalizacji gdzie znajduje siÄ™ plik aplikacji\npython app.py\nlub (jeÅ›li nie ma fragmentu app.run())\nflask run\nPowinna pojawiÄ‡ siÄ™ informacja podobna do ponizszej:\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\nW celu weryfikacji dziaÅ‚ania moÅ¼esz otworzyÄ‡ nowe okno terminalu wpisujÄ…c:\ncurl localhost:5000\n\\{\"message\":\"Hello, World!\"\\}\n\n\nUruchomienie serwera w notatniku\nBezpoÅ›renie uruchomienia kodu w notatniku spowoduje uruchomienie serwera i zatrzymanie jakiejkolwiek mozliwoÅ›ci realizacji kodu. Aby tego uniknÄ…Ä‡ mozesz wykorzystaÄ‡ bibliotekÄ™ subprocess.\n\nimport subprocess\np = subprocess.Popen([\"python\", \"app.py\"])\n\nJeÅ›li potrzebujemy zamknÄ…Ä‡ subprocess wykonaj:\n\np.kill()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#testowanie-api",
    "href": "labs/lab1.html#testowanie-api",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "2. Testowanie API",
    "text": "2. Testowanie API\nDo testowania API wykorzystamy pytest oraz bibliotekÄ™ requests. ### Instalacja pytest:\n\n!pip install pytest requests -q\n\n\n%%file test_app.py\nimport pytest\nimport requests\n\ndef test_home():\n    response = requests.get(\"http://127.0.0.1:5000/\")\n    assert response.status_code == 200\n    assert response.json()[\"message\"] == \"Hello, World!\"\n\nWriting test_app.py\n\n\n\n!pytest test_app.py\n\n============================= test session starts ==============================\nplatform linux -- Python 3.11.6, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/jovyan/notebooks\nplugins: anyio-4.0.0\ncollected 1 item                                                               \n\ntest_app.py .                                                            [100%]\n\n============================== 1 passed in 0.03s ===============================\n\n\n\n# wersja bez testu\nimport requests\nresponse = requests.get(\"http://127.0.0.1:5000/\")\nprint(response.json())\n\n{'message': 'Hello, World!'}",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab1.html#Å›rodowisko-python",
    "href": "labs/lab1.html#Å›rodowisko-python",
    "title": "Åšrodowisko produkcyjne z modelem ML",
    "section": "Åšrodowisko Python",
    "text": "Åšrodowisko Python\nAby uruchomiÄ‡ kod aplikacji app.py, potrzebujemy interpretera jÄ™zyka Python zainstalowanego na naszym komputerze. Jednak samo posiadanie interpretera nie jest wystarczajÄ…ce â€“ aby aplikacja dziaÅ‚aÅ‚a poprawnie, naleÅ¼y utworzyÄ‡ Å›rodowisko (najlepiej wirtualne), w ktÃ³rym bÄ™dÄ… dostÄ™pne wszystkie wymagane biblioteki, takie jak Flask.\n\nuwaga: wszystkie polecenia terminala dotyczyÄ‡ bÄ™dÄ… wersji linux/mac os\n\nW pierwszej kolejnoÅ›ci sprawdÅº czy dostÄ™pne sÄ… polecenia pozwalajÄ…ce realizowaÄ‡ kod pythonowy.\nwhich python\nwhich python3\nwhich pip \nwhich pip3\nWszystkie te polecenia powinny wskazyaÄ‡ na folder z domyÅ›lnym Å›rodowiskiem Pythona.\nWygeneruj i uruchom Å›rodowisko wirtualne lokalnie wpisujÄ…c w terminalu:\npython3 -m venv .venv\nsource .venv/bin/activate\n\nDobra praktyka: Å›rodowisko python to nic innego jak katalog. W naszej wersji to katalog ukryty o nazwie .venv. JeÅ›li skopiujesz ten katalog gdzie indziej przestanie peÅ‚niÄ‡ on swojÄ… funkcjÄ™ Å›rodowiska python. Dlatego jego odtworzenie nie polega na jego kopiowaniu. JeÅ›li TwÃ³j projekt jest powiÄ…zany ze Å›rodowiskiem kontroli wersji GIT zadbaj aby katalog Å›rodowiska nie byÅ‚ dodawany do repozytorium. Mozesz wykonaÄ‡ to dziaÅ‚anie dodajÄ…c odpowiedni wpis do pliki .gitignore\n\nPosiadajÄ…c utworzone nowe Å›rodowisko sprawdÅº jakie biblioteki siÄ™ w nim znajdujÄ….\npip list \n\nPackage    Version\n---------- -------\npip        23.2.1\nsetuptools 65.5.0\nMozemy ponownie sprawdziÄ‡ polecenia python i pip:\nwhich python\nwhich pip \nDomyÅ›lnie powinny pojawiÄ‡ siÄ™ biblioteki pip oraz setuptools.\nDoinstaluj bibliotekÄ™ flask.\npip install flask==3.0.3\npip list \nPackage      Version\n------------ -------\nblinker      1.7.0\nclick        8.1.7\nFlask        3.0.3\nitsdangerous 2.1.2\nJinja2       3.1.3\nMarkupSafe   2.1.5\npip          23.2.1\nsetuptools   65.5.0\nWerkzeug     3.0.2\nJak widaÄ‡ instalacja biblioteki flask wymusiÅ‚a doinstalowanie rÃ³wniez innych pakietÃ³w.\nJedynÄ… mozliwoÅ›ciÄ… przeniesienia Å›rodowiska python jest jego ponowna instalacja na nowej maszynie i instalacja wszystkich pakietÃ³w. Aby jednak nie instalowaÄ‡ kazdego pakietu osobno mozemy wykorzystaÄ‡ plik konfiguracyjny requirements.txt zawierajÄ…cy listÄ™ pakietÃ³w.\n\nPamiÄ™taj - kazdy pakiet powinien zawieraÄ‡ nr wersji pakietu. W innym przypadku moze okazaÄ‡ siÄ™, ze nowe werjse pakietÃ³w spowodujÄ… brak obsÅ‚ugi twojego kodu.\n\nAby utworzyÄ‡ plik konfiguracyjny uzyj polecenia w terminalu:\npip freeze &gt;&gt; requirements.txt\nTak wygenerowany plik mozesz uzywaÄ‡ na dowolnej maszynie do instalacji i odtworzenia potrzebnego Å›rodowiska wykonawczego python.\n\nDygresja. W momencie przygotowywania materiaÅ‚Ã³w Flask byÅ‚ w wersji 3.0.1 - dziÅ› juz realizowany jest w wersji 3.0.3. Zmiany nastÄ™pujÄ… szybciej niz siÄ™ wydaje. Instalacja pakietÃ³w z pliku odbywa siÄ™ z wykorzystaniem polecenia:\n\npip install -r requierements.txt\nMamy teraz dwa pliki: app.py, i requirements.txt. PrzenoszÄ…c je do dowolnego projektu na serwerach github jesteÅ›my w stanie uruchomiÄ‡ naszÄ… aplikacjÄ™ wszÄ™dzie tam gdzie dostÄ™pny bÄ™dzie interpreter python na ktÃ³rym mozemy utworzyÄ‡ nowe wirtualne Å›rodowisko i zainstalowaÄ‡ biblioteki z pliku requirements.txt.\nDo peÅ‚nej automatyzacji przydaÅ‚aby siÄ™ jeszcze mozliwoÅ›Ä‡ uruchomienia Å›rodowiska python na dowolnej maszynie.\nW tym celu utwÃ³rz plik Dockerfile:\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 5000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\"]\n\nPowyzszy plik pozwala w docker desktop uruchomiÄ‡ obraz wykorzystujÄ…cy podstawowy system operacyjny (tutaj linux) wraz z podstawowym Å›rodowiskiem python3.11.\nPonadto plik ten kopiuje potrzebne pliki (app.py, requirements.txt) na obraz dockera.\nPolecenie RUN pozwala uruchomiÄ‡ dowolne polecenie bash wewnÄ…trz obrazu dockera.\nPolecenie CMD pozwala uruchomiÄ‡ polecenie uruchamiajÄ…ce serwer w trybie tak by nie zamknÄ…Ä‡ tego polecenia.\nOstatniÄ… informacjÄ… jest ustalenie portu na 8000.\nutworzenie kontenera na podstawie pliku Dockerfile\ndocker build -t modelML .\nuruchomienie kontenera\ndocker run -p 8000:8000 modelML",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Python",
      "Åšrodowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "labs/lab7.html",
    "href": "labs/lab7.html",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "",
    "text": "W tym laboratorium zapoznasz sie z roznymi metodami zasilania danych strumieniowych w Apache Spark oraz zastosowaniem prostych transformacji, filtrowania i segmentacji klientow w czasie rzeczywistym.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#wprowadzenie",
    "href": "labs/lab7.html#wprowadzenie",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "",
    "text": "W tym laboratorium zapoznasz sie z roznymi metodami zasilania danych strumieniowych w Apache Spark oraz zastosowaniem prostych transformacji, filtrowania i segmentacji klientow w czasie rzeczywistym.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#pomocnicza-funkcja-do-wyswietlania-naszych-danych-strumieniowych",
    "href": "labs/lab7.html#pomocnicza-funkcja-do-wyswietlania-naszych-danych-strumieniowych",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ’¡ Pomocnicza funkcja do wyswietlania naszych danych strumieniowych",
    "text": "ğŸ’¡ Pomocnicza funkcja do wyswietlania naszych danych strumieniowych\n\nbatch_counter = {\"count\": 0}\n\ndef process_batch(df, batch_id):\n    batch_counter[\"count\"] += 1\n    print(f\"Batch ID: {batch_id}\")\n    df.show(truncate=False)\n    if batch_counter[\"count\"] % 5 == 0:\n        spark.stop()",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#rate-jako-ÅºrÃ³dÅ‚o-kontrolowanego-strumienia",
    "href": "labs/lab7.html#rate-jako-ÅºrÃ³dÅ‚o-kontrolowanego-strumienia",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ rate jako ÅºrÃ³dÅ‚o kontrolowanego strumienia",
    "text": "ğŸ”¹ rate jako ÅºrÃ³dÅ‚o kontrolowanego strumienia\n\nâœ… Zadanie 1\n\nPrzygotuj strumien danych z format('rate'), ustaw rowsPerSecond na 5.\nUtworz kolumne user_id: expr(\"concat('u', cast(rand()*100 as int))\")\nDodaj kolumne event_type: expr(\"case when rand() &gt; 0.7 then 'purchase' else 'view' end\")\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\n\nspark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\nrate_df = (spark....)\n\nevents = (rate_df....)\n    \nquery = (events.writeStream\n         .format(\"console\")\n         .foreachBatch(process_batch)\n         .start())",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#filtrowanie-danych-bez-agregacji-append-mode",
    "href": "labs/lab7.html#filtrowanie-danych-bez-agregacji-append-mode",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ Filtrowanie danych bez agregacji (append mode)",
    "text": "ğŸ”¹ Filtrowanie danych bez agregacji (append mode)\nZobacz jak dziaÅ‚a poniÅ¼szy kod i na jego podstawie:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"AppendExample\").getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\n# Å¹rÃ³dÅ‚o rate - generuje timestamp + value\nrate_df = (spark.readStream\n           .format(\"rate\")\n           .option(\"rowsPerSecond\", 5)\n           .load())\n\n# Filtracja bez potrzeby agregacji (bezstanowe przetwarzanie)\nfiltered = rate_df.filter(col(\"value\") % 2 == 0) \\\n                  .withColumn(\"info\", expr(\"concat('even:', value)\"))\n\n# outputMode = append â†’ pokazuje tylko nowe wiersze, bez stanu\nquery = (filtered.writeStream \n    .outputMode(\"append\") \n    .format(\"console\") \n    .option(\"truncate\", False) \n    .foreachBatch(process_batch)\n    .start()\n        )\n\nSkorzystaj z danych z poprzedniego zadania.\nWyfiltruj tylko purchase.\n\npurchases = events....\n\nquery = (purchases.writeStream\n         .format(\"console\")\n         .outputMode(\"append\")\n         .foreachBatch(process_batch)\n         .start())",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#ÅºrÃ³dÅ‚o-plikowe-json",
    "href": "labs/lab7.html#ÅºrÃ³dÅ‚o-plikowe-json",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ Å¹rÃ³dÅ‚o plikowe (JSON)",
    "text": "ğŸ”¹ Å¹rÃ³dÅ‚o plikowe (JSON)\n\nâœ… Generator danych:\n\n%%file generator.py\n# generator.py\nimport json, os, random, time\nfrom datetime import datetime, timedelta\n\noutput_dir = \"data/stream\"\nos.makedirs(output_dir, exist_ok=True)\n\nevent_types = [\"view\", \"cart\", \"purchase\"]\ncategories = [\"electronics\", \"books\", \"fashion\", \"home\", \"sports\"]\n\ndef generate_event():\n    return {\n        \"user_id\": f\"u{random.randint(1, 50)}\",\n        \"event_type\": random.choices(event_types, weights=[0.6, 0.25, 0.15])[0],\n        \"timestamp\": (datetime.utcnow() - timedelta(seconds=random.randint(0, 300))).isoformat(),\n        \"product_id\": f\"p{random.randint(100, 120)}\",\n        \"category\": random.choice(categories),\n        \"price\": round(random.uniform(10, 1000), 2)\n    }\n\n# Simulate file-based streaming\nwhile True:\n    batch = [generate_event() for _ in range(50)]\n    filename = f\"{output_dir}/events_{int(time.time())}.json\"\n    with open(filename, \"w\") as f:\n        for e in batch:\n            f.write(json.dumps(e) + \"\\n\")\n    print(f\"Wrote: {filename}\")\n    time.sleep(5)\n\n\n\nâœ… Schemat danych:\n{\n  \"user_id\": \"u123\",\n  \"event_type\": \"purchase\", // albo \"view\", \"cart\", \"click\"\n  \"timestamp\": \"2025-05-09T15:24:00Z\",\n  \"product_id\": \"p456\",\n  \"category\": \"electronics\",\n  \"price\": 299.99\n}\n\nUtwÃ³rz zmiennÄ… schema, ktÃ³ra zrealizuje schamat danych naszej ramki. Wykorzystaj StringType(), TimestampType(),DoubleType()\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nspark = SparkSession.builder.appName(\"RealTimeEcommerce\").getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\n# StringType(), TimestampType(), DoubleType()\n\nschema = ...\n\n\n\nâœ… Odczyt danych z katalogu:\n\nstream = (spark.readStream\n          .schema(schema)\n          .json(\"data/stream\"))\n\nquery = (stream.writeStream\n         .format(\"console\")\n         .foreachBatch(process_batch)\n         .start())",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#bezstanowe-zliczanie-zdarzen",
    "href": "labs/lab7.html#bezstanowe-zliczanie-zdarzen",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ Bezstanowe zliczanie zdarzen",
    "text": "ğŸ”¹ Bezstanowe zliczanie zdarzen\n\nPrzygotuj zmiennÄ… agg1 zliczajÄ…cÄ… zdarzenia naleÅ¼Ä…ce do danej grupy event_type.\n\n\nagg1 = (stream....)\n\n# pamietaj, Å¼e agregacje wymagajÄ… opcji complete\nquery = (agg1\n         .writeStream\n         .outputMode(\"complete\")\n         .format(\"console\")\n         .foreachBatch(process_batch)\n         .start()\n        )",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#agregacja-w-oknach-czasowych",
    "href": "labs/lab7.html#agregacja-w-oknach-czasowych",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ Agregacja w oknach czasowych",
    "text": "ğŸ”¹ Agregacja w oknach czasowych\nwithWatermark(\"timestamp\", \"1 minute\")\nğŸ’¡ Do czego sÅ‚uÅ¼y: Informuje Sparka, Å¼e dane przychodzÄ… z opÃ³Åºnieniem i naleÅ¼y je przetwarzaÄ‡ tylko do okreÅ›lonego limitu wstecz (tutaj: 1 minuta).\nğŸš¨ Dlaczego waÅ¼ne: Bez watermarku Spark trzymaÅ‚by w pamiÄ™ci wszystkie dane, by mÃ³c je jeszcze pogrupowaÄ‡. Watermark pozwala zwolniÄ‡ pamiÄ™Ä‡.\n\nPogrupuj typy zdarzen w thumbling window, w oknie co 5 minut\ndodaj watermark z ustawieniem na 1 minutÄ™.\n\n\nwindowed = (stream...)\n\nquery = (\n    windowed.writeStream\n    .outputMode(\"append\")\n    .foreachBatch(process_batch)\n    .format(\"console\")\n    .start()\n)\n\n\nZmieÅ„ thumbling window na sliding window z szerokoÅ›ciÄ… okna 5 minut i startem nowego okna co 1 minutÄ™.\n\n\nwindowed = (stream...)\n\nquery = (\n    windowed.writeStream\n    .outputMode(\"append\")\n    .foreachBatch(process_batch)\n    .format(\"console\")\n    .start()\n)",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#segmentacja-klientow",
    "href": "labs/lab7.html#segmentacja-klientow",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "ğŸ”¹ Segmentacja klientow",
    "text": "ğŸ”¹ Segmentacja klientow\nğŸ§© Logika segmentacji:\n\njeÅ›li byÅ‚ purchase â†’ â€œBuyerâ€\njeÅ›li byÅ‚ cart, ale nie purchase â†’ â€œCart abandonerâ€\njeÅ›li tylko view â†’ â€œLurkerâ€\n\ngroupBy(window(...), \"user_id\")\nğŸ’¡ Do czego sÅ‚uÅ¼y: Grupujemy dane per uÅ¼ytkownik w konkretnym przedziale czasu (oknie 5-minutowym).\nâ±ï¸ window(â€œtimestampâ€, â€œ5 minutesâ€): Funkcja okna czasowego â€“ kaÅ¼da grupa bÄ™dzie dotyczyÄ‡ jednego uÅ¼ytkownika w konkretnym 5-minutowym interwale.\nagg(collect_set(\"event_type\"))\nğŸ’¡ Do czego sÅ‚uÅ¼y: Zbiera wszystkie typy zdarzeÅ„ (view, cart, purchase) danego uÅ¼ytkownika w danym oknie.\nğŸ§  Dlaczego collect_set a nie collect_list?: collect_set usuwa duplikaty â€” interesuje nas tylko czy coÅ› siÄ™ zdarzyÅ‚o, a nie ile razy.\n\nwithColumn(â€¦ expr(â€¦))\n\nğŸ’¡ Do czego sÅ‚uÅ¼y: Na podstawie zbioru zdarzeÅ„ okreÅ›lamy segment uÅ¼ytkownika.\nğŸ” array_contains: Funkcja sprawdzajÄ…ca, czy dany typ zdarzenia znajduje siÄ™ w tablicy.\nğŸ§  Co warto wiedzieÄ‡:\n\nSegmentacja to klasyczne zastosowanie agregacji i transformacji strumienia.\nÅÄ…czenie window + watermark jest kluczowe do kontroli stanu.\ncollect_set umoÅ¼liwia prostÄ… analizÄ™ zachowaÅ„, bez potrzeby przechowywania surowych danych.\nexpr() daje elastycznoÅ›Ä‡, by uÅ¼ywaÄ‡ skÅ‚adni SQL wewnÄ…trz kodu DataFrame.",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "labs/lab7.html#porownanie-trybow-outputmode",
    "href": "labs/lab7.html#porownanie-trybow-outputmode",
    "title": "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow",
    "section": "âš™ï¸ Porownanie trybow outputMode",
    "text": "âš™ï¸ Porownanie trybow outputMode\n\n\n\n\n\n\n\n\n\noutputMode\nOpis\nKiedy uzywac\nWymagania\n\n\n\n\nappend\nWypisywane sa tylko nowe wiersze\nFiltrowanie, wzbogacanie bez agregacji\nNie dziala z groupBy\n\n\nupdate\nWypisywane sa tylko zmienione wiersze\nAgregacje z watermarkami\nWymaga zarzadzania stanem\n\n\ncomplete\nWypisywane jest calosciowe podsumowanie\nPodsumowania okien, snapshoty\nMoze byc kosztowne",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Spark",
      "Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow"
    ]
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJÄ™zyk prowadzenia: polski\nPoziom przedmiotu: Å›rednio-zaawansowany\nProwadzÄ…cy: Sebastian ZajÄ…c, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2025/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nWspÃ³Å‚czesny biznes opiera siÄ™ na podejmowaniu decyzji opartych na danych. Coraz wiÄ™ksza iloÅ›Ä‡ informacji, rosnÄ…ce wymagania rynku oraz potrzeba natychmiastowej reakcji sprawiajÄ…, Å¼e analiza danych w czasie rzeczywistym staje siÄ™ kluczowym elementem nowoczesnych procesÃ³w biznesowych.\nNa zajÄ™ciach studenci zapoznajÄ… siÄ™ z metodami i technologiami umoÅ¼liwiajÄ…cymi przetwarzanie danych w czasie rzeczywistym. SzczegÃ³lnÄ… uwagÄ™ poÅ›wiÄ™cimy zastosowaniu uczenia maszynowego (machine learning), sztucznej inteligencji (artificial intelligence) oraz gÅ‚Ä™bokich sieci neuronowych (deep learning) w analizie danych. Zrozumienie tych metod pozwala nie tylko lepiej interpretowaÄ‡ zjawiska biznesowe, ale takÅ¼e podejmowaÄ‡ szybkie i trafne decyzje.\nW ramach kursu omÃ³wimy zarÃ³wno dane ustrukturyzowane, jak i nieustrukturyzowane (obrazy, dÅºwiÄ™k, strumieniowanie wideo). Studenci poznajÄ… architektury przetwarzania danych, takie jak lambda i kappa, wykorzystywane w systemach data lake, a takÅ¼e wyzwania zwiÄ…zane z modelowaniem danych w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\nKurs obejmuje czÄ™Å›Ä‡ teoretycznÄ… oraz praktyczne laboratoria, podczas ktÃ³rych studenci bÄ™dÄ… pracowaÄ‡ z rzeczywistymi danymi w Å›rodowiskach takich jak: JupyterLab, PyTorch, Apache Spark, Apache Kafka. DziÄ™ki temu studenci nie tylko zdobÄ™dÄ… wiedzÄ™ na temat metod analitycznych, ale takÅ¼e nauczÄ… siÄ™ korzystaÄ‡ z najnowszych technologii informatycznych stosowanych w analizie danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Sylabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nModelowanie, uczenie i predykcja w trybie wsadowym (offline learning) i przyrostowym (online learning). Problemy przyrostowego uczenia maszynowego.\nModele przetwarzania danych w Big Data. Od plikoÌw pÅ‚askich do Data Lake. Mity i fakty przetwarzania danych w czasie rzeczywistym. (WykÅ‚ad)\nSystemy NRT (near real-time systems), pozyskiwanie danych, streaming, analityka.\nAlgorytmy estymacji parametroÌw modelu w trybie przyrostowym. Stochastic Gradient Descent.\nArchitektura Lambda i Kappa. Zaprojektowanie architektury IT dla przetwarzania danych w czasie rzeczywistym.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrukturyzowane i niestrukturyzowane dane. Relacyjne bazy danych i bazy NoSQL\nAgregacje i raportowanie w bazach NoSQL (na przykÅ‚adzie bazy Cassandra).\nPodstawy obiektowego programowania w Pythonie w analizie regresji liniowej, logistycznej oraz sieci neuronowych z wykorzystaniem biblioteki sklearn, TensorFLow i Keras\nArchitektura IT przetwarzania Big Data. Przygotowanie wirtualnego sÌrodowiska dla Sparka. Pierwszy program w PySpark. Wykorzystanie przygotowanego sÌrodowiska do analizy danych z serwisu Twitter.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 1.\nAnaliza 1 Detekcja wyÅ‚udzenÌ w zgÅ‚oszeniach szkoÌd samochodowych w czasie rzeczywistym z wykorzystaniem przygotowanego, darmowego sÌrodowiska. Cz 2.\nPrzygotowanie sÌrodowiska Microsoft Azure. Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 1.\nAnaliza 2 Detekcja anomalii i wartosÌci odstajaÌ¨cych w logowanych zdarzeniach sieci Ethernet cz 2. Inne narzeÌ¨dzia IT do szybkiej analizy logoÌw.\nNarzeÌ¨dzia SAS do strumieniowego przetwarzania danych"
  },
  {
    "objectID": "sylabus.html#efekty-ksztaÅ‚cenia",
    "href": "sylabus.html#efekty-ksztaÅ‚cenia",
    "title": "Sylabus",
    "section": "Efekty ksztaÅ‚cenia",
    "text": "Efekty ksztaÅ‚cenia\n\nWiedza:\n\n\nZna historieÌ¨ i filozofieÌ¨ modeli przetwarzania danych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nZna mozÌ‡liwosÌci i obszary zastosowania procesowania danych w czasie rzeczywistym\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08\nMetody weryfikacji: egzamin pisemny (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ egzaminacyjnych\n\nZna teoretyczne aspekty struktury lambda i kappa\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08\nMetody weryfikacji: kolokwium pisemne (pytania otwarte, zadania)\nMetody dokumentacji: wykaz pytanÌ z kolokwium\n\nUmie wybracÌ struktureÌ¨ IT dla danego problemu biznesowego\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo kroÌtkim czasie\n\nPowiaÌ¨zania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmiejÄ™tnoÅ›ci:\n\n\nRozroÌzÌ‡nia typy danych strukturyzowanych jak i niestrukturyzowanych\n\nPowiaÌ¨zania: K2A_U02, K2A_U07, K2A_U10, O2_U02\nMetody weryfikacji: test\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ, przetwarzacÌ oraz zachowywacÌ dane generowane w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nRozumie ograniczenia wynikajaÌ¨ce z czasu przetwarzania przez urzaÌ¨dzenia oraz systemy informatyczne\n\nPowiaÌ¨zania: K2A_U01, K2A_U07, K2A_U11, O2_U02\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie zastosowacÌ i skonstruowacÌ system do przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUmie przygotowacÌ raportowanie dla systemu przetwarzania w czasie rzeczywistym\n\nPowiaÌ¨zania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nKompetencje:\n\n\nFormuÅ‚uje problem analityczny wraz z jego informatycznym rozwiaÌ¨zaniem\n\nPowiaÌ¨zania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07\nMetody weryfikacji: projekt, prezentacja\nMetody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\nUtrwala umiejeÌ¨tnosÌcÌ samodzielnego uzupeÅ‚niania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\n\nPowiaÌ¨zania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06\nMetody weryfikacji: projekt\nMetody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 20%\nZadania 40%\nProjekt 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n1ï¸âƒ£ ZajÄ…c S. (red.), Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzÄ™dzia informatyczne i biznesowe, SGH, Warszawa 2022.\n2ï¸âƒ£ FrÄ…tczak E. (red.), Modelowanie dla biznesu: Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring, SGH, Warszawa 2019.\n3ï¸âƒ£ Bellemare A., MikrousÅ‚ugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na duÅ¼Ä… skalÄ™, Oâ€™Reilly 2021.\n4ï¸âƒ£ Shapira G., Palino T., Sivaram R., Petty K., Kafka: The Definitive Guide. Real-time data and stream processing at scale, Oâ€™Reilly 2022.\n5ï¸âƒ£ Lakshmanan V., Robinson S., Munn M., Wzorce projektowe uczenia maszynowego. RozwiÄ…zania typowych problemÃ³w dotyczÄ…cych przygotowania danych, konstruowania modeli i MLOps, Oâ€™Reilly 2021.\n6ï¸âƒ£ Gift N., Deza A., Practical MLOps: Operationalizing Machine Learning Models, Oâ€™Reilly 2022.\n7ï¸âƒ£ Tiark Rompf, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, Oâ€™Reilly 2018.\n8ï¸âƒ£ SebastiÃ¡n RamÃ­rez, FastAPI: Modern Web APIs with Python, Manning (w przygotowaniu, aktualnie dostÄ™pna online).\n9ï¸âƒ£ Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer 2017.\nğŸ”Ÿ Anirudh Koul, Siddha Ganju, Meher Kasam, Practical Deep Learning for Cloud, Mobile & Edge, Oâ€™Reilly 2019."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr: 2024/2025 Uczelnia: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje o kursie znajdziesz w sylabusie.\nPolecane materiaÅ‚y znajdziesz na liÅ›cie ksiÄ…Å¼ek.\nMateriaÅ‚y z wykÅ‚adu i laboratoriÃ³w nie sÄ… wspierane przez Google. ObecnoÅ›Ä‡ na wykÅ‚adach i Ä‡wiczeniach nie zmniejszy Twoich 5 dolarÃ³w.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w Auli VI bud G\n\n18-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 1\n25-02-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 2\n04-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 3 online\n11-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 4\n18-03-2025 (wtorek) 13:30-15:10 - WykÅ‚ad 5\n\nWykÅ‚ad 5 koÅ„czy siÄ™ TESTEM: 20 pytaÅ„ - 30 minut. Test przeprowadzany jest za poÅ›rednictwem MS Teams.\n\n\nLaboratoria\n\nLab1\n24-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n25-03-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab2\n31-03-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n01-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab3\n07-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n08-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab4\n14-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n15-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab5\n28-04-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n29-04-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab6\n05-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n06-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab7\n12-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n13-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab8\n19-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n20-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab9\n26-05-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n27-05-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\nLab10\n02-06-2025 (poniedziaÅ‚ek) 08:00-15:10 - G-235 grupy 11, 12, 13, 14\n03-06-2025 (wtorek) 11:40-15:10 - W-60 grupy 15, 16\n\n\n\nZaliczenie i Egzamin\nWykÅ‚ady zakoÅ„czÄ… siÄ™ testem (podczas ostatnich zajÄ™Ä‡).\nAby zaliczyÄ‡ test, naleÅ¼y zdobyÄ‡ wiÄ™cej niÅ¼ 13 punktÃ³w â€“ jest to warunek konieczny do uczestnictwa w Ä‡wiczeniach.\nLaboratoria\nPodczas laboratoriÃ³w bÄ™dÄ… zadawane prace domowe, ktÃ³re naleÅ¼y przesyÅ‚aÄ‡ za poÅ›rednictwem MS Teams. KaÅ¼dy brak pracy domowej obniÅ¼a koÅ„cowÄ… ocenÄ™ o 0,5 stopnia.\n\nProjekt\nProjekty naleÅ¼y realizowaÄ‡ w grupach maksymalnie 5-osobowych.\nWymagania projektu\n\nProjekt powinien rozwiÄ…zywaÄ‡ realny problem biznesowy, ktÃ³ry moÅ¼na opracowaÄ‡ przy uÅ¼yciu danych przetwarzanych w trybie online. (Nie wyklucza to uÅ¼ycia przetwarzania wsadowego, np. do generowania modelu).\nDane powinny byÄ‡ przesyÅ‚ane do Apache Kafka, skÄ…d bÄ™dÄ… poddawane dalszemu przetwarzaniu i analizie.\nMoÅ¼na uÅ¼ywaÄ‡ dowolnego jÄ™zyka programowania w kaÅ¼dym komponencie projektu.\nMoÅ¼na wykorzystaÄ‡ narzÄ™dzia BI.\nÅ¹rÃ³dÅ‚em danych moÅ¼e byÄ‡ dowolne API, sztucznie generowane dane, IoT itp.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Kafka\nDatabricks Community edition Web page.",
    "crumbs": [
      "222890-S",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html",
    "href": "lectures/wyklad2.html",
    "title": "WykÅ‚ad 2",
    "section": "",
    "text": "â³ Czas trwania: 1,5h ğŸ¯ Cel wykÅ‚adu\nzrozumienie, jak dane ewoluowaÅ‚y w rÃ³Å¼nych branÅ¼ach i jakie narzÄ™dzia sÄ… dziÅ› wykorzystywane do ich analizy.\nNa tym wykÅ‚adzie przedstawimy ewolucjÄ™ analizy danych, pokazujÄ…c, jak zmieniaÅ‚y siÄ™ technologie i podejÅ›cia do przetwarzania danych na przestrzeni lat. Rozpoczniemy od klasycznych struktur tabelarycznych, przez bardziej zaawansowane modele grafowe i tekstowe, aÅ¼ po nowoczesne podejÅ›cie do strumieniowego przetwarzania danych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "href": "lectures/wyklad2.html#dane-tabelaryczne-tabele-sql",
    "title": "WykÅ‚ad 2",
    "section": "Dane tabelaryczne (tabele SQL)",
    "text": "Dane tabelaryczne (tabele SQL)\nPoczÄ…tkowo dane byÅ‚y przechowywane w postaci tabel, gdzie kaÅ¼da tabela zawieraÅ‚a zorganizowane informacje w kolumnach i wierszach (np. bazy danych SQL).\nModele takie doskonale nadawaÅ‚y siÄ™ do danych ustrukturyzowanych.\n\nğŸ“Œ Cechy:\nâœ… Dane podzielone na kolumny o staÅ‚ej strukturze.\nâœ… MoÅ¼liwoÅ›Ä‡ stosowania operacji CRUD (Create, Read, Update, Delete).\nâœ… ÅšcisÅ‚e reguÅ‚y spÃ³jnoÅ›ci i normalizacji.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Systemy bankowe, e-commerce, ERP, systemy CRM.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-grafowe",
    "href": "lectures/wyklad2.html#dane-grafowe",
    "title": "WykÅ‚ad 2",
    "section": "Dane grafowe",
    "text": "Dane grafowe\nWraz z rozwojem potrzeb biznesowych pojawiÅ‚y siÄ™ dane grafowe, w ktÃ³rych relacje miÄ™dzy obiektami sÄ… reprezentowane jako wierzchoÅ‚ki i krawÄ™dzie.\n\nğŸ“Œ Cechy:\nâœ… Dane opisujÄ…ce relacje i powiÄ…zania.\nâœ… Elastyczna struktura (grafy zamiast tabel).\nâœ… MoÅ¼liwoÅ›Ä‡ analizy poÅ‚Ä…czeÅ„ (np. algorytmy PageRank, centralnoÅ›Ä‡).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Sieci spoÅ‚ecznoÅ›ciowe (Facebook, LinkedIn), wyszukiwarki (Google), systemy rekomendacji (Netflix, Amazon).\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Graf Karate - NetworkX):\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "href": "lectures/wyklad2.html#dane-pÃ³Å‚strukturyzowane-json-xml-yaml",
    "title": "WykÅ‚ad 2",
    "section": "Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)",
    "text": "Dane pÃ³Å‚strukturyzowane (JSON, XML, YAML)\nDane te nie sÄ… w peÅ‚ni ustrukturyzowane jak w bazach SQL, ale majÄ… pewien schemat.\n\nğŸ“Œ Cechy:\nâœ… Hierarchiczna struktura (np. klucz-wartoÅ›Ä‡, obiekty zagnieÅ¼dÅ¼one).\nâœ… Brak Å›cisÅ‚ego schematu (moÅ¼liwoÅ›Ä‡ dodawania nowych pÃ³l).\nâœ… PopularnoÅ›Ä‡ w systemach NoSQL i API.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Dokumenty w MongoDB, pliki konfiguracyjne, REST API, pliki logÃ³w.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "href": "lectures/wyklad2.html#dane-tekstowe-nlp",
    "title": "WykÅ‚ad 2",
    "section": "Dane tekstowe (NLP)",
    "text": "Dane tekstowe (NLP)\nTekst staÅ‚ siÄ™ kluczowym ÅºrÃ³dÅ‚em informacji, szczegÃ³lnie w analizie opinii, chatbotach czy wyszukiwarkach.\n\nğŸ“Œ Cechy:\nâœ… Nieustrukturyzowane dane wymagajÄ…ce przeksztaÅ‚cenia.\nâœ… Stosowanie embeddingÃ³w (np. Word2Vec, BERT, GPT).\nâœ… DuÅ¼e zastosowanie w analizie sentymentu i chatbotach.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Media spoÅ‚ecznoÅ›ciowe, e-maile, chatboty, tÅ‚umaczenie maszynowe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie:\n\n\nCode\nimport ollama\n\n# PrzykÅ‚adowe zdanie\nsentence = \"Sztuczna inteligencja zmienia Å›wiat.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-1.6779385805130005, 3.0364203453063965, -6.6012187004089355, -1.7487436532974243]",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "href": "lectures/wyklad2.html#dane-multimedialne-obrazy-dÅºwiÄ™k-wideo",
    "title": "WykÅ‚ad 2",
    "section": "Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)",
    "text": "Dane multimedialne (obrazy, dÅºwiÄ™k, wideo)\nNowoczesne systemy analizy danych wykorzystujÄ… rÃ³wnieÅ¼ obrazy i dÅºwiÄ™k.\n\nğŸ“Œ Cechy:\nâœ… WymagajÄ… duÅ¼ej mocy obliczeniowej (sztuczna inteligencja, deep learning).\nâœ… Przetwarzane przez modele CNN (obrazy) i RNN/Transformers (dÅºwiÄ™k).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Rozpoznawanie twarzy, analiza mowy, biometria, analiza treÅ›ci wideo.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Obraz - OpenCV):\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-strumieniowe",
    "href": "lectures/wyklad2.html#dane-strumieniowe",
    "title": "WykÅ‚ad 2",
    "section": "Dane strumieniowe",
    "text": "Dane strumieniowe\nObecnie najbardziej dynamicznie rozwija siÄ™ analiza danych strumieniowych, gdzie dane sÄ… analizowane na bieÅ¼Ä…co, w miarÄ™ ich napÅ‚ywania.\n\nğŸ“Œ Cechy:\nâœ… Przetwarzanie w czasie rzeczywistym.\nâœ… Wykorzystanie technologii takich jak Apache Kafka, Flink, Spark Streaming.\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Transakcje bankowe (detekcja oszustw), analiza social media, IoT.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Strumieniowe transakcje bankowe):\n\n\nCode\nimport time\ntransactions = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': 200}]\nfor transaction in transactions:\n    print(f\"Processing transaction: {transaction}\")\n    time.sleep(1)\n\n\nProcessing transaction: {'id': 1, 'amount': 100}\nProcessing transaction: {'id': 2, 'amount': 200}",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "href": "lectures/wyklad2.html#dane-sensoryczne-i-iot",
    "title": "WykÅ‚ad 2",
    "section": "Dane sensoryczne i IoT",
    "text": "Dane sensoryczne i IoT\nDane z czujnikÃ³w i urzÄ…dzeÅ„ IoT sÄ… kolejnym krokiem w ewolucji.\n\nğŸ“Œ Cechy:\nâœ… CzÄ™sto pochodzÄ… z miliardÃ³w urzÄ…dzeÅ„ (big data).\nâœ… WymagajÄ… analizy brzegowej (edge computing).\n\n\nğŸ“Œ PrzykÅ‚ady:\nâ¡ï¸ Smart home, wearables, samochody autonomiczne, systemy przemysÅ‚owe.\n\n\nğŸ–¥ï¸ PrzykÅ‚adowy kod w Pythonie (Sensor - temperatura):\n\n\nCode\nimport random\ndef get_temperature():\n    return round(random.uniform(20.0, 25.0), 2)\nprint(f\"Current temperature: {get_temperature()}Â°C\")\n\n\nCurrent temperature: 21.52Â°C",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "href": "lectures/wyklad2.html#hadoop-map-reduce-skalowanie-obliczeÅ„-na-big-data",
    "title": "WykÅ‚ad 2",
    "section": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data",
    "text": "Hadoop Map-Reduce â€“ Skalowanie obliczeÅ„ na Big Data\nKiedy mÃ³wimy o skalowalnym przetwarzaniu danych, pierwszym skojarzeniem moÅ¼e byÄ‡ Google.\nAle co tak naprawdÄ™ sprawia, Å¼e moÅ¼emy wyszukiwaÄ‡ informacje w uÅ‚amku sekundy, przetwarzajÄ…c petabajty danych?\nğŸ‘‰ Czy wiesz, Å¼e nazwa â€œGoogleâ€ pochodzi od sÅ‚owa â€œGoogolâ€, czyli liczby rÃ³wnej 10Â¹â°â°?\nTo wiÄ™cej niÅ¼ liczba atomÃ³w w znanym WszechÅ›wiecie! ğŸŒŒ\n\nğŸ”¥ Wyzwanie: Czy uda Ci siÄ™ zapisaÄ‡ liczbÄ™ Googol do koÅ„ca zajÄ™Ä‡?",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "href": "lectures/wyklad2.html#dlaczego-sql-i-klasyczne-algorytmy-nie-wystarczajÄ…",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?",
    "text": "ğŸ” Dlaczego SQL i klasyczne algorytmy nie wystarczajÄ…?\nTradycyjne bazy danych SQL czy jednowÄ…tkowe algorytmy zawodzÄ…, gdy skala danych przekracza pojedynczy komputer.\nW tym miejscu pojawia siÄ™ MapReduce â€“ rewolucyjny model obliczeniowy stworzony przez Google.\n\nğŸ› ï¸ RozwiÄ…zania Google dla Big Data:\nâœ… Google File System (GFS) â€“ rozproszony system plikÃ³w.\nâœ… Bigtable â€“ system do przechowywania ogromnych iloÅ›ci ustrukturyzowanych danych.\nâœ… MapReduce â€“ algorytm podziaÅ‚u pracy na wiele maszyn.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "href": "lectures/wyklad2.html#graficzne-przedstawienie-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "Graficzne przedstawienie MapReduce",
    "text": "Graficzne przedstawienie MapReduce\n\nMapowanie rozdziela zadania (Map)\nKaÅ¼de wejÅ›cie dzielone jest na mniejsze czÄ™Å›ci i przetwarzane rÃ³wnolegle.\nğŸŒ WyobraÅº sobie, Å¼e masz ksiÄ…Å¼kÄ™ telefonicznÄ… i chcesz znaleÅºÄ‡ wszystkie osoby o nazwisku â€œNowakâ€.\nâ¡ï¸ Podziel ksiÄ…Å¼kÄ™ na fragmenty i daj kaÅ¼demu do przeanalizowania jeden fragment.\n\n\nRedukcja zbiera wyniki (Reduce)\nWszystkie czÄ™Å›ciowe wyniki sÄ… Å‚Ä…czone w jednÄ…, koÅ„cowÄ… odpowiedÅº.\nğŸ”„ Wszyscy uczniowie zgÅ‚aszajÄ… swoje wyniki, a jeden student zbiera i podsumowuje odpowiedÅº.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "href": "lectures/wyklad2.html#klasyczny-przykÅ‚ad-liczenie-sÅ‚Ã³w-w-tekÅ›cie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie",
    "text": "ğŸ’¡ Klasyczny przykÅ‚ad: Liczenie sÅ‚Ã³w w tekÅ›cie\nZaÅ‚Ã³Å¼my, Å¼e mamy miliony ksiÄ…Å¼ek i chcemy policzyÄ‡, ile razy wystÄ™puje kaÅ¼de sÅ‚owo.\n\nğŸ–¥ï¸ Kod MapReduce w Pythonie (z uÅ¼yciem multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Funkcja Map (podziaÅ‚ tekstu na sÅ‚owa)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Funkcja Reduce (sumowanie wynikÃ³w)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\nğŸ”¹ Co tu siÄ™ dzieje?\nâœ… KaÅ¼dy fragment tekstu jest przetwarzany niezaleÅ¼nie (map).\nâœ… Wyniki sÄ… zbierane i sumowane (reduce).\nâœ… Efekt: MoÅ¼emy przetwarzaÄ‡ terabajty tekstu rÃ³wnolegle!",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "href": "lectures/wyklad2.html#wizualizacja-porÃ³wnanie-klasycznego-podejÅ›cia-i-mapreduce",
    "title": "WykÅ‚ad 2",
    "section": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce",
    "text": "ğŸ¨ Wizualizacja â€“ PorÃ³wnanie klasycznego podejÅ›cia i MapReduce\nğŸ“Š Stare podejÅ›cie â€“ Jeden komputer wykonuje wszystko sekwencyjnie.\nğŸ“Š Nowe podejÅ›cie (MapReduce) â€“ KaÅ¼da maszyna liczy fragment i wyniki sÄ… agregowane.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "href": "lectures/wyklad2.html#wyzwanie-dla-ciebie",
    "title": "WykÅ‚ad 2",
    "section": "ğŸš€ Wyzwanie dla Ciebie!",
    "text": "ğŸš€ Wyzwanie dla Ciebie!\nğŸ”¹ ZnajdÅº i uruchom swÃ³j wÅ‚asny algorytm MapReduce w dowolnym jÄ™zyku!\nğŸ”¹ Czy potrafisz zaimplementowaÄ‡ wÅ‚asny MapReduce do innego zadania? (np. analiza logÃ³w, zliczanie klikniÄ™Ä‡ na stronie)",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#big-data",
    "href": "lectures/wyklad2.html#big-data",
    "title": "WykÅ‚ad 2",
    "section": "Big Data",
    "text": "Big Data\nSystemy Big data mogÄ… byÄ‡ czÄ™Å›ciÄ… (ÅºrÃ³dÅ‚em) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie sÄ… systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsÅ‚uÅ¼y do rÃ³Å¼norodnych celÃ³w opartych na danych (analityka, data science â€¦)\nponiÅ¼ej 100% accuracy\n\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.â€™â€™ â€” Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, â€¦ four V\n\nVolume (ObjÄ™toÅ›Ä‡) - rozmiar danych produkowanych na caÅ‚ym Å›wiecie przyrasta w tempie wykÅ‚adniczym.\nVelocity (SzybkoÅ›Ä‡) - tempo produkowania danych, szybkoÅ›ci ich przesyÅ‚ania i przetwarzania.\nVariety (ZrÃ³Å¼nicowanie) - tradycyjne dane kojarzÄ… siÄ™ nam z postaciÄ… alfanumerycznÄ… zÅ‚oÅ¼onÄ… z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dÅºwiÄ™ki, pliki wideo, strumienie danych z IoT\nVeracity (WiarygodnoÅ›Ä‡) - Czy dane sÄ… kompletne i poprawne? Czy obiektywnie odzwierciedlajÄ… rzeczywistoÅ›Ä‡? Czy sÄ… podstawÄ… do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, itâ€™s all about cost and benefits.\n\n\nCelem obliczeÅ„ nie sÄ… liczby, lecz ich zrozumienie R.W. Hamming 1962.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "href": "lectures/wyklad2.html#modele-przetwarzania-danych",
    "title": "WykÅ‚ad 2",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane sÄ… praktycznie od zawsze. W ciÄ…gu ostatnich dziesiÄ™cioleci iloÅ›Ä‡ przetwarzanych danych systematycznie roÅ›nie co wpÅ‚ywa na proces przygotowania i przetwarzania danych.\n\nTrochÄ™ historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : PoczÄ…tek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPÃ³Åºniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiÄ™kszoÅ›Ä‡ danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostÄ™p do danych sprowadza siÄ™ najczÄ™Å›ciej do realizacji zapytaÅ„ poprzez aplikacjÄ™.\nSposÃ³b wykorzystania i realizacji procesu dostÄ™pu do bazy danych nazywamy modelem przetwarzania. NajczÄ™Å›ciej uÅ¼ywane sÄ… dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Åšwietnie sprawdza siÄ™ w przypadku obsÅ‚ugi bieÅ¼Ä…cej np. obsÅ‚uga klienta, rejestr zamÃ³wieÅ„, obsÅ‚uga sprzedaÅ¼y itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiÄ…zaÅ„ m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostÄ™pu do danych,\nzarzÄ…dzania wspÃ³Å‚bieÅ¼noÅ›ciÄ…,\nprzetwarzania zdarzeÅ„ -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemÃ³w (np. dla wielu sklepÃ³w),\nraportowanie i podsumowania danych,\noptymalizacja zÅ‚oÅ¼onych zapytaÅ„,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziÅ‚y do sformuÅ‚owania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesÃ³w analizy i dostarczanie narzÄ™dzi umoÅ¼liwiajÄ…cych analizÄ™ wielowymiarowÄ… (czas, miejsce, produkt).\nProces zrzucania danych z rÃ³Å¼nych systemÃ³w do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatÃ³w (podsumowaÅ„) dotyczÄ…cych wymiarÃ³w hurtowni. Proces ten jest caÅ‚kowicie sterowany przez uÅ¼ytkownika.\nPrzykÅ‚ad\nZaÅ‚Ã³Å¼my, Å¼e mamy dostÄ™p do hurtowni danych gdzie przechowywane sÄ… informacje dotyczÄ…ce sprzedaÅ¼y produktÃ³w w supermarkecie. Jak przeanalizowaÄ‡ zapytania:\n\nJaka jest Å‚Ä…czna sprzedaÅ¼ produktÃ³w w kolejnych kwartaÅ‚ach, miesiÄ…cach, tygodniach ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na rodzaje produktÃ³w ?\nJaka jest sprzedaÅ¼ produktÃ³w z podziaÅ‚em na oddziaÅ‚y supermarketu ?\n\nOdpowiedzi na te pytania pozwalajÄ… okreÅ›liÄ‡ wÄ…skie gardÅ‚a sprzedaÅ¼y produktÃ³w przynoszÄ…cych deficyt, zaplanowaÄ‡ zapasy w magazynach czy porÃ³wnaÄ‡ sprzedaÅ¼ rÃ³Å¼nych grup w rÃ³Å¼nych oddziaÅ‚ach supermarketu.\nW ramach Hurtowni Danych najczÄ™Å›ciej wykonuje siÄ™ dwa rodzaje zapytaÅ„(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczajÄ…ce biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagajÄ…ce krytyczne decyzje biznesowe.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 2"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "WykÅ‚ad 1",
    "section": "",
    "text": "â³ Czas trwania: 1,5h\nğŸ¯ Cel wykÅ‚adu\nZapoznanie studentÃ³w z podstawami real-time analytics, rÃ³Å¼nicami miÄ™dzy trybami przetwarzania danych (batch, streaming, real-time) oraz kluczowymi zastosowaniami i wyzwaniami.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#czym-jest-analiza-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Czym jest analiza danych w czasie rzeczywistym?",
    "text": "Czym jest analiza danych w czasie rzeczywistym?\n\nDefinicja i kluczowe koncepcje\nAnaliza danych w czasie rzeczywistym (ang. Real-Time Data Analytics) to proces przetwarzania i analizy danych natychmiast po ich wygenerowaniu, bez koniecznoÅ›ci przechowywania i oczekiwania na pÃ³Åºniejsze przetworzenie. Celem jest uzyskanie natychmiastowych wnioskÃ³w i reakcji na zmieniajÄ…ce siÄ™ warunki w systemach biznesowych, technologicznych i naukowych.\n\n\nKluczowe cechy analizy danych w czasie rzeczywistym:\n\nNiska latencja (ang. low-latency) â€“ dane sÄ… analizowane w ciÄ…gu milisekund lub sekund od ich wygenerowania.\nStreaming vs.Â Batch Processing â€“ analiza danych moÅ¼e odbywaÄ‡ siÄ™ w sposÃ³b ciÄ…gÅ‚y (streaming) lub w z gÃ³ry okreÅ›lonych interwaÅ‚ach (batch).\nIntegracja z IoT, AI i ML â€“ real-time analytics czÄ™sto wspÃ³Å‚pracuje z Internetem Rzeczy (IoT) oraz algorytmami sztucznej inteligencji.\nPodejmowanie decyzji w czasie rzeczywistym â€“ np. natychmiastowa detekcja oszustw w transakcjach bankowych.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "href": "lectures/wyklad1.html#zastosowanie-analizy-danych-w-czasie-rzeczywistym-w-biznesie",
    "title": "WykÅ‚ad 1",
    "section": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie",
    "text": "Zastosowanie analizy danych w czasie rzeczywistym w biznesie\n\nFinanse i bankowoÅ›Ä‡\n\nWykrywanie oszustw â€“ analiza transakcji w czasie rzeczywistym pozwala na wykrycie anomalii wskazujÄ…cych na oszustwa.\nAutomatyczny trading â€“ systemy HFT (High-Frequency Trading) analizujÄ… miliony danych w uÅ‚amkach sekundy.\nDynamiczne oceny kredytowe â€“ natychmiastowa analiza ryzyka kredytowego klienta.\n\n\n\nE-commerce i marketing cyfrowy\n\nPersonalizacja ofert w czasie rzeczywistym â€“ dynamiczne rekomendacje produktÃ³w na podstawie aktualnego zachowania uÅ¼ytkownika.\nDynamiczne ceny â€“ np. Uber, Amazon i hotele stosujÄ… dynamiczne ustalanie cen na podstawie popytu.\nMonitorowanie mediÃ³w spoÅ‚ecznoÅ›ciowych â€“ analiza nastrojÃ³w klientÃ³w i natychmiastowa reakcja na negatywne komentarze.\n\n\n\nTelekomunikacja i IoT\n\nMonitorowanie infrastruktury sieciowej â€“ analiza logÃ³w w czasie rzeczywistym pozwala na wykrywanie awarii przed ich wystÄ…pieniem.\nSmart Cities â€“ analiza ruchu drogowego i natychmiastowa optymalizacja sygnalizacji Å›wietlnej.\nAnalityka IoT â€“ urzÄ…dzenia IoT generujÄ… strumienie danych, ktÃ³re moÅ¼na analizowaÄ‡ w czasie rzeczywistym (np. inteligentne liczniki energii).\n\n\n\nOchrona zdrowia\n\nMonitorowanie pacjentÃ³w â€“ analiza sygnaÅ‚Ã³w z urzÄ…dzeÅ„ medycznych w celu natychmiastowego wykrycia zagroÅ¼enia Å¼ycia.\nAnalityka epidemiologiczna â€“ Å›ledzenie rozprzestrzeniania siÄ™ chorÃ³b na podstawie danych w czasie rzeczywistym.\n\nAnaliza danych w czasie rzeczywistym to kluczowy element nowoczesnych systemÃ³w informatycznych, ktÃ³ry umoÅ¼liwia firmom podejmowanie decyzji szybciej i bardziej precyzyjnie. Jest wykorzystywana w wielu branÅ¼ach â€“ od finansÃ³w, przez e-commerce, aÅ¼ po ochronÄ™ zdrowia i IoT.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "href": "lectures/wyklad1.html#rÃ³Å¼nice-miÄ™dzy-batch-processing-near-real-time-analytics-real-time-analytics",
    "title": "WykÅ‚ad 1",
    "section": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics",
    "text": "RÃ³Å¼nice miÄ™dzy Batch Processing, Near Real-Time Analytics, Real-Time Analytics\nIstniejÄ… trzy gÅ‚Ã³wne podejÅ›cia do przetwarzania informacji:\n\nBatch Processing (Przetwarzanie wsadowe)\nNear Real-Time Analytics (Analiza niemal w czasie rzeczywistym)\nReal-Time Analytics (Analiza w czasie rzeczywistym)\n\nKaÅ¼de z nich rÃ³Å¼ni siÄ™ szybkoÅ›ciÄ… przetwarzania, wymaganiami technologicznymi oraz zastosowaniami biznesowymi.\n\nBatch Processing â€“ Przetwarzanie wsadowe\nğŸ“Œ Definicja:\nBatch Processing polega na zbieraniu duÅ¼ych iloÅ›ci danych i ich przetwarzaniu w okreÅ›lonych odstÄ™pach czasu (np. co godzinÄ™, codziennie, co tydzieÅ„).\nğŸ“Œ Cechy:\n\nâœ… Wysoka wydajnoÅ›Ä‡ dla duÅ¼ych zbiorÃ³w danych\nâœ… Przetwarzanie danych po ich zgromadzeniu\nâœ… Nie wymaga natychmiastowej analizy\nâœ… Zwykle taÅ„sze niÅ¼ przetwarzanie w czasie rzeczywistym\nâŒ OpÃ³Åºnienia â€“ wyniki sÄ… dostÄ™pne dopiero po zakoÅ„czeniu przetwarzania\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nGenerowanie raportÃ³w finansowych na koniec dnia/miesiÄ…ca\nAnaliza trendÃ³w sprzedaÅ¼y na podstawie historycznych danych\nTworzenie modeli uczenia maszynowego offline\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nHadoop MapReduce\nApache Spark (w trybie batch)\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiÄ…ca\n\n# Agregacja danych - miesiÄ™czne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wynikÃ³w do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nGdybyÅ› chciaÅ‚ utworzyÄ‡ dane do przykÅ‚adu\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)\n\n\nNear Real-Time Analytics â€“ Analiza niemal w czasie rzeczywistym\nğŸ“Œ Definicja:\nNear Real-Time Analytics to analiza danych, ktÃ³ra odbywa siÄ™ z minimalnym opÃ³Åºnieniem (zazwyczaj od kilku sekund do kilku minut). Jest stosowana tam, gdzie peÅ‚na analiza w czasie rzeczywistym nie jest konieczna, ale zbyt duÅ¼e opÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na biznes.\nğŸ“Œ Cechy:\n\nâœ… Przetwarzanie danych w krÃ³tkich odstÄ™pach czasu (kilka sekund â€“ minut)\nâœ… UmoÅ¼liwia szybkie podejmowanie decyzji, ale nie wymaga reakcji w milisekundach\nâœ… Optymalny balans miÄ™dzy kosztami a szybkoÅ›ciÄ…\nâŒ Nie nadaje siÄ™ do systemÃ³w wymagajÄ…cych natychmiastowej reakcji\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nMonitorowanie transakcji bankowych i wykrywanie oszustw (np. analiza w ciÄ…gu 30 sekund)\nDynamiczne dostosowywanie reklam online na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w\nAnaliza logÃ³w serwerÃ³w i sieci w celu wykrycia anomalii\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Kafka + Spark Streaming\nElasticsearch + Kibana (np. analiza logÃ³w IT)\nAmazon Kinesis\n\nPrzykÅ‚ad producenta danych realizujÄ…cego tranzakcje wysyÅ‚ane do systemu Apache Kafka.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generujÄ…ca przykÅ‚adowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota miÄ™dzy 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# ZakoÅ„czenie dziaÅ‚ania producenta\nproducer.flush()\nproducer.close()\nPrzykÅ‚ad consumenta - programu sparawdzajÄ…cego zbyt duÅ¼e transakcje\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"ğŸš¨ Wykryto duÅ¼Ä… transakcjÄ™: {transaction}\")\nPrzykÅ‚adowy zestaw danych\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\n\nReal-Time Analytics â€“ Analiza w czasie rzeczywistym\nğŸ“Œ Definicja:\nReal-Time Analytics to natychmiastowa analiza danych i podejmowanie decyzji w uÅ‚amku sekundy (milisekundy do jednej sekundy). Wykorzystywana w systemach wymagajÄ…cych reakcji w czasie rzeczywistym, np. w transakcjach gieÅ‚dowych, systemach IoT czy cyberbezpieczeÅ„stwie.\nğŸ“Œ Cechy:\n\nâœ… Bardzo niskie opÃ³Åºnienie (milliseconds-seconds)\nâœ… UmoÅ¼liwia natychmiastowÄ… reakcjÄ™ systemu\nâœ… Wymaga wysokiej mocy obliczeniowej i skalowalnej architektury\nâŒ DroÅ¼sze i bardziej zÅ‚oÅ¼one technologicznie niÅ¼ batch processing\n\nğŸ“Œ PrzykÅ‚ady zastosowaÅ„:\n\nHigh-Frequency Trading (HFT) â€“ analiza i podejmowanie decyzji w transakcjach gieÅ‚dowych w milisekundach\nAutonomiczne samochody â€“ analiza strumieni danych z kamer i sensorÃ³w w czasie rzeczywistym\nCyberbezpieczeÅ„stwo â€“ detekcja atakÃ³w w sieciach komputerowych w uÅ‚amku sekundy\nAnalityka IoT â€“ np. natychmiastowa detekcja anomalii w danych z czujnikÃ³w przemysÅ‚owych\n\nğŸ“Œ PrzykÅ‚adowe technologie:\n\nApache Flink\nApache Storm\nGoogle Dataflow\n\nğŸ” PorÃ³wnanie:\n\n\n\n\n\n\n\n\n\nCecha\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nOpÃ³Åºnienie\nMinuty â€“ godziny â€“ dni\nSekundy â€“ minuty\nMilisekundy â€“ sekundy\n\n\nTyp przetwarzania\nWsadowe (offline)\nStrumieniowe (ale nie w peÅ‚ni natychmiastowe)\nStrumieniowe (prawdziwy real-time)\n\n\nKoszt infrastruktury\nğŸ“‰ Niski\nğŸ“ˆ Åšredni\nğŸ“ˆğŸ“ˆ Wysoki\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ implementacji\nğŸ“‰ Prosta\nğŸ“ˆ Åšrednia\nğŸ“ˆğŸ“ˆ Trudna\n\n\nPrzykÅ‚ady zastosowaÅ„\nRaporty, ML offline, analizy historyczne\nMonitorowanie transakcji, dynamiczne reklamy\nHFT, IoT, detekcja oszustw w czasie rzeczywistym\n\n\n\nğŸ“Œ Kiedy stosowaÄ‡ Batch Processing?\n\nâœ… Gdy nie wymagasz natychmiastowej analizy\nâœ… Gdy masz duÅ¼e iloÅ›ci danych, ale przetwarzane sÄ… one okresowo\nâœ… Gdy chcesz obniÅ¼yÄ‡ koszty\n\nğŸ“Œ Kiedy stosowaÄ‡ Near Real-Time Analytics?\n\nâœ… Gdy wymagasz analizy w krÃ³tkim czasie (sekundy â€“ minuty)\nâœ… Gdy potrzebujesz bardziej aktualnych danych, ale nie w peÅ‚nym real-time\nâœ… Gdy szukasz kompromisu miÄ™dzy wydajnoÅ›ciÄ… a kosztami\n\nğŸ“Œ Kiedy stosowaÄ‡ Real-Time Analytics?\n\nâœ… Gdy kaÅ¼da milisekunda ma znaczenie (np. gieÅ‚da, autonomiczne pojazdy)\nâœ… Gdy chcesz wykrywaÄ‡ oszustwa, anomalie lub incydenty natychmiast\nâœ… Gdy system musi natychmiast reagowaÄ‡ na zdarzenia\n\nReal-time analytics nie zawsze jest konieczne â€“ w wielu przypadkach near real-time jest wystarczajÄ…ce i bardziej opÅ‚acalne. Kluczowe jest zrozumienie wymagaÅ„ biznesowych przed wyborem odpowiedniego rozwiÄ…zania.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "href": "lectures/wyklad1.html#dlaczego-real-time-analytics-jest-waÅ¼ne",
    "title": "WykÅ‚ad 1",
    "section": "Dlaczego Real-Time Analytics jest waÅ¼ne?",
    "text": "Dlaczego Real-Time Analytics jest waÅ¼ne?\nReal-time analytics (analiza danych w czasie rzeczywistym) staje siÄ™ coraz bardziej istotna w wielu branÅ¼ach, poniewaÅ¼ umoÅ¼liwia organizacjom podejmowanie natychmiastowych decyzji na podstawie aktualnych danych. Oto kilka kluczowych powodÃ³w, dla ktÃ³rych real-time analytics jest waÅ¼ne:\n\nSzybkie podejmowanie decyzji\nReal-time analytics pozwala firmom reagowaÄ‡ na zmiany i wydarzenia w czasie rzeczywistym. DziÄ™ki temu moÅ¼na podejmowaÄ‡ decyzje szybciej, co jest kluczowe w dynamicznych Å›rodowiskach, takich jak:\n\nMarketing: Reklamy mogÄ… byÄ‡ dostosowane do zachowaÅ„ uÅ¼ytkownikÃ³w w czasie rzeczywistym (np. personalizacja treÅ›ci reklamowych).\nFinanse: Wykrywanie oszustw w czasie rzeczywistym, gdzie kaÅ¼da minuta moÅ¼e oznaczaÄ‡ rÃ³Å¼nicÄ™ w prewencji strat finansowych.\n\n\n\nMonitorowanie w czasie rzeczywistym\nFirmy mogÄ… monitorowaÄ‡ kluczowe wskaÅºniki operacyjne na bieÅ¼Ä…co. PrzykÅ‚ady:\n\nIoT (Internet of Things): Monitorowanie stanu maszyn i urzÄ…dzeÅ„ w fabrykach, aby natychmiast wykrywaÄ‡ awarie i zapobiegaÄ‡ przestojom.\nHealthtech: Åšledzenie parametrÃ³w Å¼yciowych pacjentÃ³w i wykrywanie anomalii, co moÅ¼e ratowaÄ‡ Å¼ycie.\n\n\n\nZwiÄ™kszenie efektywnoÅ›ci operacyjnej\nReal-time analytics umoÅ¼liwia natychmiastowe wykrywanie i eliminowanie problemÃ³w operacyjnych, zanim stanÄ… siÄ™ powaÅ¼niejsze. PrzykÅ‚ady:\n\nLogistyka: Åšledzenie przesyÅ‚ek i monitorowanie statusu transportu w czasie rzeczywistym, co poprawia efektywnoÅ›Ä‡ i zmniejsza opÃ³Åºnienia.\nRetail: Monitorowanie poziomu zapasÃ³w na bieÅ¼Ä…co i dostosowywanie zamÃ³wieÅ„ do aktualnych potrzeb.\n\n\n\nKonkurencyjnoÅ›Ä‡\nOrganizacje, ktÃ³re wykorzystujÄ… analitykÄ™ w czasie rzeczywistym, majÄ… przewagÄ™ nad konkurencjÄ…, poniewaÅ¼ mogÄ… szybciej reagowaÄ‡ na zmiany na rynku, nowe potrzeby klientÃ³w i sytuacje kryzysowe. DziÄ™ki natychmiastowym informacjom:\n\nMoÅ¼na podejmowaÄ‡ decyzje z wyprzedzeniem przed konkurentami.\nUtrzymywaÄ‡ lepsze relacje z klientami, reagujÄ…c na ich potrzeby w czasie rzeczywistym (np. dostosowywanie oferty).\n\n\n\nLepsze doÅ›wiadczenia uÅ¼ytkownikÃ³w (Customer Experience)\nAnaliza danych w czasie rzeczywistym pozwala na dostosowywanie interakcji z uÅ¼ytkownikami w trakcie ich trwania. PrzykÅ‚ady:\n\nE-commerce: Analiza koszyka zakupowego uÅ¼ytkownika w czasie rzeczywistym, aby np. zaoferowaÄ‡ rabat lub przypomnieÄ‡ o porzuconych produktach.\nStreaming: Optymalizacja jakoÅ›ci usÅ‚ugi wideo/streamingowej w zaleÅ¼noÅ›ci od dostÄ™pnej przepustowoÅ›ci Å‚Ä…cza.\n\n\n\nWykrywanie i reagowanie na anomalie\nW dzisiejszym Å›wiecie peÅ‚nym danych, wykrywanie anomalii w czasie rzeczywistym jest kluczowe dla bezpieczeÅ„stwa. PrzykÅ‚ady:\n\nCyberbezpieczeÅ„stwo: Real-time analytics umoÅ¼liwia wykrywanie podejrzanych dziaÅ‚aÅ„ w sieci i zapobieganie atakom w czasie rzeczywistym (np. ataki DDoS, nieautoryzowane logowanie).\nWykrywanie oszustw: Natychmiastowa identyfikacja podejrzanych transakcji w systemach bankowych i kartach kredytowych.\n\n\n\nOptymalizacja kosztÃ³w\nDziÄ™ki analizie w czasie rzeczywistym moÅ¼na optymalizowaÄ‡ zasoby i zmniejszaÄ‡ koszty. Na przykÅ‚ad:\n\nZarzÄ…dzanie energiÄ…: Analiza zuÅ¼ycia energii w czasie rzeczywistym, umoÅ¼liwiajÄ…ca optymalizacjÄ™ wydatkÃ³w na energiÄ™ w firmach.\nOptymalizacja Å‚aÅ„cucha dostaw: DziÄ™ki bieÅ¼Ä…cemu Å›ledzeniu zapasÃ³w i dostaw moÅ¼na lepiej zarzÄ…dzaÄ‡ kosztami magazynowania i transportu.\n\n\n\nZdolnoÅ›Ä‡ do przewidywania i zapobiegania\nAnaliza w czasie rzeczywistym wspiera procesy predykcyjne, ktÃ³re mogÄ… przewidywaÄ‡ przyszÅ‚e zachowania lub problemy, a takÅ¼e je eliminowaÄ‡ zanim siÄ™ pojawiÄ…. Na przykÅ‚ad:\n\nUtrzymanie predykcyjne w produkcji: Wykorzystanie analizy w czasie rzeczywistym w poÅ‚Ä…czeniu z modelami predykcyjnymi pozwala przewidywaÄ‡ awarie maszyn.\nPrognozy popytu: W czasie rzeczywistym moÅ¼na dostosowywaÄ‡ produkcjÄ™ lub zapasy na podstawie bieÅ¼Ä…cych trendÃ³w.\n\nReal-time analytics to nie tylko analiza danych â€“ to kluczowy element strategii firm w Å›wiecie, ktÃ³ry wymaga szybkich reakcji, elastycznoÅ›ci i dostosowywania siÄ™ do zmieniajÄ…cego siÄ™ otoczenia. Firmy, ktÃ³re wdraÅ¼ajÄ… te technologie, mogÄ… znaczÄ…co poprawiÄ‡ swoje wyniki finansowe, obsÅ‚ugÄ™ klienta, wydajnoÅ›Ä‡ operacyjnÄ…, a takÅ¼e przewagÄ™ konkurencyjnÄ….",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "href": "lectures/wyklad1.html#wyzwania-i-problemy-analizy-danych-w-czasie-rzeczywistym",
    "title": "WykÅ‚ad 1",
    "section": "Wyzwania i problemy analizy danych w czasie rzeczywistym",
    "text": "Wyzwania i problemy analizy danych w czasie rzeczywistym\nAnaliza danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z wieloma wyzwaniami i trudnoÅ›ciami, ktÃ³re trzeba rozwiÄ…zaÄ‡, aby systemy real-time dziaÅ‚aÅ‚y efektywnie i niezawodnie. Pomimo ogromnego potencjaÅ‚u, jaki daje moÅ¼liwoÅ›Ä‡ natychmiastowego przetwarzania danych, realizacja tych procesÃ³w w praktyce wiÄ…Å¼e siÄ™ z licznymi problemami technologicznymi, organizacyjnymi i dotyczÄ…cymi zarzÄ…dzania danymi.\nPoniÅ¼ej przedstawiamy najwaÅ¼niejsze wyzwania oraz moÅ¼liwe rozwiÄ…zania, ktÃ³re naleÅ¼y uwzglÄ™dniÄ‡ podczas implementacji systemÃ³w analizy danych w czasie rzeczywistym.\n\nSkalowalnoÅ›Ä‡ systemÃ³w\n\nWyzwanie:\nSkalowanie systemu analitycznego w czasie rzeczywistym jest jednym z najtrudniejszych zadaÅ„. W miarÄ™ jak iloÅ›Ä‡ generowanych danych roÅ›nie, systemy muszÄ… byÄ‡ w stanie obsÅ‚ugiwaÄ‡ wiÄ™ksze obciÄ…Å¼enie bez opÃ³Åºnienia w przetwarzaniu.\nZwiÄ™kszona iloÅ›Ä‡ danych: W systemach real-time, jak np. monitorowanie danych IoT czy transakcje w systemach finansowych, iloÅ›Ä‡ generowanych danych moÅ¼e byÄ‡ olbrzymia. Potrzebna jest elastycznoÅ›Ä‡: System musi automatycznie dostosowywaÄ‡ zasoby w zaleÅ¼noÅ›ci od obciÄ…Å¼enia.\n\n\nRozwiÄ…zanie:\nWykorzystanie skalowalnych systemÃ³w chmurowych, ktÃ³re pozwalajÄ… na dynamiczne zwiÄ™kszanie zasobÃ³w obliczeniowych (np. AWS, Azure, Google Cloud). Kubernetes do zarzÄ…dzania kontenerami i automatycznego skalowania mikroserwisÃ³w. Technologie strumieniowe (Apache Kafka, Apache Flink) umoÅ¼liwiajÄ…ce przetwarzanie danych w sposÃ³b wydajny i rozproszony.\n\n\n\nOpÃ³Åºnienia (Latency)\n\nWyzwanie:\nW systemach analizy danych w czasie rzeczywistym, kaÅ¼de opÃ³Åºnienie w przetwarzaniu danych moÅ¼e mieÄ‡ powaÅ¼ne konsekwencje. Dotyczy to zwÅ‚aszcza obszarÃ³w takich jak:\nWykrywanie oszustw: W przypadku systemÃ³w pÅ‚atnoÅ›ci online, opÃ³Åºnienie w analizie transakcji moÅ¼e oznaczaÄ‡ przegapienie nieautoryzowanej transakcji. Monitorowanie zdrowia pacjentÃ³w: OpÃ³Åºnienia mogÄ… wpÅ‚ynÄ…Ä‡ na skutecznoÅ›Ä‡ reakcji w sytuacjach kryzysowych.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie algorytmÃ³w optymalizujÄ…cych czas przetwarzania, np. stream processing z wykorzystaniem systemÃ³w takich jak Apache Kafka lub Apache Flink. Edge computing: Przesuwanie przetwarzania danych bliÅ¼ej ÅºrÃ³dÅ‚a (np. urzÄ…dzenia IoT), aby zmniejszyÄ‡ opÃ³Åºnienia w transmisji danych do chmury.\n\n\n\nJakoÅ›Ä‡ danych i zarzÄ…dzanie danymi\n\nWyzwanie:\nW systemach real-time musimy nie tylko analizowaÄ‡ dane w czasie rzeczywistym, ale takÅ¼e zapewniÄ‡ ich wysokÄ… jakoÅ›Ä‡. W przeciwnym razie analizy mogÄ… prowadziÄ‡ do bÅ‚Ä™dnych wnioskÃ³w lub opÃ³ÅºnieÅ„ w reagowaniu na nieprawidÅ‚owe dane.\nZanieczyszczone dane: W systemach real-time dane czÄ™sto sÄ… niepeÅ‚ne, brudne, bÅ‚Ä™dne lub nieuporzÄ…dkowane. Zmiana charakterystyki danych: Dane mogÄ… zmieniaÄ‡ siÄ™ w czasie, co moÅ¼e utrudniaÄ‡ ich przetwarzanie i analizÄ™. #### RozwiÄ…zanie:\nData cleansing i data validation na wstÄ™pnym etapie procesu. Automatyczne systemy monitorowania jakoÅ›ci danych w celu wykrywania bÅ‚Ä™dÃ³w w czasie rzeczywistym. ZarzÄ…dzanie danymi w strumieniu: NarzÄ™dzia takie jak Apache Kafka pozwalajÄ… na filtrowanie i oczyszczanie danych w locie.\n\n\n\nZÅ‚oÅ¼onoÅ›Ä‡ integracji systemÃ³w\n\nWyzwanie:\nSystemy analizy danych w czasie rzeczywistym czÄ™sto muszÄ… wspÃ³Å‚pracowaÄ‡ z istniejÄ…cymi systemami IT i ÅºrÃ³dÅ‚ami danych (np. bazami danych, czujnikami IoT, aplikacjami). Integracja tych systemÃ³w, zwÅ‚aszcza w rozproszonej architekturze, moÅ¼e byÄ‡ skomplikowana.\n\n\nRozwiÄ…zanie:\nUÅ¼ywanie API do Å‚atwiejszej integracji z zewnÄ™trznymi systemami. Mikroserwisy i konteneryzacja z pomocÄ… narzÄ™dzi takich jak Docker i Kubernetes. Przetwarzanie w chmurze, ktÃ³re umoÅ¼liwia Å‚atwÄ… integracjÄ™ rÃ³Å¼nych ÅºrÃ³deÅ‚ danych oraz zapewnia elastycznoÅ›Ä‡ w dostosowywaniu systemÃ³w do rosnÄ…cych potrzeb.\n\n\n\nBezpieczeÅ„stwo i prywatnoÅ›Ä‡\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wiÄ…Å¼e siÄ™ z ogromnÄ… iloÅ›ciÄ… wraÅ¼liwych informacji, szczegÃ³lnie w branÅ¼ach takich jak finanse, zdrowie czy e-commerce. Zapewnienie, Å¼e dane sÄ… odpowiednio chronione przed nieautoryzowanym dostÄ™pem, jest kluczowe.\nOchrona danych w czasie transmisji: MuszÄ… byÄ‡ szyfrowane zarÃ³wno podczas przesyÅ‚ania, jak i przechowywania. Zabezpieczenia przed atakami: Przetwarzanie danych w czasie rzeczywistym moÅ¼e byÄ‡ celem atakÃ³w, takich jak DDoS czy SQL injection.\n\n\nRozwiÄ…zanie:\nSzyfrowanie danych zarÃ³wno w spoczynku, jak i podczas przesyÅ‚ania (np. TLS). Autentykacja i autoryzacja z wykorzystaniem nowoczesnych technologii bezpieczeÅ„stwa. ZgodnoÅ›Ä‡ z regulacjami prawnymi, np. RODO w Unii Europejskiej czy GDPR w przypadku danych osobowych.\n\n\n\nZarzÄ…dzanie bÅ‚Ä™dami i awariami\n\nWyzwanie:\nBÅ‚Ä™dy i awarie w systemach real-time mogÄ… prowadziÄ‡ do powaÅ¼nych konsekwencji, w tym utraty danych, opÃ³ÅºnieÅ„ w analizach czy nawet usuniÄ™cia usÅ‚ug. W systemach rozproszonych trudno jest osiÄ…gnÄ…Ä‡ peÅ‚nÄ… niezawodnoÅ›Ä‡.\n\n\nRozwiÄ…zanie:\nRedundancja: Tworzenie kopii zapasowych systemÃ³w i danych. Systemy monitorowania i alertowania (np. Prometheus, Grafana), ktÃ³re pozwalajÄ… na szybkie wykrycie i naprawienie problemÃ³w. ZarzÄ…dzanie stanem: DziÄ™ki uÅ¼yciu narzÄ™dzi jak Apache Kafka, moÅ¼na ponownie przetwarzaÄ‡ dane, jeÅ›li wystÄ…piÅ‚ bÅ‚Ä…d w transmisji.\n\n\n\nKoszty zwiÄ…zane z infrastrukturÄ…\n\nWyzwanie:\nPrzetwarzanie danych w czasie rzeczywistym wymaga odpowiedniej infrastruktury, ktÃ³ra zapewni odpowiedniÄ… moc obliczeniowÄ… i pamiÄ™Ä‡. To moÅ¼e wiÄ…zaÄ‡ siÄ™ z duÅ¼ymi kosztami, szczegÃ³lnie gdy dane muszÄ… byÄ‡ przechowywane i przetwarzane w czasie rzeczywistym na duÅ¼Ä… skalÄ™.\n\n\nRozwiÄ…zanie:\nChmura obliczeniowa: MoÅ¼liwoÅ›Ä‡ elastycznego skalowania zasobÃ³w w chmurze. Serverless computing: Technologie takie jak AWS Lambda pozwalajÄ… na uruchamianie procesÃ³w bez potrzeby utrzymywania staÅ‚ej infrastruktury.\nChociaÅ¼ analiza danych w czasie rzeczywistym oferuje ogromne korzyÅ›ci, wiÄ…Å¼e siÄ™ takÅ¼e z wieloma wyzwaniami. WÅ‚aÅ›ciwa architektura, narzÄ™dzia i technologie, takie jak Apache Kafka, Flink, Spark czy Kubernetes, mogÄ… pomÃ³c w przezwyciÄ™Å¼eniu wielu z tych trudnoÅ›ci. Warto rÃ³wnieÅ¼ pamiÄ™taÄ‡ o koniecznoÅ›ci zapewnienia wysokiej jakoÅ›ci danych, ich bezpieczeÅ„stwa, a takÅ¼e elastycznoÅ›ci i skalowalnoÅ›ci systemÃ³w, ktÃ³re bÄ™dÄ… w stanie sprostaÄ‡ rosnÄ…cym wymaganiom.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 1"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html",
    "href": "lectures/wyklad5.html",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n1. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania:\n\nAnaliza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\n2. Algorytmy dokonujÄ…ce wielu obliczeÅ„\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA.\nProces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych.\nCzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\n\nğŸ”¹ Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\n3. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych.\nMoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\n\nğŸ”¹ Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#algorytmy",
    "href": "lectures/wyklad5.html#algorytmy",
    "title": "WykÅ‚ad 5",
    "section": "",
    "text": "Zanim zaprojektujesz rozwiÄ…zanie problemu biznesowego, warto zastanowiÄ‡ siÄ™ nad zÅ‚oÅ¼onoÅ›ciÄ… Twojego problemu.\n\n\n1. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych\nPrzetwarzanie ogromnych zbiorÃ³w danych wymaga odpowiedniego podejÅ›cia do ich organizacji i analizy. W sytuacji, gdy iloÅ›Ä‡ danych przekracza dostÄ™pnÄ… pamiÄ™Ä‡ jednostki obliczeniowej, czÄ™sto stosuje siÄ™ iteracyjne sposoby ich przetwarzania.\nğŸ”¹ PrzykÅ‚ad: System rekomendacji w e-commerce (np. Amazon, Netflix)\n\nAnalizuje ogromne zbiory danych o uÅ¼ytkownikach, ich historii zakupÃ³w i oglÄ…danych treÅ›ci.\nPrzetwarza dane w sposÃ³b iteracyjny (np. strumieniowe przetwarzanie w Apache Spark).\nWykorzystuje algorytmy filtracji kolaboratywnej lub algorytmy grafowe do przewidywania preferencji uÅ¼ytkownika.\n\nğŸ”¹ Inne zastosowania:\n\nAnaliza logÃ³w serwerowych w czasie rzeczywistym (np. wykrywanie atakÃ³w DDoS).\nMonitoring sieci IoT (np. analiza danych z sensorÃ³w w inteligentnym mieÅ›cie).\n\n2. Algorytmy dokonujÄ…ce wielu obliczeÅ„\nWymagajÄ… duÅ¼ej mocy obliczeniowej, ale zazwyczaj nie operujÄ… na wielkich zbiorach danych. PrzykÅ‚adem moÅ¼e byÄ‡ algorytm wyszukujÄ…cy duÅ¼Ä… liczbÄ™ pierwszÄ…. CzÄ™sto wykorzystuje siÄ™ tutaj podziaÅ‚ obliczeÅ„ na rÃ³wnolegÅ‚e procesy w celu optymalizacji wydajnoÅ›ci.\nğŸ”¹ PrzykÅ‚ad: Kryptografia i znalezienie duÅ¼ej liczby pierwszej (np. RSA)\n\nAlgorytm generuje bardzo duÅ¼e liczby pierwsze, ktÃ³re sÄ… podstawÄ… dla szyfrowania RSA.\nProces wymaga intensywnych obliczeÅ„, ale nie operuje na ogromnych zbiorach danych.\nCzÄ™sto wykorzystywane sÄ… metody rÃ³wnolegÅ‚e, np. algorytm probabilistyczny Millera-Rabina do testowania pierwszoÅ›ci.\n\nğŸ”¹ Inne zastosowania:\n\nSymulacje fizyczne (np. prognozowanie pogody, modele klimatyczne).\nAlgorytmy optymalizacyjne (np. znajdowanie najkrÃ³tszej trasy w problemie komiwojaÅ¼era).\n\n3. Algorytmy przetwarzajÄ…ce duÅ¼e iloÅ›ci danych i dokonujÄ…ce wielu obliczeÅ„\nÅÄ…czÄ… wymagania obu poprzednich typÃ³w, potrzebujÄ…c zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych, jak i obsÅ‚ugi duÅ¼ych zbiorÃ³w danych. PrzykÅ‚adem moÅ¼e byÄ‡ analiza sentymentu w transmisjach wideo na Å¼ywo.\nğŸ”¹ PrzykÅ‚ad: Analiza sentymentu w transmisjach wideo na Å¼ywo (np. YouTube, Twitch)\n\nAlgorytm analizuje zarÃ³wno tekst (czat), jak i obraz/wideo w czasie rzeczywistym.\nWymaga zarÃ³wno duÅ¼ych zasobÃ³w obliczeniowych (przetwarzanie NLP i CV), jak i obsÅ‚ugi duÅ¼ej iloÅ›ci danych.\nMoÅ¼e wykorzystywaÄ‡ modele Transformer (np. BERT) do analizy tekstu oraz CNN/RNN do analizy obrazu i dÅºwiÄ™ku.\n\nğŸ”¹ Inne zastosowania:\n\nAutonomiczne pojazdy (analiza obrazu i decyzje w czasie rzeczywistym).\nWyszukiwanie anomalii w ogromnych zbiorach danych finansowych (np. wykrywanie oszustw bankowych).\n\n\n\n\nAby okreÅ›liÄ‡ wymiar danych problemu, nie wystarczy podaÄ‡ jedynie iloÅ›ci miejsca zajmowanego przez dane. Istotne sÄ… trzy gÅ‚Ã³wne aspekty:\n\nRozmiar wejÅ›cia â€“ oczekiwany rozmiar danych do przetwarzania.\nSzybkoÅ›Ä‡ narastania â€“ tempo generowania nowych danych podczas dziaÅ‚ania algorytmu.\nRÃ³Å¼norodnoÅ›Ä‡ struktury â€“ typy danych, jakie algorytm musi obsÅ‚uÅ¼yÄ‡.\n\n\n\n\nDotyczy zasobÃ³w procesowania i mocy obliczeniowej. Na przykÅ‚ad algorytmy uczenia gÅ‚Ä™bokiego (DL) wymagajÄ… duÅ¼ej mocy obliczeniowej, dlatego warto zapewniÄ‡ zrÃ³wnoleglonÄ… architekturÄ™, wykorzystujÄ…cÄ… GPU lub TPU, co znaczÄ…co przyspiesza obliczenia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "href": "lectures/wyklad5.html#wyjaÅ›nialnoÅ›Ä‡-algorytmÃ³w",
    "title": "WykÅ‚ad 5",
    "section": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w",
    "text": "WyjaÅ›nialnoÅ›Ä‡ algorytmÃ³w\nW wielu przypadkach modelowanie jest wykorzystywane w sytuacjach krytycznych, np. w oprogramowaniu do podawania lekÃ³w. W takich sytuacjach kluczowe staje siÄ™ wyjaÅ›nienie przyczyny kaÅ¼dego wyniku dziaÅ‚ania algorytmu. Jest to konieczne, aby zapewniÄ‡, Å¼e decyzje podejmowane na jego podstawie sÄ… wolne od bÅ‚Ä™dÃ³w i uprzedzeÅ„.\nZdolnoÅ›Ä‡ algorytmu do wskazania mechanizmÃ³w generujÄ…cych wyniki nazywamy moÅ¼liwoÅ›ciÄ… wyjaÅ›nienia. Analiza etyczna stanowi standardowy element procesu walidacji algorytmu.\nUzyskanie wysokiej wyjaÅ›nialnoÅ›ci jest szczegÃ³lnie trudne w przypadku algorytmÃ³w uczenia maszynowego (ML) i gÅ‚Ä™bokiego uczenia (DL). Na przykÅ‚ad banki korzystajÄ…ce z algorytmÃ³w do podejmowania decyzji kredytowych muszÄ… zapewniÄ‡ transparentnoÅ›Ä‡ i wskazaÄ‡ powody wydanej decyzji.\nJednÄ… z metod poprawy wyjaÅ›nialnoÅ›ci algorytmÃ³w jest LIME (Local Interpretable Model-Agnostic Explanations), opublikowana w 2016 roku. Metoda ta polega na wprowadzaniu niewielkich zmian w danych wejÅ›ciowych i analizowaniu ich wpÅ‚ywu na wynik, co pozwala okreÅ›liÄ‡ lokalne zasady podejmowania decyzji przez model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# PodziaÅ‚ na zbiÃ³r treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykÅ‚adu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # WybÃ³r losowego przykÅ‚adu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# WyÅ›wietlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nJak dziaÅ‚a ten kod? 1. Åadowanie danych i trenowanie modelu\n\nUÅ¼ywamy zbioru Iris, ktÃ³ry zawiera 150 przykÅ‚adÃ³w kwiatÃ³w z trzema gatunkami:\nSetosa\nVersicolor\nVirginica\nModel RandomForestClassifier trenuje siÄ™ na tych danych.\n\n\nTworzenie interpretowalnego modelu za pomocÄ… LIME\n\n\nLIME generuje lokalne wyjaÅ›nienia, czyli interpretuje model dla pojedynczych predykcji.\nWybieramy losowy przykÅ‚ad z danych testowych.\n\n\nEksploracja wyniku dla jednego przykÅ‚adu\n\n\nLIME modyfikuje lekko wartoÅ›ci wejÅ›ciowe i obserwuje, jak zmienia siÄ™ wynik predykcji.\nTworzy â€lokalnyâ€ model liniowy, ktÃ³ry pokazuje, ktÃ³re cechy miaÅ‚y najwiÄ™kszy wpÅ‚yw na decyzjÄ™.\n\nZaÅ‚Ã³Å¼my, Å¼e nasz model wybraÅ‚ przykÅ‚adowÄ… roÅ›linÄ™ i sklasyfikowaÅ‚ jÄ… jako Virginica.\nOto interpretacja wynikÃ³w:\n\nNajwaÅ¼niejsze cechy wpÅ‚ywajÄ…ce na decyzjÄ™ modelu:\n\n\nDÅ‚ugoÅ›Ä‡ pÅ‚atka (petal length): najwiÄ™kszy wpÅ‚yw na predykcjÄ™ (np. im wiÄ™ksza, tym wiÄ™ksze prawdopodobieÅ„stwo, Å¼e to Virginica).\nSzerokoÅ›Ä‡ pÅ‚atka (petal width): rÃ³wnieÅ¼ istotny czynnik (np. powyÅ¼ej pewnej wartoÅ›ci sugeruje Virginica).\nDÅ‚ugoÅ›Ä‡ kielicha (sepal length): mniejszy wpÅ‚yw, ale nadal istotny.\nSzerokoÅ›Ä‡ kielicha (sepal width): zwykle najmniej istotna cecha.\n\n\nWizualizacja wynikÃ³w\n\n\nLIME generuje wykres sÅ‚upkowy, ktÃ³ry pokazuje wpÅ‚yw kaÅ¼dej cechy na klasyfikacjÄ™.\nNa wykresie widaÄ‡, ktÃ³re cechy zwiÄ™kszaÅ‚y, a ktÃ³re zmniejszaÅ‚y prawdopodobieÅ„stwo przypisania do danej klasy.\n\n\nCo oznacza wynik?\n\n\nJeÅ›li model przewidziaÅ‚ klasÄ™ Virginica z wysokim prawdopodobieÅ„stwem, oznacza to, Å¼e kluczowe cechy (np. dÅ‚ugi pÅ‚atek) mocno wskazujÄ… na ten gatunek.\nJeÅ›li cechy miaÅ‚y zrÃ³Å¼nicowany wpÅ‚yw, oznacza to, Å¼e model miaÅ‚ pewne trudnoÅ›ci w klasyfikacji (np. szerokoÅ›Ä‡ pÅ‚atka nie byÅ‚a jednoznaczna).",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#detekcja-anomalii",
    "href": "lectures/wyklad5.html#detekcja-anomalii",
    "title": "WykÅ‚ad 5",
    "section": "Detekcja anomalii",
    "text": "Detekcja anomalii\n\nWartoÅ›Ä‡ odstajÄ…ca (Outlier)\nWartoÅ›Ä‡ odstajÄ…ca (ang. outlier) to obserwacja (wiersz w tabeli danych), ktÃ³ra jest znacznie oddalona od pozostaÅ‚ych elementÃ³w prÃ³bki. Oznacza to, Å¼e zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi niezaleÅ¼nymi i zaleÅ¼nymi dla tej obserwacji moÅ¼e rÃ³Å¼niÄ‡ siÄ™ od pozostaÅ‚ych przypadkÃ³w.\nDla pojedynczych zmiennych wartoÅ›ci odstajÄ…ce moÅ¼na okreÅ›liÄ‡, korzystajÄ…c z wykresu pudeÅ‚kowego (box plot). Wykres ten bazuje na kwartylach:\n\nPierwszy kwartyl \\(Q_1\\) i trzeci kwartyl \\(Q_3\\) wyznaczajÄ… boki pudeÅ‚ka,\nDrugi kwartyl \\(Q_2\\) (mediana) jest zaznaczony wewnÄ…trz pudeÅ‚ka,\n\nWartoÅ›ci odstajÄ…ce speÅ‚niajÄ… zaleÅ¼noÅ›Ä‡:\n\\[\nx_{out} &lt; Q_1 - 1.5 \\times IQR \\quad \\text{lub} \\quad x_{out} &gt; Q_3 + 1.5 \\times IQR\n\\]\nGdzie: \\[\nIQR = Q_3 - Q_1\n\\]\nPrzykÅ‚adem wartoÅ›ci odstajÄ…cej moÅ¼e byÄ‡ bolid FormuÅ‚y 1 â€“ pod wzglÄ™dem prÄ™dkoÅ›ci jest on anomaliÄ… wÅ›rÃ³d zwykÅ‚ych samochodÃ³w.\n\n\nWykorzystanie detekcji anomalii\nWykrywanie wartoÅ›ci odstajÄ…cych ma szerokie zastosowanie, np.:\n\nFinanse â€“ wykrywanie transakcji fraudowych w analizie danych bankowych,\nCyberbezpieczeÅ„stwo â€“ identyfikacja intruzÃ³w w sieci na podstawie zachowaÅ„ uÅ¼ytkownikÃ³w,\nMedycyna â€“ monitorowanie parametrÃ³w zdrowotnych i wykrywanie nieprawidÅ‚owoÅ›ci,\nPrzemysÅ‚ â€“ wykrywanie wadliwych komponentÃ³w poprzez analizÄ™ obrazu.\n\n\n\nMetody wykrywania anomalii\n\n1. Metody nadzorowane (supervised learning)\nStosowane, gdy mamy oznaczone dane (np. przypadki oszustw w transakcjach).\n\nSieci neuronowe,\nAlgorytm K-najbliÅ¼szych sÄ…siadÃ³w (KNN),\nSieci Bayesowskie.\n\n\n\n2. Metody nienadzorowane (unsupervised learning)\nZakÅ‚adajÄ…, Å¼e wiÄ™kszoÅ›Ä‡ danych jest poprawna, a anomalie to niewielki odsetek przypadkÃ³w.\n\nKlasteryzacja metodÄ… K-Å›rednich (K-Means),\nAutoenkodery w sieciach neuronowych,\nTesty statystyczne.\n\n\n\n\nMetoda klasyczna â€“ detekcja na podstawie prawdopodobieÅ„stwa\nAby okreÅ›liÄ‡, czy dana obserwacja jest anomaliÄ…, moÅ¼na uÅ¼yÄ‡ prawdopodobieÅ„stwa \\(p(x)\\):\n\nJeÅ›li \\(p(x) &lt; \\epsilon\\), uznajemy wartoÅ›Ä‡ za odstajÄ…cÄ….\nW praktyce zakÅ‚adamy, Å¼e dane majÄ… rozkÅ‚ad normalny \\(N(\\mu, \\sigma)\\).\nSzacujemy parametry \\(\\mu\\) (Å›rednia) i \\(\\sigma^2\\) (wariancja) na podstawie prÃ³bki.\nNastÄ™pnie dla kaÅ¼dej wartoÅ›ci obliczamy prawdopodobieÅ„stwo jej wystÄ…pienia i porÃ³wnujemy z \\(\\epsilon\\).\n\nPrzykÅ‚ad: Analiza wynagrodzeÅ„ w firmie\nWykrywamy, czy w danej firmie sÄ… osoby o wynagrodzeniach znacznie odbiegajÄ…cych od Å›redniej.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PrzykÅ‚adowe wynagrodzenia w firmie (w tysiÄ…cach)\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\n# Obliczenie kwartylÃ³w\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\n# Definicja wartoÅ›ci odstajÄ…cych\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\n# Znalezienie outlierÃ³w\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\n# Wizualizacja\nsns.boxplot(salaries)\nplt.title(\"Wykres pudeÅ‚kowy wynagrodzeÅ„\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nWynik: Na wykresie pudeÅ‚kowym widaÄ‡, Å¼e 150 tys. to anomalia.\n\n\nIsolation Forest â€“ detekcja anomalii za pomocÄ… lasu izolacyjnego\nIsolation Forest to algorytm bazujÄ…cy na drzewach decyzyjnych, zaproponowany przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku. Identyfikuje anomalie poprzez izolowanie wartoÅ›ci odstajÄ…cych w procesie podziaÅ‚u danych:\n\nWybiera losowo cechÄ™ oraz wartoÅ›Ä‡ podziaÅ‚u,\nWartoÅ›ci odstajÄ…ce szybciej zostajÄ… odizolowane (sÄ… bliÅ¼ej korzenia drzewa),\nWynik jest agregowany na podstawie wielu drzew.\n\nJego zalety to niskie wymagania obliczeniowe i skutecznoÅ›Ä‡ w analizie wielowymiarowych danych.\nMetody detekcji anomalii sklearn\nPrzykÅ‚ad: Wykrywanie oszustw bankowych\nBank analizuje transakcje kartÄ… kredytowÄ… i wykrywa te, ktÃ³re mogÄ… byÄ‡ nieautoryzowane.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# PrzykÅ‚adowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duÅ¼a kwota, rzadkoÅ›Ä‡)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nWynik: Transakcja 5000 zÅ‚ zostanie wykryta jako anomalia.",
    "crumbs": [
      "222890-S",
      "WykÅ‚ady",
      "WykÅ‚ad 5"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html",
    "href": "kafka_codes/kafka3.html",
    "title": "Consumer w Å›rodowisku Python",
    "section": "",
    "text": "Przygotuj Å›rodowisko i uruchom skrypt producenta.\nRozpatrzmy kod konsumenta czytajÄ…cego z topicu oraz realizujÄ…cego prostÄ… reguÅ‚Ä™ decyzyjnÄ….\n\n%%file konsument.py\nfrom kafka import KafkaConsumer\nimport json  \n\nSERVER = \"broker:9092\"\nTOPIC  = \"streaming\"\n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    TOPIC,\n    bootstrap_servers=SERVER,\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"values\"] &gt; 80:\n        print(f\"ğŸš¨ Wykryto duÅ¼Ä… transakcjÄ™: {transaction}\")\n\nKod ten uruchom poleceniem:\npython konsument.py",
    "crumbs": [
      "222890-S",
      "Laboratoria",
      "Apache Kafka",
      "Consumer w Å›rodowisku Python"
    ]
  }
]